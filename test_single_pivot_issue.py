"""
å•æ¢çº½å¬å›ç‡ä¸‹é™é—®é¢˜éªŒè¯æµ‹è¯•
"""
import numpy as np
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from method3.kmeans_hnsw import KMeansHNSW
from method3.tune_kmeans_hnsw_optimized import SharedKMeansHNSWSystem, OptimizedSinglePivotSystem
from hnsw.hnsw import HNSW

def test_single_pivot_comparison():
    """å¯¹æ¯”åŸå§‹ç‰ˆæœ¬å’Œä¼˜åŒ–ç‰ˆæœ¬çš„å•æ¢çº½æ€§èƒ½"""
    print("ğŸ”¬ å•æ¢çº½å¬å›ç‡å¯¹æ¯”æµ‹è¯•")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    n_data = 5000
    n_queries = 50
    dim = 128
    
    dataset = np.random.randn(n_data, dim).astype(np.float32)
    query_vectors = np.random.randn(n_queries, dim).astype(np.float32)
    query_ids = list(range(n_queries))
    
    distance_func = lambda x, y: np.linalg.norm(x - y)
    
    # æ„å»ºåŸºç¡€HNSWç´¢å¼•
    print("æ„å»ºåŸºç¡€HNSWç´¢å¼•...")
    base_index = HNSW(distance_func=distance_func, m=16, ef_construction=200)
    for i, vector in enumerate(dataset):
        base_index.insert(i, vector)
    
    # æµ‹è¯•å‚æ•°
    test_params = {
        'n_clusters': 32,
        'k_children': 200,
        'child_search_ef': 300
    }
    
    print(f"\næµ‹è¯•å‚æ•°: {test_params}")
    
    # === æµ‹è¯•1: åŸå§‹ç‰ˆæœ¬ï¼ˆæ— ä¼˜åŒ–ï¼‰ ===
    print("\n=== æµ‹è¯•åŸå§‹ç‰ˆæœ¬ï¼ˆæ— diversify/repairï¼‰===")
    original_system = KMeansHNSW(
        base_index=base_index,
        n_clusters=test_params['n_clusters'],
        k_children=test_params['k_children'],
        child_search_ef=test_params['child_search_ef'],
        diversify_max_assignments=None,  # ç¦ç”¨diversify
        repair_min_assignments=None      # ç¦ç”¨repair
    )
    
    # æ”¶é›†åŸå§‹ç‰ˆæœ¬ç»Ÿè®¡
    original_stats = original_system.get_stats()
    print(f"åŸå§‹ç‰ˆæœ¬ç»Ÿè®¡:")
    print(f"  - å­èŠ‚ç‚¹æ€»æ•°: {original_stats['num_children']}")
    print(f"  - å¹³å‡å­èŠ‚ç‚¹/è´¨å¿ƒ: {original_stats['avg_children_per_centroid']:.1f}")
    print(f"  - è¦†ç›–ç‡: {original_stats['coverage_fraction']:.3f}")
    
    # === æµ‹è¯•2: ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆæ— diversify/repairï¼‰===
    print("\n=== æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆæ— diversify/repairï¼‰===")
    
    params = {
        'n_clusters': test_params['n_clusters'],
        'k_children': test_params['k_children'],
        'child_search_ef': test_params['child_search_ef']
    }
    
    adaptive_config_clean = {
        'diversify_max_assignments': None,  # ç¦ç”¨diversify
        'repair_min_assignments': None      # ç¦ç”¨repair
    }
    
    shared_system = SharedKMeansHNSWSystem(
        base_index=base_index,
        params=params,
        adaptive_config=adaptive_config_clean
    )
    
    optimized_system_clean = OptimizedSinglePivotSystem(
        shared_system=shared_system,
        adaptive_config=adaptive_config_clean
    )
    
    optimized_stats_clean = optimized_system_clean.get_stats()
    print(f"ä¼˜åŒ–ç‰ˆæœ¬ç»Ÿè®¡ï¼ˆæ— ä¼˜åŒ–ï¼‰:")
    print(f"  - å­èŠ‚ç‚¹æ€»æ•°: {optimized_stats_clean['num_children']}")
    print(f"  - å¹³å‡å­èŠ‚ç‚¹/è´¨å¿ƒ: {optimized_stats_clean['num_children'] / test_params['n_clusters']:.1f}")
    
    # === æµ‹è¯•3: ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆå¸¦diversifyï¼‰===
    print("\n=== æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆå¸¦diversifyé™åˆ¶ï¼‰===")
    adaptive_config_diversify = {
        'diversify_max_assignments': 3,  # å¯ç”¨diversify
        'repair_min_assignments': None
    }
    
    # éœ€è¦é‡æ–°åˆ›å»ºshared_systemï¼Œå› ä¸ºadaptive_configåœ¨åˆå§‹åŒ–æ—¶å°±å›ºå®šäº†
    shared_system_diversify = SharedKMeansHNSWSystem(
        base_index=base_index,
        params=params,
        adaptive_config=adaptive_config_diversify
    )
    
    optimized_system_diversify = OptimizedSinglePivotSystem(
        shared_system=shared_system_diversify,
        adaptive_config=adaptive_config_diversify
    )
    
    optimized_stats_diversify = optimized_system_diversify.get_stats()
    print(f"ä¼˜åŒ–ç‰ˆæœ¬ç»Ÿè®¡ï¼ˆå¸¦diversify=3ï¼‰:")
    print(f"  - å­èŠ‚ç‚¹æ€»æ•°: {optimized_stats_diversify['num_children']}")
    print(f"  - å¹³å‡å­èŠ‚ç‚¹/è´¨å¿ƒ: {optimized_stats_diversify['num_children'] / test_params['n_clusters']:.1f}")
    
    # === å¬å›ç‡æµ‹è¯• ===
    print("\n=== å¬å›ç‡å¯¹æ¯”æµ‹è¯• ===")
    k = 10
    n_probe = 5
    
    def evaluate_recall(system, name):
        """ç®€å•çš„å¬å›ç‡è¯„ä¼°"""
        print(f"\nè¯„ä¼° {name}...")
        
        # è®¡ç®—ground truth (ç®€åŒ–ç‰ˆ)
        total_correct = 0
        total_queries = min(10, len(query_vectors))  # åªæµ‹è¯•å‰10ä¸ªæŸ¥è¯¢ä»¥èŠ‚çœæ—¶é—´
        
        for i in range(total_queries):
            query_vector = query_vectors[i]
            
            # Ground truth: æš´åŠ›æœç´¢
            gt_distances = []
            for j, data_vector in enumerate(dataset):
                dist = distance_func(query_vector, data_vector)
                gt_distances.append((dist, j))
            gt_distances.sort()
            gt_neighbors = {node_id for _, node_id in gt_distances[:k]}
            
            # ç³»ç»Ÿæœç´¢ç»“æœ
            results = system.search(query_vector, k=k, n_probe=n_probe)
            found_neighbors = {node_id for node_id, _ in results}
            
            # è®¡ç®—å¬å›ç‡
            correct = len(gt_neighbors & found_neighbors)
            total_correct += correct
            
            if i < 3:  # åªæ‰“å°å‰3ä¸ªæŸ¥è¯¢çš„è¯¦ç»†ä¿¡æ¯
                print(f"  æŸ¥è¯¢ {i}: æ‰¾åˆ° {len(results)} ç»“æœ, {correct}/{k} æ­£ç¡®, å¬å›ç‡={correct/k:.3f}")
        
        overall_recall = total_correct / (total_queries * k)
        print(f"  {name} æ€»ä½“å¬å›ç‡: {overall_recall:.3f}")
        return overall_recall
    
    # è¯„ä¼°ä¸‰ä¸ªç³»ç»Ÿ
    recall_original = evaluate_recall(original_system, "åŸå§‹ç‰ˆæœ¬")
    recall_optimized_clean = evaluate_recall(optimized_system_clean, "ä¼˜åŒ–ç‰ˆæœ¬(æ— ä¼˜åŒ–)")
    recall_optimized_diversify = evaluate_recall(optimized_system_diversify, "ä¼˜åŒ–ç‰ˆæœ¬(diversify=3)")
    
    # === åˆ†æç»“æœ ===
    print("\n" + "="*60)
    print("ğŸ“Š å¯¹æ¯”åˆ†æç»“æœ")
    print("="*60)
    print(f"åŸå§‹ç‰ˆæœ¬å¬å›ç‡:              {recall_original:.3f}")
    print(f"ä¼˜åŒ–ç‰ˆæœ¬(æ— ä¼˜åŒ–)å¬å›ç‡:      {recall_optimized_clean:.3f}")
    print(f"ä¼˜åŒ–ç‰ˆæœ¬(diversify=3)å¬å›ç‡:  {recall_optimized_diversify:.3f}")
    
    print(f"\nå­èŠ‚ç‚¹æ•°é‡å¯¹æ¯”:")
    print(f"åŸå§‹ç‰ˆæœ¬:              {original_stats['num_children']}")
    print(f"ä¼˜åŒ–ç‰ˆæœ¬(æ— ä¼˜åŒ–):      {optimized_stats_clean['num_children']}")
    print(f"ä¼˜åŒ–ç‰ˆæœ¬(diversify=3): {optimized_stats_diversify['num_children']}")
    
    # åˆ†æå·®å¼‚
    if recall_optimized_clean < recall_original:
        print(f"\nâš ï¸  å‘ç°é—®é¢˜: å³ä½¿ä¸å¯ç”¨diversifyï¼Œä¼˜åŒ–ç‰ˆæœ¬å¬å›ç‡ä»ç„¶è¾ƒä½")
        print(f"   å¬å›ç‡å·®å¼‚: {recall_original - recall_optimized_clean:.3f}")
        print(f"   å¯èƒ½åŸå› : å‘é‡è·å–æ–¹å¼å·®å¼‚æˆ–å…¶ä»–å®ç°å·®å¼‚")
    
    if recall_optimized_diversify < recall_optimized_clean:
        print(f"\nâš ï¸  ç¡®è®¤é—®é¢˜: diversifyå‚æ•°æ˜¾è‘—é™ä½äº†å¬å›ç‡")
        print(f"   å¬å›ç‡ä¸‹é™: {recall_optimized_clean - recall_optimized_diversify:.3f}")
        print(f"   å­èŠ‚ç‚¹å‡å°‘: {optimized_stats_clean['num_children'] - optimized_stats_diversify['num_children']}")

if __name__ == "__main__":
    test_single_pivot_comparison()
