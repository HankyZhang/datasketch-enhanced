"""
æ–¹æ³•3å‚æ•°è°ƒä¼˜ï¼šK-Means HNSWç³»ç»Ÿ (Method 3 Parameter Tuning: K-Means HNSW System)

æœ¬æ¨¡å—æä¾›K-Means HNSWç³»ç»Ÿçš„å‚æ•°è°ƒä¼˜å’Œä¼˜åŒ–åŠŸèƒ½ã€‚
åŒ…å«å…¨é¢çš„è¯„ä¼°ã€å‚æ•°æ‰«æå’Œæ€§èƒ½åˆ†æã€‚

åŠŸèƒ½ç‰¹æ€§:
- å…¨é¢çš„å‚æ•°æ‰«æå’Œä¼˜åŒ–
- åŸºå‡†å¯¹æ¯”è¯„ä¼° (HNSWåŸºçº¿ã€çº¯K-Meansã€K-Means HNSW)
- å¬å›ç‡å’ŒæŸ¥è¯¢æ—¶é—´åˆ†æ
- è‡ªé€‚åº”å‚æ•°è°ƒæ•´
- ç»“æœä¿å­˜å’Œåˆ†æ

This module provides parameter tuning and optimization for the K-Means HNSW system.
It includes comprehensive evaluation, parameter sweeps, and performance analysis.
"""

import os
import sys
import time
import json
import argparse
import random
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from itertools import product

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ (Add parent directory to path)
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from method3.kmeans_hnsw import KMeansHNSW
from hnsw.hnsw import HNSW
# ä½¿ç”¨sklearn MiniBatchKMeansä½œä¸ºçº¯k-meansåŸºçº¿ (Switch to sklearn MiniBatchKMeans for pure k-means baseline)
from sklearn.cluster import MiniBatchKMeans


class KMeansHNSWEvaluator:
    """
    K-Means HNSWç³»ç»Ÿæ€§èƒ½å…¨é¢è¯„ä¼°å™¨ (Comprehensive evaluator for K-Means HNSW system performance)
    
    æ­¤ç±»æä¾›äº†å¯¹K-Means HNSWç³»ç»Ÿçš„å…¨é¢è¯„ä¼°åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - çœŸå®å€¼(Ground Truth)è®¡ç®—
    - å¬å›ç‡è¯„ä¼°
    - å‚æ•°æ‰«æå’Œä¼˜åŒ–
    - ä¸åŸºçº¿æ–¹æ³•çš„æ€§èƒ½å¯¹æ¯”
    """
    
    def __init__(
        self, 
        dataset: np.ndarray, 
        query_set: np.ndarray, 
        query_ids: List[int],
        distance_func: callable
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨ (Initialize the evaluator)
        
        Args:
            dataset: å®Œæ•´æ•°æ®é›†å‘é‡ (Full dataset vectors) - shape: [n_vectors, dim]
            query_set: æŸ¥è¯¢å‘é‡ (Query vectors) - shape: [n_queries, dim]  
            query_ids: æŸ¥è¯¢å‘é‡IDåˆ—è¡¨ (IDs for query vectors)
            distance_func: ç”¨äºçœŸå®å€¼è®¡ç®—çš„è·ç¦»å‡½æ•° (Distance function for ground truth computation)
        """
        self.dataset = dataset
        self.query_set = query_set
        self.query_ids = query_ids
        self.distance_func = distance_func
        
        # çœŸå®å€¼ç¼“å­˜ (Ground truth cache)
        self._ground_truth_cache = {}
    
    def compute_ground_truth(
        self, 
        k: int, 
        exclude_query_ids: bool = True
    ) -> Dict[int, List[Tuple[int, float]]]:
        """
        ä½¿ç”¨æš´åŠ›æœç´¢è®¡ç®—çœŸå®å€¼ (Compute ground truth using brute force search)
        
        é€šè¿‡å¯¹æ¯ä¸ªæŸ¥è¯¢å‘é‡è®¡ç®—ä¸æ‰€æœ‰æ•°æ®å‘é‡çš„è·ç¦»ï¼Œæ‰¾å‡ºçœŸæ­£çš„kä¸ªæœ€è¿‘é‚»ã€‚
        è¿™æ˜¯è¯„ä¼°å…¶ä»–ç®—æ³•å¬å›ç‡çš„æ ‡å‡†åŸºå‡†ã€‚
        
        æ³¨æ„ï¼šåœ¨å½“å‰å®ç°ä¸­ï¼ŒæŸ¥è¯¢å‘é‡å’Œæ•°æ®é›†å‘é‡æ˜¯ç‹¬ç«‹ç”Ÿæˆçš„ï¼Œå› æ­¤
        exclude_query_ids å‚æ•°é€šå¸¸åº”è®¾ä¸º Falseï¼Œé™¤éæŸ¥è¯¢å‘é‡æ˜¯ä»æ•°æ®é›†ä¸­é‡‡æ ·çš„ã€‚
        
        Args:
            k: æœ€è¿‘é‚»æ•°é‡ (Number of nearest neighbors)
            exclude_query_ids: æ˜¯å¦ä»ç»“æœä¸­æ’é™¤æŸ¥è¯¢ID (Whether to exclude query IDs from results)
                              ä»…å½“æŸ¥è¯¢å‘é‡æ˜¯æ•°æ®é›†å­é›†æ—¶æ‰æœ‰æ„ä¹‰ (Only meaningful when queries are subset of dataset)
            
        Returns:
            å­—å…¸ï¼šæŸ¥è¯¢ID -> (è·ç¦», æ•°æ®ç´¢å¼•)å…ƒç»„åˆ—è¡¨ (Dictionary mapping query_id to list of (distance, data_index) tuples)
        """
        cache_key = (k, exclude_query_ids)
        if cache_key in self._ground_truth_cache:
            return self._ground_truth_cache[cache_key]
        
        print(f"æ­£åœ¨è®¡ç®— {len(self.query_set)} ä¸ªæŸ¥è¯¢çš„çœŸå®å€¼ (k={k}, exclude_query_ids={exclude_query_ids})...")
        print(f"Computing ground truth for {len(self.query_set)} queries against {len(self.dataset)} data points")
        start_time = time.time()
        
        ground_truth = {}
        excluded_count = 0
        
        for i, (query_vector, query_id) in enumerate(zip(self.query_set, self.query_ids)):
            distances = []
            
            for j, data_vector in enumerate(self.dataset):
                if exclude_query_ids and j == query_id:
                    excluded_count += 1
                    continue  # è·³è¿‡æŸ¥è¯¢æœ¬èº« (Skip the query itself)
                
                distance = self.distance_func(query_vector, data_vector)
                distances.append((distance, j))
            
            # æŒ‰è·ç¦»æ’åºå¹¶å–å‰kä¸ª (Sort by distance and take top-k)
            distances.sort()
            ground_truth[query_id] = distances[:k]
            
            if (i + 1) % 10 == 0:
                print(f"  å·²å¤„ç† {i + 1}/{len(self.query_set)} ä¸ªæŸ¥è¯¢ (Processed {i + 1}/{len(self.query_set)} queries)")
        
        elapsed = time.time() - start_time
        if exclude_query_ids and excluded_count == 0:
            print(f"âš ï¸  è­¦å‘Šï¼šexclude_query_ids=Trueä½†æ²¡æœ‰æ’é™¤ä»»ä½•æ•°æ®ç‚¹ã€‚æŸ¥è¯¢å‘é‡å¯èƒ½ä¸åœ¨æ•°æ®é›†ä¸­ã€‚")
            print(f"   Warning: exclude_query_ids=True but no data points were excluded. Query vectors may not be in dataset.")
        
        print(f"çœŸå®å€¼è®¡ç®—å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}ç§’ï¼Œæ’é™¤äº† {excluded_count} ä¸ªæ•°æ®ç‚¹")
        print(f"Ground truth computed in {elapsed:.2f}s, excluded {excluded_count} data points")
        
        self._ground_truth_cache[cache_key] = ground_truth
        return ground_truth
    
    def evaluate_recall(
        self,
        kmeans_hnsw: KMeansHNSW,
        k: int,
        n_probe: int,
        ground_truth: Optional[Dict] = None,
        exclude_query_ids: bool = True
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°K-Means HNSWç³»ç»Ÿçš„å¬å›ç‡æ€§èƒ½ (Evaluate recall performance of the K-Means HNSW system)
        
        è®¡ç®—ç³»ç»Ÿåœ¨ç»™å®šå‚æ•°ä¸‹çš„å¬å›ç‡ã€æŸ¥è¯¢æ—¶é—´ç­‰æ€§èƒ½æŒ‡æ ‡ã€‚
        å¬å›ç‡ = æ‰¾åˆ°çš„çœŸå®é‚»å±…æ•° / åº”è¯¥æ‰¾åˆ°çš„é‚»å±…æ•°
        
        Args:
            kmeans_hnsw: è¦è¯„ä¼°çš„K-Means HNSWç³»ç»Ÿ (The K-Means HNSW system to evaluate)
            k: è¿”å›ç»“æœæ•°é‡ (Number of results to return)
            n_probe: æ¢æµ‹çš„èšç±»ä¸­å¿ƒæ•°é‡ (Number of centroids to probe)
            ground_truth: é¢„è®¡ç®—çš„çœŸå®å€¼(å¯é€‰) (Precomputed ground truth, optional)
            exclude_query_ids: æ˜¯å¦ä»è¯„ä¼°ä¸­æ’é™¤æŸ¥è¯¢ID (Whether to exclude query IDs from evaluation)
            
        Returns:
            åŒ…å«å¬å›ç‡æŒ‡æ ‡å’Œæ€§èƒ½æ•°æ®çš„å­—å…¸ (Dictionary containing recall metrics and performance data)
        """
        if ground_truth is None:
            ground_truth = self.compute_ground_truth(k, exclude_query_ids)
        
        print(f"æ­£åœ¨è¯„ä¼° {len(self.query_set)} ä¸ªæŸ¥è¯¢çš„å¬å›ç‡ (k={k}, n_probe={n_probe})... (Evaluating recall)")
        start_time = time.time()
        
        total_correct = 0
        total_expected = len(self.query_set) * k
        query_times = []
        individual_recalls = []
        
        for i, (query_vector, query_id) in enumerate(zip(self.query_set, self.query_ids)):
            # Get ground truth for this query
            true_neighbors = {neighbor_id for _, neighbor_id in ground_truth[query_id]}
            
            # Search using K-Means HNSW
            search_start = time.time()
            results = kmeans_hnsw.search(query_vector, k=k, n_probe=n_probe)
            search_time = time.time() - search_start
            query_times.append(search_time)
            
            # Count correct results
            found_neighbors = {node_id for node_id, _ in results}
            correct = len(true_neighbors & found_neighbors)
            total_correct += correct
            
            # Individual recall for this query
            individual_recall = correct / k if k > 0 else 0.0
            individual_recalls.append(individual_recall)
            
            if (i + 1) % 20 == 0:
                current_recall = total_correct / ((i + 1) * k)
                print(f"  Processed {i + 1}/{len(self.query_set)} queries, "
                      f"current recall: {current_recall:.4f}")
        
        # Calculate final metrics
        overall_recall = total_correct / total_expected
        avg_query_time = np.mean(query_times)
        std_query_time = np.std(query_times)
        total_evaluation_time = time.time() - start_time
        
        return {
            'recall_at_k': overall_recall,
            'total_correct': total_correct,
            'total_expected': total_expected,
            'individual_recalls': individual_recalls,
            'avg_individual_recall': np.mean(individual_recalls),
            'std_individual_recall': np.std(individual_recalls),
            'avg_query_time_ms': avg_query_time * 1000,
            'std_query_time_ms': std_query_time * 1000,
            'total_evaluation_time': total_evaluation_time,
            'k': k,
            'n_probe': n_probe,
            'system_stats': kmeans_hnsw.get_stats()
        }

    # -------------------- Phase-Specific Evaluations --------------------
    def evaluate_hnsw_baseline(
        self,
        base_index: HNSW,
        k: int,
        ef: int,
        ground_truth: Dict
    ) -> Dict[str, Any]:
        """Evaluate recall using ONLY the base HNSW index (Phase 1)."""
        query_times = []
        total_correct = 0
        total_expected = len(self.query_set) * k
        individual_recalls = []
        
        print(f"ğŸ” è¯„ä¼°HNSWåŸºçº¿æ€§èƒ½ (k={k}, ef={ef})...")
        
        for query_vector, query_id in zip(self.query_set, self.query_ids):
            # Ground truth format: {query_id: [(distance, node_id), ...]}
            # Extract the node_ids (which are data indices) from ground truth
            true_neighbors = {node_id for _, node_id in ground_truth[query_id]}
            
            t0 = time.time()
            results = base_index.query(query_vector, k=k, ef=ef)
            dt = time.time() - t0
            query_times.append(dt)
            
            # HNSW query returns [(node_id, distance), ...]
            found = {nid for nid, _ in results}
            correct = len(true_neighbors & found)
            total_correct += correct
            
            # Individual recall for this query
            individual_recall = correct / k if k > 0 else 0.0
            individual_recalls.append(individual_recall)
            
            # Debug info for first few queries
            if query_id < 3:
                print(f"  Query {query_id}: found {len(found)} results, {correct}/{k} correct, recall={individual_recall:.4f}")
                print(f"    True neighbors (first 5): {list(true_neighbors)[:5]}")
                print(f"    Found neighbors (first 5): {list(found)[:5]}")
        
        avg_recall = np.mean(individual_recalls)
        print(f"  HNSWåŸºçº¿å¬å›ç‡: {avg_recall:.4f} (æ€»è®¡ {total_correct}/{total_expected})")
        
        return {
            'phase': 'baseline_hnsw',
            'ef': ef,
            'recall_at_k': total_correct / total_expected if total_expected else 0.0,
            'avg_query_time_ms': float(np.mean(query_times) * 1000),
            'std_query_time_ms': float(np.std(query_times) * 1000),
            'total_correct': total_correct,
            'total_expected': total_expected,
            'individual_recalls': individual_recalls,
            'avg_individual_recall': float(np.mean(individual_recalls)),
            'std_individual_recall': float(np.std(individual_recalls))
        }


    def parameter_sweep(
        self,
        base_index: HNSW,
        param_grid: Dict[str, List[Any]],
        evaluation_params: Dict[str, Any],
        max_combinations: Optional[int] = None,
        adaptive_config: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œå…¨é¢çš„å‚æ•°æ‰«æä¼˜åŒ– (Perform comprehensive parameter sweep for optimization)
        
        é€šè¿‡ç³»ç»Ÿæ€§åœ°æµ‹è¯•ä¸åŒå‚æ•°ç»„åˆï¼Œæ‰¾åˆ°æœ€ä¼˜çš„K-Means HNSWé…ç½®ã€‚
        åŒ…æ‹¬åŸºçº¿HNSWã€çº¯K-Meanså’ŒK-Means HNSWçš„å¯¹æ¯”è¯„ä¼°ã€‚
        
        Args:
            base_index: ä½¿ç”¨çš„åŸºç¡€HNSWç´¢å¼• (Base HNSW index to use)
            param_grid: å‚æ•°åŠå…¶æµ‹è¯•å€¼çš„å­—å…¸ (Dictionary of parameters and their values to test)
            evaluation_params: è¯„ä¼°å‚æ•° (Parameters for evaluation) - kå€¼, n_probeå€¼ç­‰
            max_combinations: æœ€å¤§æµ‹è¯•ç»„åˆæ•° (Maximum number of combinations to test)
            adaptive_config: è‡ªé€‚åº”é…ç½®å‚æ•° (Adaptive configuration parameters, optional)
            
        Returns:
            æ¯ä¸ªå‚æ•°ç»„åˆçš„è¯„ä¼°ç»“æœåˆ—è¡¨ (List of evaluation results for each parameter combination)
        """
        # è®¾ç½®é»˜è®¤çš„è‡ªé€‚åº”é…ç½® (Set default adaptive configuration)
        if adaptive_config is None:
            adaptive_config = {
                'adaptive_k_children': False,
                'k_children_scale': 1.5,
                'k_children_min': 100,
                'k_children_max': None,
                'diversify_max_assignments': None,
                'repair_min_assignments': None
            }
        
        print("å¼€å§‹K-Means HNSWå‚æ•°æ‰«æ... (Starting parameter sweep for K-Means HNSW)")
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ (Generate all parameter combinations)
        param_names = list(param_grid.keys())
        param_values = list(param_grid.values())
        combinations = list(product(*param_values))
        
        if max_combinations and len(combinations) > max_combinations:
            print(f"é™åˆ¶æµ‹è¯• {max_combinations} ä¸ªç»„åˆï¼Œæ€»å…± {len(combinations)} ä¸ª (Limiting to {max_combinations} combinations out of {len(combinations)})")
            combinations = random.sample(combinations, max_combinations)
        
        print(f"æµ‹è¯• {len(combinations)} ä¸ªå‚æ•°ç»„åˆ... (Testing {len(combinations)} parameter combinations)")
        
        results = []
        k_values = evaluation_params.get('k_values', [10])
        n_probe_values = evaluation_params.get('n_probe_values', [5, 10, 20])
        
        # é¢„è®¡ç®—æ‰€æœ‰kå€¼çš„çœŸå®å€¼ (Precompute ground truth for all k values)
        ground_truths = {}
        for k in k_values:
            # æŸ¥è¯¢å‘é‡é»˜è®¤å¹¶éç›´æ¥æºè‡ª base_index ç›¸åŒ id çš„å‘é‡ï¼Œå› æ­¤ä¸æ’é™¤åŒ id
            ground_truths[k] = self.compute_ground_truth(k, exclude_query_ids=False)

        for i, combination in enumerate(combinations):
            print(f"\n--- Combination {i + 1}/{len(combinations)} ---")

            # Create parameter dictionary
            params = dict(zip(param_names, combination))
            print(f"Parameters: {params}")

            try:
                phase_records = []
                
                # Phase 2: æ„å»ºå®Œæ•´çš„K-Means HNSWç³»ç»Ÿ (Build full K-Means HNSW system)
                # å…ˆæ„å»ºç³»ç»Ÿä»¥è·å–å®é™…ä½¿ç”¨çš„å‚æ•°
                construction_start = time.time()
                kmeans_hnsw = KMeansHNSW(
                    base_index=base_index,
                    **params,
                    adaptive_k_children=adaptive_config['adaptive_k_children'],
                    k_children_scale=adaptive_config['k_children_scale'],
                    k_children_min=adaptive_config['k_children_min'],
                    k_children_max=adaptive_config['k_children_max'],
                    diversify_max_assignments=adaptive_config['diversify_max_assignments'],
                    repair_min_assignments=adaptive_config['repair_min_assignments']
                )
                construction_time = time.time() - construction_start
                print(f"  æ„å»ºK-Means HNSWç³»ç»Ÿè€—æ—¶ {construction_time:.2f}ç§’ (Built KMeansHNSW system in {construction_time:.2f}s)")
                
                # ä»æ„å»ºçš„ç³»ç»Ÿä¸­æå–å®é™…ä½¿ç”¨çš„å‚æ•°ä»¥ç¡®ä¿ä¸€è‡´æ€§
                actual_n_clusters = kmeans_hnsw.n_clusters

                
                # Phase 1: åŸºçº¿HNSWè¯„ä¼° - ä½¿ç”¨base_indexçš„ef_constructionå‚æ•°
                base_ef = base_index._ef_construction
                print(f"  ä½¿ç”¨base_indexçš„ef_constructionå‚æ•°: {base_ef}")
                for k in k_values:
                    b_eval = self.evaluate_hnsw_baseline(base_index, k, base_ef, ground_truths[k])
                    phase_records.append({**b_eval, 'k': k})
                    print(f"  [åŸºçº¿HNSW/Baseline HNSW] k={k} ef={base_ef} recall={b_eval['recall_at_k']:.4f} avg_time={b_eval['avg_query_time_ms']:.2f}ms")

                # Phase 3: K-Meansèšç±»å•ç‹¬è¯„ä¼° - ä½¿ç”¨ä¸KMeansHNSWç›¸åŒçš„èšç±»å‚æ•°å’Œn_probeå€¼
                print(f"  ä½¿ç”¨ä¸KMeansHNSWç›¸åŒçš„èšç±»å‚æ•°: n_clusters={actual_n_clusters}")
                for k in k_values:
                    for n_probe in n_probe_values:
                        c_eval = self._evaluate_pure_kmeans_from_existing(
                            kmeans_hnsw,  # ç›´æ¥ä¼ é€’KMeansHNSWå¯¹è±¡
                            k,
                            ground_truths[k],
                            n_probe=n_probe
                        )
                        # æ·»åŠ phaseæ ‡è¯†ä»¥ä¾¿ä¸å…¶ä»–é˜¶æ®µåŒºåˆ†
                        c_eval['phase'] = 'clusters_only'
                        phase_records.append({**c_eval, 'k': k})
                        print(f"  [ä»…K-Meansèšç±»/Clusters Only] k={k} n_probe={n_probe} recall={c_eval['recall_at_k']:.4f} avg_time={c_eval['avg_query_time_ms']:.2f}ms")

                # Phase 4: K-Means HNSWæ··åˆç³»ç»Ÿå®Œæ•´è¯„ä¼° (Full K-Means HNSW hybrid system evaluation)
                for k in k_values:
                    for n_probe in n_probe_values:
                        eval_result = self.evaluate_recall(kmeans_hnsw, k, n_probe, ground_truths[k])
                        phase_records.append({**eval_result, 'phase': 'kmeans_hnsw_hybrid'})
                        print(f"  [K-Means HNSWæ··åˆ/Hybrid] k={k} n_probe={n_probe} recall={eval_result['recall_at_k']:.4f} avg_time={eval_result['avg_query_time_ms']:.2f}ms")

                # æ”¶é›†æ­¤å‚æ•°ç»„åˆçš„æ‰€æœ‰è¯„ä¼°ç»“æœ (Collect all evaluation results for this parameter combination)
                combination_results = {
                    'parameters': params,
                    'construction_time': construction_time,
                    'phase_evaluations': phase_records
                }
                results.append(combination_results)
                
                # æ˜¾ç¤ºæ­¤ç»„åˆçš„æœ€ä½³å¬å›ç‡ (Show best recall for this combination)
                best_recall = max(r['recall_at_k'] for r in phase_records if 'recall_at_k' in r)
                print(f"  æ­¤ç»„åˆæœ€ä½³å¬å›ç‡ (Best recall): {best_recall:.4f}")
                
            except Exception as e:
                print(f"âŒ å‚æ•°ç»„åˆ {params} å‡ºé”™: {e} (Error with combination)")
                continue
        
        print(f"\nğŸ¯ å‚æ•°æ‰«æå®Œæˆï¼æµ‹è¯•äº† {len(results)} ä¸ªç»„åˆ (Parameter sweep completed. Tested {len(results)} combinations)")
        return results
    
    def _evaluate_pure_kmeans_from_existing(
        self, 
        kmeans_hnsw: KMeansHNSW, 
        k: int, 
        ground_truth: Dict, 
        n_probe: int = 1
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨KMeansHNSWå†…éƒ¨å·²æœ‰çš„èšç±»ç»“æœè¯„ä¼°çº¯K-Meansæ€§èƒ½
        é¿å…é‡å¤èšç±»ï¼Œç›´æ¥å¤ç”¨å·²è®­ç»ƒçš„æ¨¡å‹å’Œæ•°æ®æ˜ å°„
        
        Evaluate pure K-Means using existing clustering results from KMeansHNSW.
        Avoids redundant clustering by reusing trained model and data mappings.
        """
        print(f"Reusing existing clustering from KMeansHNSW (n_clusters={kmeans_hnsw.n_clusters}, n_probe={n_probe})...")
        
        # ç›´æ¥ä½¿ç”¨KMeansHNSWå†…éƒ¨çš„èšç±»ç»“æœ
        kmeans_model = kmeans_hnsw.kmeans_model
        centers = kmeans_model.cluster_centers_
        n_clusters = centers.shape[0]
        
        # è·å–ä¸èšç±»æ—¶ç›¸åŒçš„æ•°æ®é›†å’Œç´¢å¼•æ˜ å°„
        kmeans_dataset = kmeans_hnsw._extract_dataset_vectors()
        labels = kmeans_model.labels_
        
        # æ„å»ºèšç±»åˆ°æˆå‘˜çš„æ˜ å°„ (ä¸evaluate_clusters_onlyç±»ä¼¼çš„é€»è¾‘)
        clusters = [[] for _ in range(n_clusters)]
        dataset_idx_to_original_id = list(kmeans_hnsw.base_index.keys())
        
        for dataset_idx, cluster_id in enumerate(labels):
            original_id = dataset_idx_to_original_id[dataset_idx]
            clusters[cluster_id].append((dataset_idx, original_id))
        
        query_times = []
        total_correct = 0
        total_expected = len(self.query_set) * k
        individual_recalls = []
        
        # Cap n_probe to number of clusters
        n_probe_eff = min(n_probe, n_clusters)
        
        for query_vector, query_id in zip(self.query_set, self.query_ids):
            search_start = time.time()
            
            # è®¡ç®—åˆ°èšç±»ä¸­å¿ƒçš„è·ç¦»
            diffs = centers - query_vector
            distances_to_centroids = np.linalg.norm(diffs, axis=1)
            
            # è·å–æœ€è¿‘çš„n_probeä¸ªèšç±»ä¸­å¿ƒ
            probe_centroids = np.argpartition(distances_to_centroids, n_probe_eff - 1)[:n_probe_eff]
            probe_centroids = probe_centroids[np.argsort(distances_to_centroids[probe_centroids])]
            
            # æ”¶é›†æ‰€æœ‰è¢«æ¢æµ‹èšç±»çš„æˆå‘˜
            all_candidates = []
            for cluster_idx in probe_centroids:
                cluster_members = clusters[cluster_idx]
                for dataset_idx, original_id in cluster_members:
                    if original_id != query_id:  # æ’é™¤æŸ¥è¯¢æœ¬èº«
                        member_vec = kmeans_dataset[dataset_idx]
                        dist = np.linalg.norm(member_vec - query_vector)
                        all_candidates.append((dist, original_id))
            
            # æŒ‰è·ç¦»æ’åºå¹¶å–top-k
            all_candidates.sort(key=lambda x: x[0])
            results = all_candidates[:k]
            found_neighbors = {original_id for _, original_id in results}
            
            search_time = time.time() - search_start
            query_times.append(search_time)
            
            # è®¡ç®—å¬å›ç‡
            true_neighbors = {neighbor_id for _, neighbor_id in ground_truth[query_id]}
            correct = len(true_neighbors & found_neighbors)
            total_correct += correct
            individual_recalls.append(correct / k if k > 0 else 0.0)
        
        overall_recall = total_correct / total_expected if total_expected > 0 else 0.0
        
        return {
            'method': 'pure_kmeans_from_existing',
            'recall_at_k': overall_recall,
            'total_correct': total_correct,
            'total_expected': total_expected,
            'individual_recalls': individual_recalls,
            'avg_individual_recall': float(np.mean(individual_recalls)),
            'std_individual_recall': float(np.std(individual_recalls)),
            'avg_query_time_ms': float(np.mean(query_times) * 1000),
            'std_query_time_ms': float(np.std(query_times) * 1000),
            'clustering_time': 0.0,  # æ²¡æœ‰é‡æ–°èšç±»ï¼Œæ—¶é—´ä¸º0
            'n_clusters': n_clusters,
            'n_probe': n_probe_eff,
            'k': k,
            'reused_existing_clustering': True
        }


def save_results(results: Dict[str, Any], filename: str):
    """Save evaluation results to JSON file."""
    # Convert numpy arrays to lists for JSON serialization
    def convert_numpy(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, dict):
            return {k: convert_numpy(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy(item) for item in obj]
        return obj
    
    results_serializable = convert_numpy(results)
    
    with open(filename, 'w') as f:
        json.dump(results_serializable, f, indent=2)
    
    print(f"Results saved to {filename}")


def load_sift_data():
    """
    åŠ è½½SIFTæ•°æ®é›†ç”¨äºè¯„ä¼° (Load SIFT dataset for evaluation)
    
    SIFT (Scale-Invariant Feature Transform) æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ç»å…¸ç‰¹å¾æè¿°ç¬¦ã€‚
    è¯¥æ•°æ®é›†åŒ…å«100ä¸‡ä¸ª128ç»´çš„ç‰¹å¾å‘é‡ï¼Œå¸¸ç”¨äºç›¸ä¼¼æ€§æœç´¢ç®—æ³•çš„åŸºå‡†æµ‹è¯•ã€‚
    
    Returns:
        tuple: (base_vectors, query_vectors) æˆ– (None, None) å¦‚æœåŠ è½½å¤±è´¥
    """
    sift_dir = os.path.join(os.path.dirname(__file__), '..', 'sift')
    
    try:
        def read_fvecs(path: str, max_vectors: Optional[int] = None) -> np.ndarray:
            """
            è¯»å–.fvecsæ–‡ä»¶ (FAISSæ ¼å¼)ã€‚æ¯ä¸ªå‘é‡å­˜å‚¨ä¸ºï¼šint32ç»´åº¦ + ç»´åº¦ä¸ªfloat32å€¼ã€‚
            æ­¤å®ç°é€šè¿‡å…ˆè¯»å–int32å¤´éƒ¨æ¥é¿å…è§£æé”™è¯¯ã€‚
            
            Read .fvecs file (FAISS format). Each vector stored as: int32 dim + dim float32.
            This implementation avoids mis-parsing by reading int32 header first.
            """
            if not os.path.exists(path):
                raise FileNotFoundError(path)
            raw = np.fromfile(path, dtype=np.int32)
            if raw.size == 0:
                raise ValueError(f"ç©ºçš„fvecsæ–‡ä»¶: {path} (Empty fvecs file)")
            dim = raw[0]
            if dim <= 0 or dim > 4096:
                raise ValueError(f"ä¸åˆç†çš„å‘é‡ç»´åº¦ {dim}ï¼Œè§£æè‡ª {path} (Unreasonable vector dimension)")
            record_size = dim + 1
            count = raw.size // record_size
            raw = raw.reshape(count, record_size)
            vecs = raw[:, 1:].astype(np.float32)
            if max_vectors is not None and count > max_vectors:
                vecs = vecs[:max_vectors]
            return vecs

        base_path = os.path.join(sift_dir, 'sift_base.fvecs')
        query_path = os.path.join(sift_dir, 'sift_query.fvecs')

        # ä¸ºè°ƒä¼˜æ¼”ç¤ºé™åˆ¶æ•°é‡ä»¥ä¿æŒåˆç†çš„è¿è¡Œæ—¶é—´ (Limit for tuning demo to keep runtime reasonable)
        base_vectors = read_fvecs(base_path, max_vectors=50000)
        query_vectors = read_fvecs(query_path, max_vectors=1000)

        print(f"å·²åŠ è½½SIFTæ•°æ®: {base_vectors.shape[0]} ä¸ªåŸºç¡€å‘é‡, "
              f"{query_vectors.shape[0]} ä¸ªæŸ¥è¯¢å‘é‡, ç»´åº¦ {base_vectors.shape[1]}")
        print(f"Loaded SIFT data: {base_vectors.shape[0]} base vectors, "
              f"{query_vectors.shape[0]} query vectors, dimension {base_vectors.shape[1]}")

        return base_vectors, query_vectors
    
    except Exception as e:
        print(f"åŠ è½½SIFTæ•°æ®æ—¶å‡ºé”™: {e} (Error loading SIFT data)")
        print("æ”¹ç”¨åˆæˆæ•°æ®... (Using synthetic data instead)")
        return None, None


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="K-Means HNSWå‚æ•°è°ƒä¼˜å’Œè¯„ä¼° (K-Means HNSW Parameter Tuning and Evaluation)")
    
    # æ•°æ®é›†é€‰é¡¹ (Dataset options)
    parser.add_argument('--dataset-size', type=int, default=10000, 
                        help='ä½¿ç”¨çš„åŸºç¡€å‘é‡æ•°é‡ (é»˜è®¤: 10000) (Number of base vectors to use)')
    parser.add_argument('--query-size', type=int, default=50, 
                        help='ä½¿ç”¨çš„æŸ¥è¯¢å‘é‡æ•°é‡ (é»˜è®¤: 50) (Number of query vectors to use)')
    parser.add_argument('--dimension', type=int, default=128, 
                        help='åˆæˆæ•°æ®çš„å‘é‡ç»´åº¦ (å¦‚æœæœªåŠ è½½SIFT) (Vector dimensionality for synthetic data)')
    parser.add_argument('--no-sift', action='store_true', 
                        help='å¼ºåˆ¶ä½¿ç”¨åˆæˆæ•°æ®ï¼Œå³ä½¿SIFTæ–‡ä»¶å­˜åœ¨ (Force synthetic data even if SIFT files exist)')
    
    # è‡ªé€‚åº”/å¤šæ ·åŒ–/ä¿®å¤é€‰é¡¹ (Adaptive/diversification/repair options)
    parser.add_argument('--adaptive-k-children', action='store_true', 
                        help='å¯ç”¨åŸºäºå¹³å‡èšç±»å¤§å°çš„è‡ªé€‚åº”k_children (Enable adaptive k_children based on avg cluster size)')
    parser.add_argument('--k-children-scale', type=float, default=1.5, 
                        help='è‡ªé€‚åº”k_childrençš„ç¼©æ”¾å› å­ (é»˜è®¤1.5) (Scale factor for adaptive k_children)')
    parser.add_argument('--k-children-min', type=int, default=100, 
                        help='è‡ªé€‚åº”æ—¶çš„æœ€å°k_children (Minimum k_children when adaptive)')
    parser.add_argument('--k-children-max', type=int, default=None, 
                        help='è‡ªé€‚åº”æ—¶çš„æœ€å¤§k_children (å¯é€‰) (Maximum k_children when adaptive)')
    parser.add_argument('--diversify-max-assignments', type=int, default=None, 
                        help='æ¯ä¸ªå­èŠ‚ç‚¹çš„æœ€å¤§åˆ†é…æ•° (å¯ç”¨å¤šæ ·åŒ–) (Max assignments per child - enable diversification)')
    parser.add_argument('--repair-min-assignments', type=int, default=None, 
                        help='æ„å»ºä¿®å¤æœŸé—´æ¯ä¸ªå­èŠ‚ç‚¹çš„æœ€å°åˆ†é…æ•° (éœ€è¦å¤šæ ·åŒ–) (Min assignments per child during build repair)')
    parser.add_argument('--manual-repair', action='store_true', 
                        help='åœ¨æœ€ä¼˜æ„å»ºåè¿è¡Œæ‰‹åŠ¨ä¿®å¤ (Run manual repair after optimal build)')
    parser.add_argument('--manual-repair-min', type=int, default=None, 
                        help='æ‰‹åŠ¨ä¿®å¤çš„æœ€å°åˆ†é…æ•° (Min assignments for manual repair)')
    args = parser.parse_args()

    print("ğŸ”¬ K-Means HNSWå‚æ•°è°ƒä¼˜å’Œè¯„ä¼°ç³»ç»Ÿ (K-Means HNSW Parameter Tuning and Evaluation)")
    print(f"ğŸ“Š è¯·æ±‚çš„æ•°æ®é›†å¤§å°: {args.dataset_size}, æŸ¥è¯¢å¤§å°: {args.query_size}")
    print(f"   Requested dataset size: {args.dataset_size}, query size: {args.query_size}")
    
    # å°è¯•åŠ è½½SIFTæ•°æ®ï¼Œå¤±è´¥åˆ™ä½¿ç”¨åˆæˆæ•°æ® (Try to load SIFT data, fall back to synthetic unless disabled)
    base_vectors, query_vectors = (None, None)
    if not args.no_sift:
        base_vectors, query_vectors = load_sift_data()
    
    if base_vectors is None:
        # åˆ›å»ºåˆæˆæ•°æ® (Create synthetic data)
        print("ğŸ² åˆ›å»ºåˆæˆæ•°æ®é›†... (Creating synthetic dataset)")
        base_vectors = np.random.randn(max(args.dataset_size, 10000), args.dimension).astype(np.float32)
        query_vectors = np.random.randn(max(args.query_size, 100), args.dimension).astype(np.float32)
    
    # åˆ‡ç‰‡åˆ°è¯·æ±‚çš„å¤§å° (æŒ‰å¯ç”¨é‡é™åˆ¶) (Slice to requested sizes)
    if len(base_vectors) > args.dataset_size:
        base_vectors = base_vectors[:args.dataset_size]
    if len(query_vectors) > args.query_size:
        query_vectors = query_vectors[:args.query_size]
    print(f"ğŸ“ˆ ä½¿ç”¨åŸºç¡€å‘é‡: {len(base_vectors)} | æŸ¥è¯¢: {len(query_vectors)} | ç»´åº¦: {base_vectors.shape[1]}")
    print(f"   Using base vectors: {len(base_vectors)} | queries: {len(query_vectors)} | dim: {base_vectors.shape[1]}")
    query_ids = list(range(len(query_vectors)))
    
    # è·ç¦»å‡½æ•° (Distance function)
    distance_func = lambda x, y: np.linalg.norm(x - y)
    
    # æ„å»ºåŸºç¡€HNSWç´¢å¼• (Build base HNSW index)
    print("ğŸ—ï¸  æ„å»ºåŸºç¡€HNSWç´¢å¼•... (Building base HNSW index)")
    # åŸºçº¿ HNSW ef å›ºå®šä¸º 200ï¼ˆç”¨æˆ·æŒ‡å®šé€»è¾‘ï¼‰
    base_index = HNSW(distance_func=distance_func, m=16, ef_construction=200)
    
    for i, vector in enumerate(base_vectors):
        base_index.insert(i, vector)
        if (i + 1) % 1000 == 0:
            print(f"  Inserted {i + 1}/{len(base_vectors)} vectors")
    
    print(f"Base HNSW index built with {len(base_index)} vectors")
    
    # Initialize evaluator
    evaluator = KMeansHNSWEvaluator(base_vectors, query_vectors, query_ids, distance_func)
    
    # Define parameter grid for sweep
    # Adjust default cluster count heuristics for larger datasets: scale choices
    if args.dataset_size <= 2000:
        cluster_options = [10]
    elif args.dataset_size <= 5000:
        cluster_options = [16, 32]
    else:
        cluster_options = [32, 64, 128]

    param_grid = {
        'n_clusters': cluster_options,
        'k_children': [200],
        'child_search_ef': [300]
    }
    
    evaluation_params = {
        'k_values': [10],
        'n_probe_values': [5, 10, 20]
    }
    
    # Perform parameter sweep
    print("\nStarting parameter sweep...")
    # Limit combinations to keep runtime sane on large sets
    max_combos = 9 if len(cluster_options) > 1 else None
    
    # å‡†å¤‡è‡ªé€‚åº”é…ç½® (Prepare adaptive configuration)
    adaptive_config = {
        'adaptive_k_children': args.adaptive_k_children,
        'k_children_scale': args.k_children_scale,
        'k_children_min': args.k_children_min,
        'k_children_max': args.k_children_max,
        'diversify_max_assignments': args.diversify_max_assignments,
        'repair_min_assignments': args.repair_min_assignments
    }
    
    sweep_results = evaluator.parameter_sweep(
        base_index,
        param_grid,
        evaluation_params,
        max_combinations=max_combos,
        adaptive_config=adaptive_config
    )
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªå‚æ•°ç»„åˆè¿›è¡Œæ¼”ç¤º (Use first parameter combination for demonstration)
    if sweep_results:
        # å–ç¬¬ä¸€ä¸ªæ‰«æç»“æœä½œä¸ºæ¼”ç¤ºå‚æ•°
        demo_result = sweep_results[0]
        demo_params = demo_result['parameters']
        print(f"\nUsing first parameter combination for demonstration: {demo_params}")
        
        print("\nğŸ¯ Parameter sweep completed! All comparisons are available in sweep_results.")

        # Save results
        results = {
            'sweep_results': sweep_results,
            'demo_parameters': demo_params,
            'adaptive_config': {
                'adaptive_k_children': args.adaptive_k_children,
                'k_children_scale': args.k_children_scale,
                'k_children_min': args.k_children_min,
                'k_children_max': args.k_children_max,
                'diversify_max_assignments': args.diversify_max_assignments,
                'repair_min_assignments': args.repair_min_assignments,
                'manual_repair': args.manual_repair,
                'manual_repair_min': args.manual_repair_min
            },
            'evaluation_info': {
                'dataset_size': len(base_vectors),
                'query_size': len(query_vectors),
                'dimension': base_vectors.shape[1],
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
        }
        
        save_results(results, 'method3_tuning_results.json')
        
    print("\nParameter tuning completed!")
