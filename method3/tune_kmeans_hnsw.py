"""
æ–¹æ³•3å‚æ•°è°ƒä¼˜ï¼šK-Means HNSWç³»ç»Ÿ (Method 3 Parameter Tuning: K-Means HNSW System)

æœ¬æ¨¡å—æä¾›K-Means HNSWç³»ç»Ÿçš„å‚æ•°è°ƒä¼˜å’Œä¼˜åŒ–åŠŸèƒ½ã€‚
åŒ…å«å…¨é¢çš„è¯„ä¼°ã€å‚æ•°æ‰«æå’Œæ€§èƒ½åˆ†æã€‚

åŠŸèƒ½ç‰¹æ€§:
- å…¨é¢çš„å‚æ•°æ‰«æå’Œä¼˜åŒ–
- åŸºå‡†å¯¹æ¯”è¯„ä¼° (HNSWåŸºçº¿ã€çº¯K-Meansã€K-Means HNSW)
- å¬å›ç‡å’ŒæŸ¥è¯¢æ—¶é—´åˆ†æ
- è‡ªé€‚åº”å‚æ•°è°ƒæ•´
- ç»“æœä¿å­˜å’Œåˆ†æ

This module provides parameter tuning and optimization for the K-Means HNSW system.
It includes comprehensive evaluation, parameter sweeps, and performance analysis.
"""

import os
import sys
import time
import json
import argparse
import random
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from itertools import product

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ (Add parent directory to path)
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from method3.kmeans_hnsw import KMeansHNSW
from hnsw.hnsw import HNSW
# ä½¿ç”¨sklearn MiniBatchKMeansä½œä¸ºçº¯k-meansåŸºçº¿ (Switch to sklearn MiniBatchKMeans for pure k-means baseline)
from sklearn.cluster import MiniBatchKMeans


class KMeansHNSWEvaluator:
    """
    K-Means HNSWç³»ç»Ÿæ€§èƒ½å…¨é¢è¯„ä¼°å™¨ (Comprehensive evaluator for K-Means HNSW system performance)
    
    æ­¤ç±»æä¾›äº†å¯¹K-Means HNSWç³»ç»Ÿçš„å…¨é¢è¯„ä¼°åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - çœŸå®å€¼(Ground Truth)è®¡ç®—
    - å¬å›ç‡è¯„ä¼°
    - å‚æ•°æ‰«æå’Œä¼˜åŒ–
    - ä¸åŸºçº¿æ–¹æ³•çš„æ€§èƒ½å¯¹æ¯”
    
    å…¼å®¹æ–¹æ³•1å’Œæ–¹æ³•2çš„ç°æœ‰è¯„ä¼°æ¡†æ¶ã€‚
    Compatible with existing evaluation frameworks from Methods 1 & 2.
    """
    
    def __init__(
        self, 
        dataset: np.ndarray, 
        query_set: np.ndarray, 
        query_ids: List[int],
        distance_func: callable
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨ (Initialize the evaluator)
        
        Args:
            dataset: å®Œæ•´æ•°æ®é›†å‘é‡ (Full dataset vectors) - shape: [n_vectors, dim]
            query_set: æŸ¥è¯¢å‘é‡ (Query vectors) - shape: [n_queries, dim]  
            query_ids: æŸ¥è¯¢å‘é‡IDåˆ—è¡¨ (IDs for query vectors)
            distance_func: ç”¨äºçœŸå®å€¼è®¡ç®—çš„è·ç¦»å‡½æ•° (Distance function for ground truth computation)
        """
        self.dataset = dataset
        self.query_set = query_set
        self.query_ids = query_ids
        self.distance_func = distance_func
        
        # çœŸå®å€¼ç¼“å­˜ (Ground truth cache)
        self._ground_truth_cache = {}
    
    def compute_ground_truth(
        self, 
        k: int, 
        exclude_query_ids: bool = True
    ) -> Dict[int, List[Tuple[int, float]]]:
        """
        ä½¿ç”¨æš´åŠ›æœç´¢è®¡ç®—çœŸå®å€¼ (Compute ground truth using brute force search)
        
        é€šè¿‡å¯¹æ¯ä¸ªæŸ¥è¯¢å‘é‡è®¡ç®—ä¸æ‰€æœ‰æ•°æ®å‘é‡çš„è·ç¦»ï¼Œæ‰¾å‡ºçœŸæ­£çš„kä¸ªæœ€è¿‘é‚»ã€‚
        è¿™æ˜¯è¯„ä¼°å…¶ä»–ç®—æ³•å¬å›ç‡çš„æ ‡å‡†åŸºå‡†ã€‚
        
        Args:
            k: æœ€è¿‘é‚»æ•°é‡ (Number of nearest neighbors)
            exclude_query_ids: æ˜¯å¦ä»ç»“æœä¸­æ’é™¤æŸ¥è¯¢ID (Whether to exclude query IDs from results)
            
        Returns:
            å­—å…¸ï¼šæŸ¥è¯¢ID -> (é‚»å±…ID, è·ç¦»)å…ƒç»„åˆ—è¡¨ (Dictionary mapping query_id to list of (neighbor_id, distance) tuples)
        """
        cache_key = (k, exclude_query_ids)
        if cache_key in self._ground_truth_cache:
            return self._ground_truth_cache[cache_key]
        
        print(f"æ­£åœ¨è®¡ç®— {len(self.query_set)} ä¸ªæŸ¥è¯¢çš„çœŸå®å€¼ (k={k})... (Computing ground truth for {len(self.query_set)} queries)")
        start_time = time.time()
        
        ground_truth = {}
        
        for i, (query_vector, query_id) in enumerate(zip(self.query_set, self.query_ids)):
            distances = []
            
            for j, data_vector in enumerate(self.dataset):
                if exclude_query_ids and j == query_id:
                    continue  # è·³è¿‡æŸ¥è¯¢æœ¬èº« (Skip the query itself)
                
                distance = self.distance_func(query_vector, data_vector)
                distances.append((distance, j))
            
            # æŒ‰è·ç¦»æ’åºå¹¶å–å‰kä¸ª (Sort by distance and take top-k)
            distances.sort()
            ground_truth[query_id] = distances[:k]
            
            if (i + 1) % 10 == 0:
                print(f"  å·²å¤„ç† {i + 1}/{len(self.query_set)} ä¸ªæŸ¥è¯¢ (Processed {i + 1}/{len(self.query_set)} queries)")
        
        elapsed = time.time() - start_time
        print(f"çœŸå®å€¼è®¡ç®—å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}ç§’ (Ground truth computed in {elapsed:.2f}s)")
        
        self._ground_truth_cache[cache_key] = ground_truth
        return ground_truth
    
    def evaluate_recall(
        self,
        kmeans_hnsw: KMeansHNSW,
        k: int,
        n_probe: int,
        ground_truth: Optional[Dict] = None,
        exclude_query_ids: bool = True
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°K-Means HNSWç³»ç»Ÿçš„å¬å›ç‡æ€§èƒ½ (Evaluate recall performance of the K-Means HNSW system)
        
        è®¡ç®—ç³»ç»Ÿåœ¨ç»™å®šå‚æ•°ä¸‹çš„å¬å›ç‡ã€æŸ¥è¯¢æ—¶é—´ç­‰æ€§èƒ½æŒ‡æ ‡ã€‚
        å¬å›ç‡ = æ‰¾åˆ°çš„çœŸå®é‚»å±…æ•° / åº”è¯¥æ‰¾åˆ°çš„é‚»å±…æ•°
        
        Args:
            kmeans_hnsw: è¦è¯„ä¼°çš„K-Means HNSWç³»ç»Ÿ (The K-Means HNSW system to evaluate)
            k: è¿”å›ç»“æœæ•°é‡ (Number of results to return)
            n_probe: æ¢æµ‹çš„èšç±»ä¸­å¿ƒæ•°é‡ (Number of centroids to probe)
            ground_truth: é¢„è®¡ç®—çš„çœŸå®å€¼(å¯é€‰) (Precomputed ground truth, optional)
            exclude_query_ids: æ˜¯å¦ä»è¯„ä¼°ä¸­æ’é™¤æŸ¥è¯¢ID (Whether to exclude query IDs from evaluation)
            
        Returns:
            åŒ…å«å¬å›ç‡æŒ‡æ ‡å’Œæ€§èƒ½æ•°æ®çš„å­—å…¸ (Dictionary containing recall metrics and performance data)
        """
        if ground_truth is None:
            ground_truth = self.compute_ground_truth(k, exclude_query_ids)
        
        print(f"æ­£åœ¨è¯„ä¼° {len(self.query_set)} ä¸ªæŸ¥è¯¢çš„å¬å›ç‡ (k={k}, n_probe={n_probe})... (Evaluating recall)")
        start_time = time.time()
        
        total_correct = 0
        total_expected = len(self.query_set) * k
        query_times = []
        individual_recalls = []
        
        for i, (query_vector, query_id) in enumerate(zip(self.query_set, self.query_ids)):
            # Get ground truth for this query
            true_neighbors = {neighbor_id for _, neighbor_id in ground_truth[query_id]}
            
            # Search using K-Means HNSW
            search_start = time.time()
            results = kmeans_hnsw.search(query_vector, k=k, n_probe=n_probe)
            search_time = time.time() - search_start
            query_times.append(search_time)
            
            # Count correct results
            found_neighbors = {node_id for node_id, _ in results}
            correct = len(true_neighbors & found_neighbors)
            total_correct += correct
            
            # Individual recall for this query
            individual_recall = correct / k if k > 0 else 0.0
            individual_recalls.append(individual_recall)
            
            if (i + 1) % 20 == 0:
                current_recall = total_correct / ((i + 1) * k)
                print(f"  Processed {i + 1}/{len(self.query_set)} queries, "
                      f"current recall: {current_recall:.4f}")
        
        # Calculate final metrics
        overall_recall = total_correct / total_expected
        avg_query_time = np.mean(query_times)
        std_query_time = np.std(query_times)
        total_evaluation_time = time.time() - start_time
        
        return {
            'recall_at_k': overall_recall,
            'total_correct': total_correct,
            'total_expected': total_expected,
            'individual_recalls': individual_recalls,
            'avg_individual_recall': np.mean(individual_recalls),
            'std_individual_recall': np.std(individual_recalls),
            'avg_query_time_ms': avg_query_time * 1000,
            'std_query_time_ms': std_query_time * 1000,
            'total_evaluation_time': total_evaluation_time,
            'k': k,
            'n_probe': n_probe,
            'system_stats': kmeans_hnsw.get_stats()
        }

    # -------------------- Phase-Specific Evaluations --------------------
    def evaluate_hnsw_baseline(
        self,
        base_index: HNSW,
        k: int,
        ef: int,
        ground_truth: Dict
    ) -> Dict[str, Any]:
        """Evaluate recall using ONLY the base HNSW index (Phase 1)."""
        query_times = []
        total_correct = 0
        total_expected = len(self.query_set) * k
        for query_vector, query_id in zip(self.query_set, self.query_ids):
            true_neighbors = {nid for _, nid in ground_truth[query_id]}
            t0 = time.time()
            results = base_index.query(query_vector, k=k, ef=ef)
            dt = time.time() - t0
            query_times.append(dt)
            found = {nid for nid, _ in results}
            total_correct += len(true_neighbors & found)
        return {
            'phase': 'baseline_hnsw',
            'ef': ef,
            'recall_at_k': total_correct / total_expected if total_expected else 0.0,
            'avg_query_time_ms': float(np.mean(query_times) * 1000),
            'total_correct': total_correct,
            'total_expected': total_expected
        }

    def evaluate_clusters_only(
        self,
        kmeans_model: Any,
        dataset: np.ndarray,
        k: int,
        ground_truth: Dict
    ) -> Dict[str, Any]:
        """Evaluate recall using ONLY KMeans clusters (Phase 2) without child mapping.
        Strategy: For each query pick nearest centroid, restrict to its members, pick top-k by L2.
        """
        if not hasattr(kmeans_model, 'cluster_centers_') or not hasattr(kmeans_model, 'labels_'):
            raise ValueError("KMeans model must be fitted with cluster_centers_ and labels_ available")
        centers = kmeans_model.cluster_centers_
        labels = kmeans_model.labels_
        n_clusters = centers.shape[0]
        # Build inverse index: cluster -> indices
        clusters = [[] for _ in range(n_clusters)]
        for idx, c in enumerate(labels):
            clusters[c].append(idx)
        query_times = []
        total_correct = 0
        total_expected = len(self.query_set) * k
        individual_recalls = []
        for qvec, qid in zip(self.query_set, self.query_ids):
            t0 = time.time()
            d2c = np.linalg.norm(centers - qvec, axis=1)
            cidx = int(np.argmin(d2c))
            member_ids = clusters[cidx]
            if member_ids:
                member_vecs = dataset[member_ids]
                dists = np.linalg.norm(member_vecs - qvec, axis=1)
                # exclude identical id if present
                pairs = [(float(dist), int(mid)) for dist, mid in zip(dists, member_ids) if mid != qid]
                pairs.sort(key=lambda x: x[0])
                results = pairs[:k]
                found = {mid for _, mid in results}
            else:
                found = set()
            query_times.append(time.time() - t0)
            true_neighbors = {nid for _, nid in ground_truth[qid]}
            correct = len(true_neighbors & found)
            total_correct += correct
            individual_recalls.append(correct / k if k else 0.0)
        overall = total_correct / total_expected if total_expected else 0.0
        return {
            'phase': 'clusters_only',
            'recall_at_k': overall,
            'avg_query_time_ms': float(np.mean(query_times) * 1000),
            'std_query_time_ms': float(np.std(query_times) * 1000),
            'total_correct': total_correct,
            'total_expected': total_expected,
            'avg_individual_recall': float(np.mean(individual_recalls)),
            'n_clusters': n_clusters
        }
    
    def parameter_sweep(
        self,
        base_index: HNSW,
        param_grid: Dict[str, List[Any]],
        evaluation_params: Dict[str, Any],
        max_combinations: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œå…¨é¢çš„å‚æ•°æ‰«æä¼˜åŒ– (Perform comprehensive parameter sweep for optimization)
        
        é€šè¿‡ç³»ç»Ÿæ€§åœ°æµ‹è¯•ä¸åŒå‚æ•°ç»„åˆï¼Œæ‰¾åˆ°æœ€ä¼˜çš„K-Means HNSWé…ç½®ã€‚
        åŒ…æ‹¬åŸºçº¿HNSWã€çº¯K-Meanså’ŒK-Means HNSWçš„å¯¹æ¯”è¯„ä¼°ã€‚
        
        Args:
            base_index: ä½¿ç”¨çš„åŸºç¡€HNSWç´¢å¼• (Base HNSW index to use)
            param_grid: å‚æ•°åŠå…¶æµ‹è¯•å€¼çš„å­—å…¸ (Dictionary of parameters and their values to test)
            evaluation_params: è¯„ä¼°å‚æ•° (Parameters for evaluation) - kå€¼, n_probeå€¼ç­‰
            max_combinations: æœ€å¤§æµ‹è¯•ç»„åˆæ•° (Maximum number of combinations to test)
            
        Returns:
            æ¯ä¸ªå‚æ•°ç»„åˆçš„è¯„ä¼°ç»“æœåˆ—è¡¨ (List of evaluation results for each parameter combination)
        """
        print("å¼€å§‹K-Means HNSWå‚æ•°æ‰«æ... (Starting parameter sweep for K-Means HNSW)")
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ (Generate all parameter combinations)
        param_names = list(param_grid.keys())
        param_values = list(param_grid.values())
        combinations = list(product(*param_values))
        
        if max_combinations and len(combinations) > max_combinations:
            print(f"é™åˆ¶æµ‹è¯• {max_combinations} ä¸ªç»„åˆï¼Œæ€»å…± {len(combinations)} ä¸ª (Limiting to {max_combinations} combinations out of {len(combinations)})")
            combinations = random.sample(combinations, max_combinations)
        
        print(f"æµ‹è¯• {len(combinations)} ä¸ªå‚æ•°ç»„åˆ... (Testing {len(combinations)} parameter combinations)")
        
        results = []
        k_values = evaluation_params.get('k_values', [10])
        n_probe_values = evaluation_params.get('n_probe_values', [5, 10, 20])
        
        # é¢„è®¡ç®—æ‰€æœ‰kå€¼çš„çœŸå®å€¼ (Precompute ground truth for all k values)
        ground_truths = {}
        for k in k_values:
            ground_truths[k] = self.compute_ground_truth(k)

        for i, combination in enumerate(combinations):
            print(f"\n--- Combination {i + 1}/{len(combinations)} ---")

            # Create parameter dictionary
            params = dict(zip(param_names, combination))
            print(f"Parameters: {params}")

            try:
                phase_records = []
                # Phase 1: baseline HNSW recall (optional multiple ef values)
                baseline_ef_values = evaluation_params.get('baseline_ef_values', [evaluation_params.get('baseline_ef', 100)])
                for k in k_values:
                    for ef in baseline_ef_values:
                        b_eval = self.evaluate_hnsw_baseline(base_index, k, ef, ground_truths[k])
                        phase_records.append({**b_eval, 'k': k})
                        print(f"  [Baseline HNSW] k={k} ef={ef} recall={b_eval['recall_at_k']:.4f} avg_time={b_eval['avg_query_time_ms']:.2f}ms")

                # Pure KMeans baseline using same n_clusters; evaluate over same n_probe set for fairness
                if 'n_clusters' in params:
                    for k in k_values:
                        for n_probe in n_probe_values:
                            pure_eval = self._evaluate_pure_kmeans(
                                k,
                                ground_truths[k],
                                n_clusters=params['n_clusters'],
                                n_probe=n_probe
                            )
                            phase_records.append({**pure_eval, 'phase': 'pure_kmeans_for_combo'})
                            print(
                                f"  [Pure KMeans] k={k} n_clusters={params['n_clusters']} n_probe={n_probe} "
                                f"recall={pure_eval['recall_at_k']:.4f} avg_time={pure_eval['avg_query_time_ms']:.2f}ms"
                            )

                # Build full system (includes clustering + child mapping)
                construction_start = time.time()
                kmeans_hnsw = KMeansHNSW(
                    base_index=base_index,
                    **params,
                    adaptive_k_children=getattr(args, 'adaptive_k_children', False),
                    k_children_scale=getattr(args, 'k_children_scale', 1.5),
                    k_children_min=getattr(args, 'k_children_min', 100),
                    k_children_max=getattr(args, 'k_children_max', None),
                    diversify_max_assignments=getattr(args, 'diversify_max_assignments', None),
                    repair_min_assignments=getattr(args, 'repair_min_assignments', None)
                )
                construction_time = time.time() - construction_start
                print(f"  Built KMeansHNSW system in {construction_time:.2f}s")

                # Phase 2: clusters-only recall using fitted internal KMeans model
                for k in k_values:
                    c_eval = self.evaluate_clusters_only(
                        kmeans_hnsw.kmeans_model,
                        kmeans_hnsw._extract_dataset_vectors(),  # reuse extractor
                        k,
                        ground_truths[k]
                    )
                    phase_records.append({**c_eval, 'k': k})
                    print(f"  [Clusters Only] k={k} recall={c_eval['recall_at_k']:.4f} avg_time={c_eval['avg_query_time_ms']:.2f}ms")

                # Phase 3: full two-stage search evaluations over n_probe
                for k in k_values:
                    for n_probe in n_probe_values:
                        eval_result = self.evaluate_recall(kmeans_hnsw, k, n_probe, ground_truths[k])
                        phase_records.append({**eval_result, 'phase': 'kmeans_hnsw_two_stage'})
                        print(f"  [Two-Stage] k={k} n_probe={n_probe} recall={eval_result['recall_at_k']:.4f} avg_time={eval_result['avg_query_time_ms']:.2f}ms")

                combination_results = {
                    'parameters': params,
                    'construction_time': construction_time,
                    'phase_evaluations': phase_records
                }
                results.append(combination_results)
                best_recall = max(r['recall_at_k'] for r in phase_records if 'recall_at_k' in r)
                print(f"Best recall (any phase) for this combination: {best_recall:.4f}")
            except Exception as e:
                print(f"Error with combination {params}: {e}")
                continue
        
        print(f"\nParameter sweep completed. Tested {len(results)} combinations.")
        return results
    
    def find_optimal_parameters(
        self,
        sweep_results: List[Dict[str, Any]],
        optimization_target: str = 'recall_at_k',
        constraints: Optional[Dict[str, float]] = None
    ) -> Dict[str, Any]:
        """
        Find optimal parameters from sweep results.
        
        Args:
            sweep_results: Results from parameter_sweep()
            optimization_target: Metric to optimize ('recall_at_k', 'avg_query_time_ms', etc.)
            constraints: Constraints on other metrics (e.g., {'avg_query_time_ms': 50.0})
            
        Returns:
            Dictionary containing optimal parameters and their performance
        """
        print(f"Finding optimal parameters optimizing for {optimization_target}...")
        
        best_result = None
        best_value = -float('inf') if 'recall' in optimization_target else float('inf')
        
        for result in sweep_results:
            for evaluation in result.get('phase_evaluations', []):
                # Check constraints
                if constraints:
                    violates_constraint = False
                    for constraint_metric, constraint_value in constraints.items():
                        if constraint_metric in evaluation:
                            if evaluation[constraint_metric] > constraint_value:
                                violates_constraint = True
                                break
                    if violates_constraint:
                        continue
                
                # Check if this is better
                current_value = evaluation.get(optimization_target)
                if current_value is None:
                    continue
                
                is_better = (
                    (current_value > best_value and 'recall' in optimization_target) or
                    (current_value < best_value and 'time' in optimization_target)
                )
                
                if is_better:
                    best_value = current_value
                    best_result = {
                        'parameters': result['parameters'],
                        'performance': evaluation,
                        'construction_time': result['construction_time']
                    }
        
        if best_result:
            print(f"Optimal parameters found:")
            print(f"  Parameters: {best_result['parameters']}")
            print(f"  {optimization_target}: {best_value:.4f}")
            print(f"  Construction time: {best_result['construction_time']:.2f}s")
        else:
            print("No valid parameters found satisfying constraints.")
        
        return best_result or {}
    
    def compare_with_baselines(
        self,
        kmeans_hnsw: KMeansHNSW,
        base_index: HNSW,
        k: int = 10,
        n_probe: int = 10,
        ef_values: List[int] = None
    ) -> Dict[str, Any]:
        """Compare K-Means HNSW performance with baseline HNSW and pure K-means.

        Pure K-means is evaluated probing the same number of centroids (n_probe)
        to make the comparison fair.
        """
        if ef_values is None:
            ef_values = [50, 100, 200, 400]

        print("Comparing K-Means HNSW with baseline HNSW and pure K-means...")

        ground_truth = self.compute_ground_truth(k)

        # K-Means HNSW two-stage
        kmeans_result = self.evaluate_recall(kmeans_hnsw, k, n_probe, ground_truth)

        # Pure k-means (multi-cluster probe)
        print("Evaluating pure K-means clustering (matching n_probe)...")
        kmeans_clustering_result = self._evaluate_pure_kmeans(k, ground_truth, n_probe=n_probe)

        # Baseline HNSW
        baseline_results = []
        for ef in ef_values:
            print(f"Evaluating baseline HNSW with ef={ef}...")
            query_times: List[float] = []
            total_correct = 0
            total_expected = len(self.query_set) * k
            for query_vector, query_id in zip(self.query_set, self.query_ids):
                true_neighbors = {neighbor_id for _, neighbor_id in ground_truth[query_id]}
                t0 = time.time()
                results = base_index.query(query_vector, k=k, ef=ef)
                dt = time.time() - t0
                query_times.append(dt)
                found_neighbors = {nid for nid, _ in results}
                total_correct += len(true_neighbors & found_neighbors)
            baseline_results.append({
                'method': 'baseline_hnsw',
                'ef': ef,
                'recall_at_k': total_correct / total_expected if total_expected else 0.0,
                'avg_query_time_ms': float(np.mean(query_times) * 1000),
                'total_correct': total_correct,
                'total_expected': total_expected
            })

        return {
            'kmeans_hnsw': kmeans_result,
            'pure_kmeans': kmeans_clustering_result,
            'baseline_hnsw': baseline_results,
            'comparison_summary': {
                'kmeans_hnsw_recall': kmeans_result['recall_at_k'],
                'kmeans_hnsw_time_ms': kmeans_result['avg_query_time_ms'],
                'pure_kmeans_recall': kmeans_clustering_result['recall_at_k'],
                'pure_kmeans_time_ms': kmeans_clustering_result['avg_query_time_ms'],
                'best_baseline_recall': max(r['recall_at_k'] for r in baseline_results) if baseline_results else 0.0,
                'best_baseline_time_ms': min(r['avg_query_time_ms'] for r in baseline_results) if baseline_results else 0.0
            }
        }
    
    def _evaluate_pure_kmeans(self, k: int, ground_truth: Dict, n_clusters: Optional[int] = None, n_probe: int = 1) -> Dict[str, Any]:
        """Evaluate pure MiniBatchKMeans clustering for comparison.
        Uses centroids to pick the nearest n_probe clusters, then returns top-k points within their union.

        Args:
            k: recall@k evaluation size
            ground_truth: dict mapping query id -> list of (dist, id)
            n_clusters: if provided, force this number of clusters (aligning with current param combination)
            n_probe: number of closest centroids to probe (>=1)
        """
        # Heuristic: choose number of clusters proportional to sqrt(N) if not specified, fallback to 10
        n_samples = len(self.dataset)
        if n_clusters is None:
            suggested = int(np.sqrt(n_samples))
            n_clusters = max(10, min(256, suggested))
        if n_probe < 1:
            n_probe = 1

        print(f"Running MiniBatchKMeans clustering (n_clusters={n_clusters}, n_probe={n_probe}) for pure KMeans baseline...")

        start_time = time.time()
        kmeans = MiniBatchKMeans(
            n_clusters=n_clusters,
            random_state=42,
            batch_size=min(1024, n_samples),
            n_init=3,
            max_iter=100,
            verbose=0
        )
        kmeans.fit(self.dataset)
        clustering_time = time.time() - start_time

        query_times = []
        total_correct = 0
        total_expected = len(self.query_set) * k
        individual_recalls = []

        # Cap n_probe to number of clusters
        n_probe_eff = min(n_probe, n_clusters)

        for query_vector, query_id in zip(self.query_set, self.query_ids):
            search_start = time.time()
            # Vectorized distance to centroids
            diffs = kmeans.cluster_centers_ - query_vector
            distances_to_centroids = np.linalg.norm(diffs, axis=1)
            # Get indices of n_probe closest centroids (unordered then sort for determinism)
            probe_centroids = np.argpartition(distances_to_centroids, n_probe_eff - 1)[:n_probe_eff]
            # Optionally sort by distance (small overhead, clearer behavior)
            probe_centroids = probe_centroids[np.argsort(distances_to_centroids[probe_centroids])]

            # Collect member ids from all probed centroids
            member_ids_list = [np.where(kmeans.labels_ == cid)[0] for cid in probe_centroids]
            if member_ids_list:
                union_points = np.concatenate(member_ids_list)
                if union_points.size > 0:
                    union_vecs = self.dataset[union_points]
                    point_diffs = union_vecs - query_vector
                    dists = np.linalg.norm(point_diffs, axis=1)
                    # Exclude the query id if present
                    filtered = [
                        (dist, int(pid)) for dist, pid in zip(dists, union_points)
                        if pid != query_id
                    ]
                    # Partial sort then full sort if small; we just sort because union typically modest
                    filtered.sort(key=lambda x: x[0])
                    results = filtered[:k]
                    found_neighbors = {pid for _, pid in results}
                else:
                    found_neighbors = set()
            else:
                found_neighbors = set()

            search_time = time.time() - search_start
            query_times.append(search_time)

            true_neighbors = {neighbor_id for _, neighbor_id in ground_truth[query_id]}
            correct = len(true_neighbors & found_neighbors)
            total_correct += correct
            individual_recalls.append(correct / k if k > 0 else 0.0)

        overall_recall = total_correct / total_expected if total_expected > 0 else 0.0

        return {
            'method': 'pure_minibatch_kmeans',
            'recall_at_k': overall_recall,
            'total_correct': total_correct,
            'total_expected': total_expected,
            'individual_recalls': individual_recalls,
            'avg_individual_recall': float(np.mean(individual_recalls)),
            'std_individual_recall': float(np.std(individual_recalls)),
            'avg_query_time_ms': float(np.mean(query_times) * 1000),
            'std_query_time_ms': float(np.std(query_times) * 1000),
            'clustering_time': clustering_time,
            'n_clusters': n_clusters,
            'n_probe': n_probe_eff,
            'k': k
        }


def save_results(results: Dict[str, Any], filename: str):
    """Save evaluation results to JSON file."""
    # Convert numpy arrays to lists for JSON serialization
    def convert_numpy(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, dict):
            return {k: convert_numpy(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy(item) for item in obj]
        return obj
    
    results_serializable = convert_numpy(results)
    
    with open(filename, 'w') as f:
        json.dump(results_serializable, f, indent=2)
    
    print(f"Results saved to {filename}")


def load_sift_data():
    """
    åŠ è½½SIFTæ•°æ®é›†ç”¨äºè¯„ä¼° (Load SIFT dataset for evaluation)
    
    SIFT (Scale-Invariant Feature Transform) æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ç»å…¸ç‰¹å¾æè¿°ç¬¦ã€‚
    è¯¥æ•°æ®é›†åŒ…å«100ä¸‡ä¸ª128ç»´çš„ç‰¹å¾å‘é‡ï¼Œå¸¸ç”¨äºç›¸ä¼¼æ€§æœç´¢ç®—æ³•çš„åŸºå‡†æµ‹è¯•ã€‚
    
    Returns:
        tuple: (base_vectors, query_vectors) æˆ– (None, None) å¦‚æœåŠ è½½å¤±è´¥
    """
    sift_dir = os.path.join(os.path.dirname(__file__), '..', 'sift')
    
    try:
        def read_fvecs(path: str, max_vectors: Optional[int] = None) -> np.ndarray:
            """
            è¯»å–.fvecsæ–‡ä»¶ (FAISSæ ¼å¼)ã€‚æ¯ä¸ªå‘é‡å­˜å‚¨ä¸ºï¼šint32ç»´åº¦ + ç»´åº¦ä¸ªfloat32å€¼ã€‚
            æ­¤å®ç°é€šè¿‡å…ˆè¯»å–int32å¤´éƒ¨æ¥é¿å…è§£æé”™è¯¯ã€‚
            
            Read .fvecs file (FAISS format). Each vector stored as: int32 dim + dim float32.
            This implementation avoids mis-parsing by reading int32 header first.
            """
            if not os.path.exists(path):
                raise FileNotFoundError(path)
            raw = np.fromfile(path, dtype=np.int32)
            if raw.size == 0:
                raise ValueError(f"ç©ºçš„fvecsæ–‡ä»¶: {path} (Empty fvecs file)")
            dim = raw[0]
            if dim <= 0 or dim > 4096:
                raise ValueError(f"ä¸åˆç†çš„å‘é‡ç»´åº¦ {dim}ï¼Œè§£æè‡ª {path} (Unreasonable vector dimension)")
            record_size = dim + 1
            count = raw.size // record_size
            raw = raw.reshape(count, record_size)
            vecs = raw[:, 1:].astype(np.float32)
            if max_vectors is not None and count > max_vectors:
                vecs = vecs[:max_vectors]
            return vecs

        base_path = os.path.join(sift_dir, 'sift_base.fvecs')
        query_path = os.path.join(sift_dir, 'sift_query.fvecs')

        # ä¸ºè°ƒä¼˜æ¼”ç¤ºé™åˆ¶æ•°é‡ä»¥ä¿æŒåˆç†çš„è¿è¡Œæ—¶é—´ (Limit for tuning demo to keep runtime reasonable)
        base_vectors = read_fvecs(base_path, max_vectors=50000)
        query_vectors = read_fvecs(query_path, max_vectors=1000)

        print(f"å·²åŠ è½½SIFTæ•°æ®: {base_vectors.shape[0]} ä¸ªåŸºç¡€å‘é‡, "
              f"{query_vectors.shape[0]} ä¸ªæŸ¥è¯¢å‘é‡, ç»´åº¦ {base_vectors.shape[1]}")
        print(f"Loaded SIFT data: {base_vectors.shape[0]} base vectors, "
              f"{query_vectors.shape[0]} query vectors, dimension {base_vectors.shape[1]}")

        return base_vectors, query_vectors
    
    except Exception as e:
        print(f"åŠ è½½SIFTæ•°æ®æ—¶å‡ºé”™: {e} (Error loading SIFT data)")
        print("æ”¹ç”¨åˆæˆæ•°æ®... (Using synthetic data instead)")
        return None, None


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="K-Means HNSWå‚æ•°è°ƒä¼˜å’Œè¯„ä¼° (K-Means HNSW Parameter Tuning and Evaluation)")
    
    # æ•°æ®é›†é€‰é¡¹ (Dataset options)
    parser.add_argument('--dataset-size', type=int, default=10000, 
                        help='ä½¿ç”¨çš„åŸºç¡€å‘é‡æ•°é‡ (é»˜è®¤: 10000) (Number of base vectors to use)')
    parser.add_argument('--query-size', type=int, default=50, 
                        help='ä½¿ç”¨çš„æŸ¥è¯¢å‘é‡æ•°é‡ (é»˜è®¤: 50) (Number of query vectors to use)')
    parser.add_argument('--dimension', type=int, default=128, 
                        help='åˆæˆæ•°æ®çš„å‘é‡ç»´åº¦ (å¦‚æœæœªåŠ è½½SIFT) (Vector dimensionality for synthetic data)')
    parser.add_argument('--no-sift', action='store_true', 
                        help='å¼ºåˆ¶ä½¿ç”¨åˆæˆæ•°æ®ï¼Œå³ä½¿SIFTæ–‡ä»¶å­˜åœ¨ (Force synthetic data even if SIFT files exist)')
    
    # è‡ªé€‚åº”/å¤šæ ·åŒ–/ä¿®å¤é€‰é¡¹ (Adaptive/diversification/repair options)
    parser.add_argument('--adaptive-k-children', action='store_true', 
                        help='å¯ç”¨åŸºäºå¹³å‡èšç±»å¤§å°çš„è‡ªé€‚åº”k_children (Enable adaptive k_children based on avg cluster size)')
    parser.add_argument('--k-children-scale', type=float, default=1.5, 
                        help='è‡ªé€‚åº”k_childrençš„ç¼©æ”¾å› å­ (é»˜è®¤1.5) (Scale factor for adaptive k_children)')
    parser.add_argument('--k-children-min', type=int, default=100, 
                        help='è‡ªé€‚åº”æ—¶çš„æœ€å°k_children (Minimum k_children when adaptive)')
    parser.add_argument('--k-children-max', type=int, default=None, 
                        help='è‡ªé€‚åº”æ—¶çš„æœ€å¤§k_children (å¯é€‰) (Maximum k_children when adaptive)')
    parser.add_argument('--diversify-max-assignments', type=int, default=None, 
                        help='æ¯ä¸ªå­èŠ‚ç‚¹çš„æœ€å¤§åˆ†é…æ•° (å¯ç”¨å¤šæ ·åŒ–) (Max assignments per child - enable diversification)')
    parser.add_argument('--repair-min-assignments', type=int, default=None, 
                        help='æ„å»ºä¿®å¤æœŸé—´æ¯ä¸ªå­èŠ‚ç‚¹çš„æœ€å°åˆ†é…æ•° (éœ€è¦å¤šæ ·åŒ–) (Min assignments per child during build repair)')
    parser.add_argument('--manual-repair', action='store_true', 
                        help='åœ¨æœ€ä¼˜æ„å»ºåè¿è¡Œæ‰‹åŠ¨ä¿®å¤ (Run manual repair after optimal build)')
    parser.add_argument('--manual-repair-min', type=int, default=None, 
                        help='æ‰‹åŠ¨ä¿®å¤çš„æœ€å°åˆ†é…æ•° (Min assignments for manual repair)')
    args = parser.parse_args()

    print("ğŸ”¬ K-Means HNSWå‚æ•°è°ƒä¼˜å’Œè¯„ä¼°ç³»ç»Ÿ (K-Means HNSW Parameter Tuning and Evaluation)")
    print(f"ğŸ“Š è¯·æ±‚çš„æ•°æ®é›†å¤§å°: {args.dataset_size}, æŸ¥è¯¢å¤§å°: {args.query_size}")
    print(f"   Requested dataset size: {args.dataset_size}, query size: {args.query_size}")
    
    # å°è¯•åŠ è½½SIFTæ•°æ®ï¼Œå¤±è´¥åˆ™ä½¿ç”¨åˆæˆæ•°æ® (Try to load SIFT data, fall back to synthetic unless disabled)
    base_vectors, query_vectors = (None, None)
    if not args.no_sift:
        base_vectors, query_vectors = load_sift_data()
    
    if base_vectors is None:
        # åˆ›å»ºåˆæˆæ•°æ® (Create synthetic data)
        print("ğŸ² åˆ›å»ºåˆæˆæ•°æ®é›†... (Creating synthetic dataset)")
        base_vectors = np.random.randn(max(args.dataset_size, 10000), args.dimension).astype(np.float32)
        query_vectors = np.random.randn(max(args.query_size, 100), args.dimension).astype(np.float32)
    
    # åˆ‡ç‰‡åˆ°è¯·æ±‚çš„å¤§å° (æŒ‰å¯ç”¨é‡é™åˆ¶) (Slice to requested sizes)
    if len(base_vectors) > args.dataset_size:
        base_vectors = base_vectors[:args.dataset_size]
    if len(query_vectors) > args.query_size:
        query_vectors = query_vectors[:args.query_size]
    print(f"ğŸ“ˆ ä½¿ç”¨åŸºç¡€å‘é‡: {len(base_vectors)} | æŸ¥è¯¢: {len(query_vectors)} | ç»´åº¦: {base_vectors.shape[1]}")
    print(f"   Using base vectors: {len(base_vectors)} | queries: {len(query_vectors)} | dim: {base_vectors.shape[1]}")
    query_ids = list(range(len(query_vectors)))
    
    # è·ç¦»å‡½æ•° (Distance function)
    distance_func = lambda x, y: np.linalg.norm(x - y)
    
    # æ„å»ºåŸºç¡€HNSWç´¢å¼• (Build base HNSW index)
    print("ğŸ—ï¸  æ„å»ºåŸºç¡€HNSWç´¢å¼•... (Building base HNSW index)")
    base_index = HNSW(distance_func=distance_func, m=16, ef_construction=100)
    
    for i, vector in enumerate(base_vectors):
        base_index.insert(i, vector)
        if (i + 1) % 1000 == 0:
            print(f"  Inserted {i + 1}/{len(base_vectors)} vectors")
    
    print(f"Base HNSW index built with {len(base_index)} vectors")
    
    # Initialize evaluator
    evaluator = KMeansHNSWEvaluator(base_vectors, query_vectors, query_ids, distance_func)
    
    # Define parameter grid for sweep
    # Adjust default cluster count heuristics for larger datasets: scale choices
    if args.dataset_size <= 2000:
        cluster_options = [10]
    elif args.dataset_size <= 5000:
        cluster_options = [16, 32]
    else:
        cluster_options = [32, 64, 128]

    param_grid = {
        'n_clusters': cluster_options,
        'k_children': [200],
        'child_search_ef': [300]
    }
    
    evaluation_params = {
        'k_values': [10],
        'n_probe_values': [5, 10, 20]
    }
    
    # Perform parameter sweep
    print("\nStarting parameter sweep...")
    # Limit combinations to keep runtime sane on large sets
    max_combos = 9 if len(cluster_options) > 1 else None
    sweep_results = evaluator.parameter_sweep(
        base_index,
        param_grid,
        evaluation_params,
        max_combinations=max_combos
    )
    
    # Find optimal parameters
    optimal = evaluator.find_optimal_parameters(
        sweep_results,
        optimization_target='recall_at_k',
        constraints={'avg_query_time_ms': 100.0}  # Max 100ms per query
    )
    
    if optimal:
        # Build system with optimal parameters and compare with baseline
        print("\nBuilding system with optimal parameters...")
        optimal_kmeans_hnsw = KMeansHNSW(
            base_index=base_index,
            **optimal['parameters'],
            adaptive_k_children=args.adaptive_k_children,
            k_children_scale=args.k_children_scale,
            k_children_min=args.k_children_min,
            k_children_max=args.k_children_max,
            diversify_max_assignments=args.diversify_max_assignments,
            repair_min_assignments=args.repair_min_assignments
        )

        if args.manual_repair:
            manual_min = args.manual_repair_min or args.repair_min_assignments or 1
            print(f"\nManual repair step: ensuring each node has at least {manual_min} assignments...")
            repair_stats = optimal_kmeans_hnsw.run_repair(min_assignments=manual_min)
            print(f"Manual repair completed. Coverage={repair_stats['coverage_fraction']:.3f}")
        
        comparison = evaluator.compare_with_baselines(
            optimal_kmeans_hnsw,
            base_index,
            k=10,
            n_probe=10
        )
        
        print("\nComparison Results:")
        print(f"K-Means HNSW: Recall={comparison['comparison_summary']['kmeans_hnsw_recall']:.4f}, "
              f"Time={comparison['comparison_summary']['kmeans_hnsw_time_ms']:.2f}ms")
        print(f"Pure K-Means: Recall={comparison['comparison_summary']['pure_kmeans_recall']:.4f}, "
              f"Time={comparison['comparison_summary']['pure_kmeans_time_ms']:.2f}ms")
        print(f"Best Baseline: Recall={comparison['comparison_summary']['best_baseline_recall']:.4f}, "
              f"Time={comparison['comparison_summary']['best_baseline_time_ms']:.2f}ms")
        
        # Additional detailed output for pure K-means
        pure_kmeans_result = comparison['pure_kmeans']
        print(f"\nDetailed Pure K-Means Results:")
        print(f"  Overall Recall@{pure_kmeans_result['k']}: {pure_kmeans_result['recall_at_k']:.4f}")
        print(f"  Average Individual Recall: {pure_kmeans_result['avg_individual_recall']:.4f}")
        print(f"  Correct/Expected: {pure_kmeans_result['total_correct']}/{pure_kmeans_result['total_expected']}")
        print(f"  Clustering Time: {pure_kmeans_result['clustering_time']:.2f}s")
        print(f"  Average Query Time: {pure_kmeans_result['avg_query_time_ms']:.2f}ms")
        
        # Save results
        results = {
            'sweep_results': sweep_results,
            'optimal_parameters': optimal,
            'baseline_comparison': comparison,
            'adaptive_config': {
                'adaptive_k_children': args.adaptive_k_children,
                'k_children_scale': args.k_children_scale,
                'k_children_min': args.k_children_min,
                'k_children_max': args.k_children_max,
                'diversify_max_assignments': args.diversify_max_assignments,
                'repair_min_assignments': args.repair_min_assignments,
                'manual_repair': args.manual_repair,
                'manual_repair_min': args.manual_repair_min
            },
            'evaluation_info': {
                'dataset_size': len(base_vectors),
                'query_size': len(query_vectors),
                'dimension': base_vectors.shape[1],
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
        }
        
        save_results(results, 'method3_tuning_results.json')
        
    print("\nParameter tuning completed!")
