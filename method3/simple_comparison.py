"""
ç®€åŒ–çš„K-Means HNSWæ€§èƒ½æ¯”è¾ƒå·¥å…·
=================================

è¿™æ˜¯ä¸€ä¸ªç”¨äºæ¯”è¾ƒä¸‰ç§å‘é‡æœç´¢æ–¹æ³•æ€§èƒ½çš„ç®€åŒ–å·¥å…·ï¼š
1. çº¯HNSWç´¢å¼• - åŸºäºåˆ†å±‚å›¾çš„è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢
2. çº¯K-Meansèšç±» - åŸºäºèšç±»çš„å‘é‡æœç´¢
3. K-Means HNSWæ··åˆç³»ç»Ÿ - ç»“åˆK-Meanså’ŒHNSWçš„ä¸¤é˜¶æ®µæœç´¢

ä¸»è¦åŠŸèƒ½ï¼š
- è‡ªåŠ¨æ„å»ºä¸‰ç§ä¸åŒçš„ç´¢å¼•ç»“æ„
- è®¡ç®—ground truthä½œä¸ºæ€§èƒ½åŸºå‡†
- è¯„ä¼°å¬å›ç‡(recall@k)å’ŒæŸ¥è¯¢å“åº”æ—¶é—´
- è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜å‚æ•°ç»„åˆ
- ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æ¯”è¾ƒæŠ¥å‘Š

é€‚ç”¨åœºæ™¯ï¼š
- å‘é‡æ•°æ®åº“æ€§èƒ½è¯„ä¼°
- æœç´¢ç®—æ³•é€‰å‹
- å‚æ•°è°ƒä¼˜å®éªŒ
- å­¦æœ¯ç ”ç©¶å’Œbenchmarking

ä½œè€…ï¼šåŸºäºtune_kmeans_hnsw.pyç®€åŒ–è€Œæ¥
ç‰ˆæœ¬ï¼š1.0
"""

import os
import sys
import time
import json
import numpy as np
import argparse
from typing import Dict, List, Tuple, Optional, Any
from sklearn.cluster import MiniBatchKMeans

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from method3.kmeans_hnsw import KMeansHNSW
from hnsw.hnsw import HNSW


class SimpleComparator:
    """
    ç®€åŒ–çš„ä¸‰ç§æ–¹æ³•æ€§èƒ½æ¯”è¾ƒå™¨
    
    è¿™ä¸ªç±»å°è£…äº†ä¸‰ç§å‘é‡æœç´¢æ–¹æ³•çš„æ„å»ºã€è¯„ä¼°å’Œæ¯”è¾ƒåŠŸèƒ½ï¼š
    1. HNSW - åˆ†å±‚å¯å¯¼èˆªå°ä¸–ç•Œå›¾ç®—æ³•
    2. K-Means - åŸºäºèšç±»çš„æœç´¢ç®—æ³•  
    3. K-Means HNSW - æ··åˆä¸¤é˜¶æ®µæœç´¢ç®—æ³•
    
    ä½¿ç”¨æµç¨‹ï¼š
    1. åˆå§‹åŒ–æ¯”è¾ƒå™¨ï¼ˆä¼ å…¥æ•°æ®é›†å’ŒæŸ¥è¯¢é›†ï¼‰
    2. è°ƒç”¨run_full_comparison()æ‰§è¡Œå®Œæ•´æ¯”è¾ƒ
    3. è°ƒç”¨print_results()æ˜¾ç¤ºç»“æœ
    
    æ€§èƒ½æŒ‡æ ‡ï¼š
    - å¬å›ç‡(Recall@K)ï¼šè¿”å›ç»“æœä¸­çœŸæ­£è¿‘é‚»çš„æ¯”ä¾‹
    - æŸ¥è¯¢æ—¶é—´ï¼šå•æ¬¡æŸ¥è¯¢çš„å¹³å‡å“åº”æ—¶é—´
    - æ„å»ºæ—¶é—´ï¼šç´¢å¼•æ„å»ºæ‰€éœ€çš„æ—¶é—´
    """
    
    def __init__(self, dataset: np.ndarray, queries: np.ndarray, distance_func: callable):
        """
        åˆå§‹åŒ–æ¯”è¾ƒå™¨
        
        Args:
            dataset: æ•°æ®é›†å‘é‡ [n_vectors, dim]
            queries: æŸ¥è¯¢å‘é‡ [n_queries, dim]
            distance_func: è·ç¦»å‡½æ•°
        """
        self.dataset = dataset
        self.queries = queries
        self.distance_func = distance_func
        self.dimension = dataset.shape[1]
        
        print(f"æ•°æ®é›†: {len(dataset)}ä¸ªå‘é‡, æŸ¥è¯¢: {len(queries)}ä¸ªå‘é‡, ç»´åº¦: {self.dimension}")
    
    def compute_ground_truth(self, k: int) -> List[List[int]]:
        """
        è®¡ç®—çœŸå®çš„æœ€è¿‘é‚»ä½œä¸ºåŸºå‡†(Ground Truth)
        
        ä½¿ç”¨æš´åŠ›æœç´¢è®¡ç®—æ¯ä¸ªæŸ¥è¯¢å‘é‡çš„çœŸå®kè¿‘é‚»ï¼Œä½œä¸ºè¯„ä¼°å…¶ä»–ç®—æ³•æ€§èƒ½çš„åŸºå‡†ã€‚
        è¿™æ˜¯æœ€å‡†ç¡®ä½†è®¡ç®—å¤æ‚åº¦æœ€é«˜çš„æ–¹æ³•(O(n*m*d))ï¼Œå…¶ä¸­ï¼š
        - n: æ•°æ®é›†å¤§å°
        - m: æŸ¥è¯¢æ•°é‡  
        - d: å‘é‡ç»´åº¦
        
        Args:
            k: éœ€è¦è¿”å›çš„æœ€è¿‘é‚»æ•°é‡
            
        Returns:
            List[List[int]]: æ¯ä¸ªæŸ¥è¯¢å¯¹åº”çš„kä¸ªæœ€è¿‘é‚»ç´¢å¼•åˆ—è¡¨
            
        æ³¨æ„ï¼š
            è¿™ä¸ªè¿‡ç¨‹å¯èƒ½å¾ˆè€—æ—¶ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§æ•°æ®é›†ã€‚
            è®¡ç®—å¤æ‚åº¦ä¸ºO(n*m*d)ï¼Œå…¶ä¸­næ˜¯æ•°æ®é›†å¤§å°ï¼Œmæ˜¯æŸ¥è¯¢æ•°é‡ã€‚
        """
        print(f"è®¡ç®—ground truth (k={k})...")
        start_time = time.time()
        
        ground_truth = []
        for i, query in enumerate(self.queries):
            distances = []
            for j, data_point in enumerate(self.dataset):
                dist = self.distance_func(query, data_point)
                distances.append((dist, j))
            
            # æŒ‰è·ç¦»æ’åºï¼Œå–å‰kä¸ªæœ€è¿‘é‚»
            distances.sort()
            ground_truth.append([idx for _, idx in distances[:k]])
            
            # æ˜¾ç¤ºè¿›åº¦ï¼Œé¿å…ç”¨æˆ·ç­‰å¾…ç„¦è™‘
            if (i + 1) % 20 == 0:
                print(f"  å·²å¤„ç† {i + 1}/{len(self.queries)} ä¸ªæŸ¥è¯¢")
        
        print(f"Ground truthè®¡ç®—å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}s")
        return ground_truth
    
    def build_hnsw(self, m: int = 16, ef_construction: int = 200) -> HNSW:
        """
        æ„å»ºHNSW(åˆ†å±‚å¯å¯¼èˆªå°ä¸–ç•Œå›¾)ç´¢å¼•
        
        HNSWæ˜¯ä¸€ç§åŸºäºå›¾çš„è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ç®—æ³•ï¼Œé€šè¿‡æ„å»ºå¤šå±‚å›¾ç»“æ„
        æ¥å®ç°é«˜æ•ˆçš„å‘é‡æœç´¢ã€‚ç®—æ³•ç‰¹ç‚¹ï¼š
        - æœç´¢æ—¶é—´å¤æ‚åº¦ï¼šO(log n)
        - ç©ºé—´å¤æ‚åº¦ï¼šO(n * M)
        - é«˜å¬å›ç‡å’Œå¿«é€ŸæŸ¥è¯¢é€Ÿåº¦
        
        Args:
            m: æ¯ä¸ªèŠ‚ç‚¹çš„æœ€å¤§è¿æ¥æ•°ï¼Œå½±å“å›¾çš„è¿é€šæ€§å’ŒæŸ¥è¯¢ç²¾åº¦
               - è¾ƒå¤§çš„må€¼ï¼šæ›´é«˜ç²¾åº¦ï¼Œä½†æ›´å¤šå†…å­˜æ¶ˆè€—
               - è¾ƒå°çš„må€¼ï¼šæ›´å°‘å†…å­˜ï¼Œä½†å¯èƒ½é™ä½ç²¾åº¦
               - å…¸å‹å€¼ï¼š8-48
            ef_construction: æ„å»ºæ—¶çš„å€™é€‰é˜Ÿåˆ—å¤§å°
               - è¾ƒå¤§å€¼ï¼šæ„å»ºè´¨é‡æ›´å¥½ï¼Œä½†è€—æ—¶æ›´é•¿
               - è¾ƒå°å€¼ï¼šæ„å»ºæ›´å¿«ï¼Œä½†è´¨é‡å¯èƒ½ä¸‹é™
               - å…¸å‹å€¼ï¼š100-800
        
        Returns:
            HNSW: æ„å»ºå®Œæˆçš„HNSWç´¢å¼•å¯¹è±¡
        """
        print(f"æ„å»ºHNSWç´¢å¼• (m={m}, ef_construction={ef_construction})...")
        start_time = time.time()
        
        hnsw = HNSW(distance_func=self.distance_func, m=m, ef_construction=ef_construction)
        
        for i, vector in enumerate(self.dataset):
            hnsw.insert(i, vector)
            # æ˜¾ç¤ºæ„å»ºè¿›åº¦ï¼Œè®©ç”¨æˆ·äº†è§£å½“å‰çŠ¶æ€
            if (i + 1) % 1000 == 0:
                print(f"  å·²æ’å…¥ {i + 1}/{len(self.dataset)} ä¸ªå‘é‡")
        
        build_time = time.time() - start_time
        print(f"HNSWç´¢å¼•æ„å»ºå®Œæˆï¼Œè€—æ—¶: {build_time:.2f}s")
        return hnsw
    
    def build_pure_kmeans(self, n_clusters: int = 64) -> MiniBatchKMeans:
        """
        æ„å»ºçº¯K-Meansèšç±»æ¨¡å‹
        
        K-Meansæ˜¯ä¸€ç§æ— ç›‘ç£èšç±»ç®—æ³•ï¼Œå°†æ•°æ®åˆ†ä¸ºkä¸ªç°‡ã€‚åœ¨å‘é‡æœç´¢ä¸­ï¼š
        1. è®­ç»ƒé˜¶æ®µï¼šå°†æ•°æ®é›†èšç±»ä¸ºkä¸ªç°‡
        2. æŸ¥è¯¢é˜¶æ®µï¼šæ‰¾åˆ°æŸ¥è¯¢å‘é‡æœ€è¿‘çš„ç°‡ï¼Œåœ¨è¯¥ç°‡å†…è¿›è¡Œç²¾ç¡®æœç´¢
        
        ä½¿ç”¨MiniBatchKMeansè€Œéæ ‡å‡†KMeansçš„åŸå› ï¼š
        - æ›´é€‚åˆå¤§æ•°æ®é›†
        - å†…å­˜æ•ˆç‡æ›´é«˜
        - è®­ç»ƒé€Ÿåº¦æ›´å¿«
        - ç²¾åº¦æŸå¤±å¾ˆå°
        
        Args:
            n_clusters: èšç±»ä¸­å¿ƒçš„æ•°é‡
                - å½±å“æœç´¢ç²¾åº¦å’Œé€Ÿåº¦çš„æƒè¡¡
                - å¤ªå°‘ï¼šæ¯ä¸ªç°‡åŒ…å«å¤ªå¤šç‚¹ï¼Œæœç´¢æ…¢
                - å¤ªå¤šï¼šå¯èƒ½è¿‡æ‹Ÿåˆï¼ŒæŸ¥è¯¢è´¨é‡ä¸‹é™
                - ç»éªŒå€¼ï¼šsqrt(n) åˆ° n/100 ä¹‹é—´
        
        Returns:
            MiniBatchKMeans: è®­ç»ƒå®Œæˆçš„K-Meansæ¨¡å‹
        """
        print(f"æ„å»ºK-Meansèšç±» (n_clusters={n_clusters})...")
        start_time = time.time()
        
        kmeans = MiniBatchKMeans(
            n_clusters=n_clusters,
            random_state=42,
            batch_size=min(1024, len(self.dataset)),
            n_init=3,
            max_iter=100,
            verbose=0
        )
        kmeans.fit(self.dataset)
        
        build_time = time.time() - start_time
        print(f"K-Meansèšç±»å®Œæˆï¼Œè€—æ—¶: {build_time:.2f}s")
        return kmeans
    
    def build_kmeans_hnsw(self, hnsw: HNSW, n_clusters: int = 64, 
                         k_children: int = 200, child_search_ef: int = 300) -> KMeansHNSW:
        """
        æ„å»ºK-Means HNSWæ··åˆç³»ç»Ÿ
        
        è¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„ä¸¤é˜¶æ®µæœç´¢ç³»ç»Ÿï¼Œç»“åˆäº†K-Meanså’ŒHNSWçš„ä¼˜åŠ¿ï¼š
        
        ç¬¬ä¸€é˜¶æ®µ - ç²—æœç´¢(K-Means)ï¼š
        - ä½¿ç”¨K-Meansèšç±»å°†æ•°æ®ç©ºé—´åˆ’åˆ†
        - å¿«é€Ÿå®šä½æŸ¥è¯¢å‘é‡å¯èƒ½æ‰€åœ¨çš„åŒºåŸŸ
        - æ—¶é—´å¤æ‚åº¦ï¼šO(k*d)ï¼Œå…¶ä¸­kæ˜¯èšç±»æ•°
        
        ç¬¬äºŒé˜¶æ®µ - ç²¾æœç´¢(HNSW)ï¼š
        - åœ¨é€‰å®šçš„èšç±»åŒºåŸŸå†…ä½¿ç”¨HNSWè¿›è¡Œç²¾ç¡®æœç´¢
        - é¿å…åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šæœç´¢ï¼Œæé«˜æ•ˆç‡
        - ä¿æŒHNSWçš„é«˜ç²¾åº¦ç‰¹æ€§
        
        Args:
            hnsw: å·²æ„å»ºçš„åŸºç¡€HNSWç´¢å¼•
            n_clusters: K-Meansèšç±»æ•°é‡
                - å½±å“ç¬¬ä¸€é˜¶æ®µçš„æœç´¢é€Ÿåº¦å’Œç²¾åº¦
            k_children: æ¯ä¸ªèšç±»ä¸­å¿ƒå…³è”çš„å­èŠ‚ç‚¹æ•°é‡
                - æ§åˆ¶ç¬¬äºŒé˜¶æ®µçš„æœç´¢èŒƒå›´
                - è¾ƒå¤§å€¼ï¼šæ›´é«˜å¬å›ç‡ï¼Œä½†æœç´¢æ—¶é—´å¢åŠ 
            child_search_ef: å­èŠ‚ç‚¹æœç´¢æ—¶çš„efå‚æ•°
                - æ§åˆ¶HNSWæœç´¢çš„ç²¾åº¦
        
        Returns:
            KMeansHNSW: æ„å»ºå®Œæˆçš„æ··åˆç³»ç»Ÿ
        """
        print(f"æ„å»ºK-Means HNSWæ··åˆç³»ç»Ÿ (n_clusters={n_clusters}, k_children={k_children})...")
        start_time = time.time()
        
        kmeans_hnsw = KMeansHNSW(
            base_index=hnsw,
            n_clusters=n_clusters,
            k_children=k_children,
            child_search_ef=child_search_ef
        )
        
        build_time = time.time() - start_time
        print(f"K-Means HNSWæ··åˆç³»ç»Ÿæ„å»ºå®Œæˆï¼Œè€—æ—¶: {build_time:.2f}s")
        return kmeans_hnsw
    
    def evaluate_hnsw(self, hnsw: HNSW, ground_truth: List[List[int]], 
                     k: int, ef: int = 100) -> Dict[str, Any]:
        """è¯„ä¼°HNSWæ€§èƒ½"""
        print(f"è¯„ä¼°HNSWæ€§èƒ½ (k={k}, ef={ef})...")
        
        total_correct = 0
        query_times = []
        
        for i, (query, true_neighbors) in enumerate(zip(self.queries, ground_truth)):
            true_ids = set(true_neighbors)
            
            start_time = time.time()
            results = hnsw.query(query, k=k, ef=ef)
            query_time = time.time() - start_time
            query_times.append(query_time)
            
            found_ids = {node_id for node_id, _ in results}
            correct = len(true_ids & found_ids)
            total_correct += correct
        
        recall = total_correct / (len(self.queries) * k)
        avg_time = np.mean(query_times) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        return {
            'method': 'HNSW',
            'recall': recall,
            'avg_query_time_ms': avg_time,
            'total_correct': total_correct,
            'total_expected': len(self.queries) * k,
            'ef': ef
        }
    
    def evaluate_pure_kmeans(self, kmeans: MiniBatchKMeans, ground_truth: List[List[int]], 
                           k: int, n_probe: int = 5) -> Dict[str, Any]:
        """è¯„ä¼°çº¯K-Meansæ€§èƒ½"""
        print(f"è¯„ä¼°çº¯K-Meansæ€§èƒ½ (k={k}, n_probe={n_probe})...")
        
        # æ„å»ºèšç±»åˆ°æ•°æ®ç‚¹çš„æ˜ å°„
        clusters = [[] for _ in range(kmeans.n_clusters)]
        for idx, cluster_id in enumerate(kmeans.labels_):
            clusters[cluster_id].append(idx)
        
        total_correct = 0
        query_times = []
        
        n_probe_eff = min(n_probe, kmeans.n_clusters)
        
        for i, (query, true_neighbors) in enumerate(zip(self.queries, ground_truth)):
            true_ids = set(true_neighbors)
            
            start_time = time.time()
            
            # æ‰¾åˆ°æœ€è¿‘çš„n_probeä¸ªèšç±»ä¸­å¿ƒ
            distances_to_centers = np.linalg.norm(kmeans.cluster_centers_ - query, axis=1)
            closest_clusters = np.argpartition(distances_to_centers, n_probe_eff - 1)[:n_probe_eff]
            
            # æ”¶é›†è¿™äº›èšç±»ä¸­çš„æ‰€æœ‰ç‚¹
            candidate_ids = []
            for cluster_id in closest_clusters:
                candidate_ids.extend(clusters[cluster_id])
            
            if candidate_ids:
                # è®¡ç®—åˆ°å€™é€‰ç‚¹çš„è·ç¦»
                candidate_vectors = self.dataset[candidate_ids]
                distances = np.linalg.norm(candidate_vectors - query, axis=1)
                
                # æ’åºå¹¶å–å‰kä¸ª
                sorted_indices = np.argsort(distances)[:k]
                found_ids = {candidate_ids[idx] for idx in sorted_indices}
            else:
                found_ids = set()
            
            query_time = time.time() - start_time
            query_times.append(query_time)
            
            correct = len(true_ids & found_ids)
            total_correct += correct
        
        recall = total_correct / (len(self.queries) * k)
        avg_time = np.mean(query_times) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        return {
            'method': 'Pure K-Means',
            'recall': recall,
            'avg_query_time_ms': avg_time,
            'total_correct': total_correct,
            'total_expected': len(self.queries) * k,
            'n_probe': n_probe_eff,
            'n_clusters': kmeans.n_clusters
        }
    
    def evaluate_kmeans_hnsw(self, kmeans_hnsw: KMeansHNSW, 
                           ground_truth: List[List[int]], 
                           k: int, n_probe: int = 5) -> Dict[str, Any]:
        """è¯„ä¼°K-Means HNSWæ€§èƒ½"""
        print(f"è¯„ä¼°K-Means HNSWæ€§èƒ½ (k={k}, n_probe={n_probe})...")
        
        total_correct = 0
        query_times = []
        
        for i, (query, true_neighbors) in enumerate(zip(self.queries, ground_truth)):
            true_ids = set(true_neighbors)
            
            start_time = time.time()
            results = kmeans_hnsw.search(query, k=k, n_probe=n_probe)
            query_time = time.time() - start_time
            query_times.append(query_time)
            
            found_ids = {node_id for node_id, _ in results}
            correct = len(true_ids & found_ids)
            total_correct += correct
        
        recall = total_correct / (len(self.queries) * k)
        avg_time = np.mean(query_times) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        return {
            'method': 'K-Means HNSW',
            'recall': recall,
            'avg_query_time_ms': avg_time,
            'total_correct': total_correct,
            'total_expected': len(self.queries) * k,
            'n_probe': n_probe,
            'system_stats': kmeans_hnsw.get_stats()
        }
    
    def find_best_parameters(self, hnsw: HNSW, n_clusters_options: List[int] = None) -> Dict[str, Any]:
        """
        å¯»æ‰¾æœ€ä¼˜å‚æ•°ç»„åˆ
        
        è¿™ä¸ªæ–¹æ³•ä¼šæµ‹è¯•ä¸åŒçš„èšç±»æ•°é‡(n_clusters)å’Œæ¢æµ‹æ•°é‡(n_probe)ç»„åˆï¼Œ
        é€šè¿‡è¯„ä¼°å¬å›ç‡æ¥æ‰¾åˆ°æœ€ä¼˜çš„å‚æ•°è®¾ç½®ã€‚
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. æ ¹æ®æ•°æ®é›†å¤§å°è‡ªåŠ¨é€‰æ‹©èšç±»æ•°èŒƒå›´
        2. å¯¹æ¯ä¸ªèšç±»æ•°æµ‹è¯•å¤šä¸ªn_probeå€¼
        3. é€‰æ‹©å¬å›ç‡æœ€é«˜çš„å‚æ•°ç»„åˆ
        
        å‚æ•°é€‰æ‹©ç»éªŒï¼š
        - å°æ•°æ®é›†(â‰¤2000): èšç±»æ•°10-20
        - ä¸­ç­‰æ•°æ®é›†(â‰¤5000): èšç±»æ•°32-64  
        - å¤§æ•°æ®é›†(>5000): èšç±»æ•°64-128
        
        Args:
            hnsw: å·²æ„å»ºçš„HNSWç´¢å¼•
            n_clusters_options: å¯é€‰çš„èšç±»æ•°åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©
            
        Returns:
            Dict[str, Any]: åŒ…å«æœ€ä¼˜å‚æ•°å’Œæ‰€æœ‰æµ‹è¯•ç»“æœçš„å­—å…¸
        """
        if n_clusters_options is None:
            # æ ¹æ®æ•°æ®é›†å¤§å°è‡ªåŠ¨é€‰æ‹©èšç±»æ•°
            if len(self.dataset) <= 2000:
                n_clusters_options = [10, 20]
            elif len(self.dataset) <= 5000:
                n_clusters_options = [32, 64]
            else:
                n_clusters_options = [64, 128]
        
        print(f"å¼€å§‹å‚æ•°ä¼˜åŒ–ï¼Œæµ‹è¯•èšç±»æ•°: {n_clusters_options}")
        
        best_params = None
        best_recall = 0.0
        results_summary = []
        
        # è®¡ç®—ground truth (ç”¨äºä¼˜åŒ–çš„kå€¼)
        k_for_optimization = 10
        ground_truth = self.compute_ground_truth(k_for_optimization)
        
        for n_clusters in n_clusters_options:
            print(f"\næµ‹è¯• n_clusters={n_clusters}")
            
            # æ„å»ºK-Means HNSWç³»ç»Ÿ
            kmeans_hnsw = self.build_kmeans_hnsw(hnsw, n_clusters=n_clusters)
            
            # è¯„ä¼°ä¸åŒn_probeå€¼
            for n_probe in [5, 10]:
                result = self.evaluate_kmeans_hnsw(kmeans_hnsw, ground_truth, k_for_optimization, n_probe)
                result['n_clusters'] = n_clusters
                results_summary.append(result)
                
                # æ›´æ–°æœ€ä½³å‚æ•°
                if result['recall'] > best_recall:
                    best_recall = result['recall']
                    best_params = {
                        'n_clusters': n_clusters,
                        'n_probe': n_probe,
                        'recall': result['recall'],
                        'avg_query_time_ms': result['avg_query_time_ms']
                    }
                
                print(f"  n_probe={n_probe}: recall={result['recall']:.4f}, "
                      f"time={result['avg_query_time_ms']:.2f}ms")
        
        print(f"\næœ€ä¼˜å‚æ•°: {best_params}")
        
        return {
            'best_params': best_params,
            'all_results': results_summary
        }
    
    def run_full_comparison(self, k: int = 10, n_clusters: int = 64) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„ä¸‰æ–¹æ³•æ¯”è¾ƒå®éªŒ
        
        è¿™æ˜¯ä¸»è¦çš„è¯„ä¼°å‡½æ•°ï¼Œæ‰§è¡Œå®Œæ•´çš„æ€§èƒ½æ¯”è¾ƒæµç¨‹ï¼š
        
        1. æ•°æ®å‡†å¤‡é˜¶æ®µï¼š
           - è®¡ç®—ground truthï¼ˆçœŸå®æœ€è¿‘é‚»ï¼‰
           
        2. ç´¢å¼•æ„å»ºé˜¶æ®µï¼š
           - æ„å»ºHNSWç´¢å¼•
           - è®­ç»ƒK-Meansæ¨¡å‹
           - æ„å»ºK-Means HNSWæ··åˆç³»ç»Ÿ
           
        3. å‚æ•°ä¼˜åŒ–é˜¶æ®µï¼š
           - è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜å‚æ•°ç»„åˆ
           - åŸºäºå¬å›ç‡è¿›è¡Œä¼˜åŒ–
           
        4. æ€§èƒ½è¯„ä¼°é˜¶æ®µï¼š
           - æµ‹è¯•ä¸åŒå‚æ•°ä¸‹çš„æ€§èƒ½
           - æ”¶é›†å¬å›ç‡å’ŒæŸ¥è¯¢æ—¶é—´æ•°æ®
           
        Args:
            k: è¿”å›çš„æœ€è¿‘é‚»æ•°é‡ï¼Œå½±å“å¬å›ç‡è®¡ç®—
            n_clusters: K-Meansèšç±»æ•°é‡ï¼Œå½±å“èšç±»è´¨é‡
            
        Returns:
            Dict[str, Any]: åŒ…å«æ‰€æœ‰è¯„ä¼°ç»“æœçš„å®Œæ•´æŠ¥å‘Š
            
        æ—¶é—´å¤æ‚åº¦ï¼š
            - Ground truth: O(n*m*d)  
            - HNSWæ„å»º: O(n*log(n)*d)
            - K-Meansè®­ç»ƒ: O(k*n*d*i)
            å…¶ä¸­ n=æ•°æ®é›†å¤§å°, m=æŸ¥è¯¢æ•°é‡, d=ç»´åº¦, k=èšç±»æ•°, i=è¿­ä»£æ¬¡æ•°
        """
        print("=" * 60)
        print("å¼€å§‹ä¸‰ç§æ–¹æ³•çš„å®Œæ•´æ€§èƒ½æ¯”è¾ƒ")
        print("=" * 60)
        
        # 1. è®¡ç®—ground truth
        ground_truth = self.compute_ground_truth(k)
        
        # 2. æ„å»ºæ‰€æœ‰ç´¢å¼•
        hnsw = self.build_hnsw()
        kmeans = self.build_pure_kmeans(n_clusters)
        
        # 3. å‚æ•°ä¼˜åŒ–
        optimization_result = self.find_best_parameters(hnsw, [n_clusters])
        best_params = optimization_result['best_params']
        
        # 4. ä½¿ç”¨æœ€ä¼˜å‚æ•°æ„å»ºæœ€ç»ˆç³»ç»Ÿ
        kmeans_hnsw = self.build_kmeans_hnsw(hnsw, 
                                           n_clusters=best_params['n_clusters'],
                                           k_children=200, 
                                           child_search_ef=300)
        
        # 5. æœ€ç»ˆæ€§èƒ½è¯„ä¼°
        results = []
        
        # HNSWè¯„ä¼° (å¤šä¸ªefå€¼)
        for ef in [50, 100, 200]:
            result = self.evaluate_hnsw(hnsw, ground_truth, k, ef)
            results.append(result)
        
        # çº¯K-Meansè¯„ä¼° (å¤šä¸ªn_probeå€¼)
        for n_probe in [1, 5, 10]:
            result = self.evaluate_pure_kmeans(kmeans, ground_truth, k, n_probe)
            results.append(result)
        
        # K-Means HNSWè¯„ä¼° (å¤šä¸ªn_probeå€¼)
        for n_probe in [1, 5, 10]:
            result = self.evaluate_kmeans_hnsw(kmeans_hnsw, ground_truth, k, n_probe)
            results.append(result)
        
        return {
            'results': results,
            'optimization': optimization_result,
            'config': {
                'k': k,
                'n_clusters': n_clusters,
                'dataset_size': len(self.dataset),
                'query_size': len(self.queries),
                'dimension': self.dimension
            }
        }
    
    def print_results(self, comparison_results: Dict[str, Any]):
        """
        æ‰“å°æ ¼å¼åŒ–çš„æ¯”è¾ƒç»“æœ
        
        ä»¥ç”¨æˆ·å‹å¥½çš„æ–¹å¼å±•ç¤ºæ€§èƒ½æ¯”è¾ƒç»“æœï¼ŒåŒ…æ‹¬ï¼š
        - å®éªŒé…ç½®ä¿¡æ¯
        - å„ç®—æ³•åœ¨ä¸åŒå‚æ•°ä¸‹çš„è¡¨ç°
        - æœ€ä½³æ€§èƒ½ç®—æ³•æ¨è
        
        è¾“å‡ºæ ¼å¼ï¼š
        - æŒ‰ç®—æ³•åˆ†ç»„æ˜¾ç¤ºç»“æœ
        - çªå‡ºæ˜¾ç¤ºå…³é”®æ€§èƒ½æŒ‡æ ‡
        - æä¾›æœ€ä½³é€‰æ‹©å»ºè®®
        
        Args:
            comparison_results: run_full_comparison()è¿”å›çš„ç»“æœå­—å…¸
            
        æ˜¾ç¤ºä¿¡æ¯åŒ…æ‹¬ï¼š
        - å¬å›ç‡ï¼šç®—æ³•å‡†ç¡®æ€§çš„æ ¸å¿ƒæŒ‡æ ‡
        - æŸ¥è¯¢æ—¶é—´ï¼šç®—æ³•æ•ˆç‡çš„é‡è¦æŒ‡æ ‡  
        - å‚æ•°è®¾ç½®ï¼šå¸®åŠ©ç†è§£æ€§èƒ½å·®å¼‚
        """
        print("\n" + "=" * 80)
        print("æ€§èƒ½æ¯”è¾ƒç»“æœ")
        print("=" * 80)
        
        results = comparison_results['results']
        config = comparison_results['config']
        
        print(f"é…ç½®: k={config['k']}, èšç±»æ•°={config['n_clusters']}")
        print(f"æ•°æ®é›†å¤§å°: {config['dataset_size']}, æŸ¥è¯¢æ•°é‡: {config['query_size']}")
        print()
        
        # æŒ‰æ–¹æ³•åˆ†ç»„æ˜¾ç¤º
        methods = {}
        for result in results:
            method = result['method']
            if method not in methods:
                methods[method] = []
            methods[method].append(result)
        
        for method_name, method_results in methods.items():
            print(f"{method_name}:")
            for result in method_results:
                param_str = ""
                if 'ef' in result:
                    param_str = f"ef={result['ef']}"
                elif 'n_probe' in result:
                    param_str = f"n_probe={result['n_probe']}"
                
                print(f"  {param_str:12} å¬å›ç‡: {result['recall']:.4f}, "
                      f"å¹³å‡æŸ¥è¯¢æ—¶é—´: {result['avg_query_time_ms']:.2f}ms")
            print()
        
        # æ˜¾ç¤ºæœ€ä½³ç»“æœå’Œç®—æ³•æ¨è
        best_result = max(results, key=lambda x: x['recall'])
        print(f"ğŸ† æœ€ä½³å¬å›ç‡: {best_result['method']} - {best_result['recall']:.4f} "
              f"(æŸ¥è¯¢æ—¶é—´: {best_result['avg_query_time_ms']:.2f}ms)")
        
        # æä¾›ç®—æ³•é€‰æ‹©å»ºè®®
        print(f"\nğŸ’¡ ç®—æ³•é€‰æ‹©å»ºè®®:")
        fastest = min(results, key=lambda x: x['avg_query_time_ms'])
        print(f"   é€Ÿåº¦ä¼˜å…ˆ: {fastest['method']} ({fastest['avg_query_time_ms']:.2f}ms)")
        print(f"   ç²¾åº¦ä¼˜å…ˆ: {best_result['method']} (å¬å›ç‡ {best_result['recall']:.4f})")
        
        # è®¡ç®—å¹³è¡¡æ€§æ¨èï¼ˆå¬å›ç‡ Ã— é€Ÿåº¦å€’æ•°çš„åŠ æƒï¼‰
        balanced_scores = []
        for result in results:
            if result['avg_query_time_ms'] > 0:  # é¿å…é™¤é›¶
                balance_score = result['recall'] * (1000 / result['avg_query_time_ms'])
                balanced_scores.append((balance_score, result))
        
        if balanced_scores:
            best_balanced = max(balanced_scores, key=lambda x: x[0])[1]
            print(f"   å¹³è¡¡æ¨è: {best_balanced['method']} "
                  f"(å¬å›ç‡ {best_balanced['recall']:.4f}, "
                  f"æ—¶é—´ {best_balanced['avg_query_time_ms']:.2f}ms)")


def load_sift_data(max_base: int = 10000, max_query: int = 100):
    """
    åŠ è½½SIFTæ•°æ®é›†
    
    SIFT(Scale-Invariant Feature Transform)æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ç»å…¸ç‰¹å¾æè¿°å­ï¼Œ
    å¸¸ç”¨äºå‘é‡æœç´¢ç®—æ³•çš„æ€§èƒ½è¯„ä¼°ã€‚è¯¥æ•°æ®é›†åŒ…å«ï¼š
    - åŸºç¡€å‘é‡ï¼šç”¨äºæ„å»ºç´¢å¼•çš„å‘é‡é›†åˆ
    - æŸ¥è¯¢å‘é‡ï¼šç”¨äºæµ‹è¯•æœç´¢æ€§èƒ½çš„æŸ¥è¯¢é›†åˆ
    - 128ç»´æµ®ç‚¹å‘é‡
    
    æ•°æ®æ ¼å¼ï¼š
    - .fvecsæ–‡ä»¶ï¼šFAISSæ ‡å‡†æ ¼å¼
    - æ¯ä¸ªå‘é‡å‰æœ‰ä¸€ä¸ªint32è¡¨ç¤ºç»´åº¦
    - éšåæ˜¯dimä¸ªfloat32æ•°å€¼
    
    Args:
        max_base: æœ€å¤§åŸºç¡€å‘é‡æ•°é‡ï¼Œç”¨äºæ§åˆ¶å†…å­˜ä½¿ç”¨
        max_query: æœ€å¤§æŸ¥è¯¢å‘é‡æ•°é‡ï¼Œç”¨äºæ§åˆ¶æµ‹è¯•è§„æ¨¡
        
    Returns:
        Tuple[np.ndarray, np.ndarray]: (åŸºç¡€å‘é‡, æŸ¥è¯¢å‘é‡) æˆ– (None, None)
        
    æ³¨æ„ï¼š
        å¦‚æœSIFTæ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œå‡½æ•°ä¼šè¿”å›Noneï¼Œ
        è°ƒç”¨è€…åº”è¯¥fallbackåˆ°åˆæˆæ•°æ®ã€‚
    """
    sift_dir = os.path.join(os.path.dirname(__file__), '..', 'sift')
    
    try:
        def read_fvecs(path: str, max_vectors: int = None) -> np.ndarray:
            """è¯»å–.fvecsæ–‡ä»¶"""
            if not os.path.exists(path):
                raise FileNotFoundError(path)
            raw = np.fromfile(path, dtype=np.int32)
            if raw.size == 0:
                raise ValueError(f"Empty file: {path}")
            dim = raw[0]
            record_size = dim + 1
            count = raw.size // record_size
            raw = raw.reshape(count, record_size)
            vecs = raw[:, 1:].astype(np.float32)
            if max_vectors and count > max_vectors:
                vecs = vecs[:max_vectors]
            return vecs
        
        base_path = os.path.join(sift_dir, 'sift_base.fvecs')
        query_path = os.path.join(sift_dir, 'sift_query.fvecs')
        
        base_vectors = read_fvecs(base_path, max_base)
        query_vectors = read_fvecs(query_path, max_query)
        
        print(f"æˆåŠŸåŠ è½½SIFTæ•°æ®: {len(base_vectors)}ä¸ªåŸºç¡€å‘é‡, {len(query_vectors)}ä¸ªæŸ¥è¯¢å‘é‡")
        return base_vectors, query_vectors
        
    except Exception as e:
        print(f"åŠ è½½SIFTæ•°æ®å¤±è´¥: {e}")
        return None, None


def create_synthetic_data(n_base: int = 10000, n_query: int = 100, dim: int = 128):
    """
    åˆ›å»ºåˆæˆæ•°æ®é›†
    
    å½“SIFTæ•°æ®ä¸å¯ç”¨æ—¶ï¼Œç”Ÿæˆéšæœºçš„é«˜æ–¯åˆ†å¸ƒå‘é‡æ•°æ®ã€‚
    åˆæˆæ•°æ®çš„ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨æ ‡å‡†æ­£æ€åˆ†å¸ƒç”Ÿæˆ
    - å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
    - å‘é‡é—´è·ç¦»ç¬¦åˆé«˜ç»´ç©ºé—´çš„ç»Ÿè®¡ç‰¹æ€§
    
    ä¼˜ç‚¹ï¼š
    - ä¸ä¾èµ–å¤–éƒ¨æ•°æ®æ–‡ä»¶
    - å¯ä»¥çµæ´»æ§åˆ¶æ•°æ®è§„æ¨¡å’Œç»´åº¦
    - ç”Ÿæˆé€Ÿåº¦å¿«
    
    å±€é™ï¼š
    - ç¼ºä¹çœŸå®æ•°æ®çš„å¤æ‚åˆ†å¸ƒç‰¹æ€§
    - å¯èƒ½æ— æ³•åæ˜ å®é™…åº”ç”¨åœºæ™¯
    
    Args:
        n_base: åŸºç¡€å‘é‡æ•°é‡
        n_query: æŸ¥è¯¢å‘é‡æ•°é‡  
        dim: å‘é‡ç»´åº¦
        
    Returns:
        Tuple[np.ndarray, np.ndarray]: (åŸºç¡€å‘é‡, æŸ¥è¯¢å‘é‡)
    """
    print(f"åˆ›å»ºåˆæˆæ•°æ®: {n_base}ä¸ªåŸºç¡€å‘é‡, {n_query}ä¸ªæŸ¥è¯¢å‘é‡, ç»´åº¦={dim}")
    
    np.random.seed(42)
    base_vectors = np.random.randn(n_base, dim).astype(np.float32)
    query_vectors = np.random.randn(n_query, dim).astype(np.float32)
    
    return base_vectors, query_vectors


def save_results(results: Dict[str, Any], filename: str):
    """
    ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    
    å°†è¯„ä¼°ç»“æœåºåˆ—åŒ–å¹¶ä¿å­˜åˆ°æ–‡ä»¶ï¼Œä¾¿äºåç»­åˆ†æå’Œæ¯”è¾ƒã€‚
    å¤„ç†äº†numpyæ•°æ®ç±»å‹çš„åºåˆ—åŒ–é—®é¢˜ï¼Œç¡®ä¿JSONå…¼å®¹æ€§ã€‚
    
    ä¿å­˜çš„æ•°æ®åŒ…æ‹¬ï¼š
    - æ‰€æœ‰ç®—æ³•çš„æ€§èƒ½æŒ‡æ ‡
    - å‚æ•°ä¼˜åŒ–ç»“æœ
    - å®éªŒé…ç½®ä¿¡æ¯
    - æ—¶é—´æˆ³ä¿¡æ¯
    
    Args:
        results: åŒ…å«è¯„ä¼°ç»“æœçš„å­—å…¸
        filename: ä¿å­˜æ–‡ä»¶åï¼ˆå»ºè®®ä½¿ç”¨.jsonæ‰©å±•åï¼‰
        
    æ³¨æ„ï¼š
        å‡½æ•°ä¼šè‡ªåŠ¨å¤„ç†numpyæ•°ç»„å’Œæ ‡é‡çš„ç±»å‹è½¬æ¢ï¼Œ
        ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½èƒ½æ­£ç¡®åºåˆ—åŒ–ä¸ºJSONæ ¼å¼ã€‚
    """
    def convert_numpy(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, dict):
            return {k: convert_numpy(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy(item) for item in obj]
        return obj
    
    results_serializable = convert_numpy(results)
    
    with open(filename, 'w') as f:
        json.dump(results_serializable, f, indent=2)
    
    print(f"ç»“æœå·²ä¿å­˜åˆ° {filename}")


if __name__ == "__main__":
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(
        description="K-Means HNSW æ€§èƒ½æ¯”è¾ƒå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python simple_comparison.py                          # é»˜è®¤å‚æ•°å¿«é€Ÿæµ‹è¯•
  python simple_comparison.py --use-sift               # ä½¿ç”¨SIFTæ•°æ®é›†
  python simple_comparison.py --dataset-size 20000     # æŒ‡å®šæ•°æ®é›†å¤§å°
  python simple_comparison.py --save-results           # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
  
ç®—æ³•è¯´æ˜:
  HNSW        - åˆ†å±‚å¯å¯¼èˆªå°ä¸–ç•Œå›¾ï¼Œé«˜ç²¾åº¦ä½†å†…å­˜æ¶ˆè€—å¤§
  K-Means     - åŸºäºèšç±»çš„æœç´¢ï¼Œé€Ÿåº¦å¿«ä½†ç²¾åº¦ä¸­ç­‰  
  Mixed       - æ··åˆç®—æ³•ï¼Œå¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦
        """)
    
    parser.add_argument('--dataset-size', type=int, default=5000, 
                       help='æ•°æ®é›†å¤§å°ï¼ˆå‘é‡æ•°é‡ï¼‰')
    parser.add_argument('--query-size', type=int, default=50, 
                       help='æŸ¥è¯¢å‘é‡æ•°é‡')
    parser.add_argument('--dimension', type=int, default=128, 
                       help='å‘é‡ç»´åº¦(åˆæˆæ•°æ®)')
    parser.add_argument('--k', type=int, default=10, 
                       help='è¿”å›çš„æœ€è¿‘é‚»æ•°é‡')
    parser.add_argument('--n-clusters', type=int, default=64, 
                       help='K-Meansèšç±»æ•°')
    parser.add_argument('--use-sift', action='store_true', 
                       help='ä½¿ç”¨SIFTæ•°æ®é›†')
    parser.add_argument('--save-results', action='store_true', 
                       help='ä¿å­˜ç»“æœåˆ°æ–‡ä»¶')
    
    args = parser.parse_args()
    
    # è·ç¦»å‡½æ•°ï¼ˆæ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰
    distance_func = lambda x, y: np.linalg.norm(x - y)
    
    # åŠ è½½æ•°æ®
    if args.use_sift:
        print(f"\nğŸ“Š å°è¯•åŠ è½½SIFTæ•°æ®é›†...")
        base_vectors, query_vectors = load_sift_data(args.dataset_size, args.query_size)
        if base_vectors is None:
            print("âŒ SIFTæ•°æ®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨åˆæˆæ•°æ®")
            base_vectors, query_vectors = create_synthetic_data(
                args.dataset_size, args.query_size, args.dimension
            )
        else:
            print("âœ… SIFTæ•°æ®åŠ è½½æˆåŠŸ")
    else:
        print(f"\nğŸ² ç”Ÿæˆåˆæˆæ•°æ®é›†...")
        base_vectors, query_vectors = create_synthetic_data(
            args.dataset_size, args.query_size, args.dimension
        )
    
    print(f"\nğŸ”§ å®éªŒé…ç½®:")
    print(f"   æ•°æ®é›†å¤§å°: {len(base_vectors):,} å‘é‡")
    print(f"   æŸ¥è¯¢æ•°é‡: {len(query_vectors):,} ä¸ª")
    print(f"   å‘é‡ç»´åº¦: {base_vectors.shape[1]} ç»´")
    print(f"   å¬å›ç‡è¯„ä¼°: Recall@{args.k}")
    print(f"   èšç±»æ•°é‡: {args.n_clusters}")
    
    # åˆ›å»ºæ¯”è¾ƒå™¨å®ä¾‹å¹¶è¿è¡Œå®Œæ•´æ¯”è¾ƒ
    comparator = SimpleComparator(base_vectors, query_vectors, distance_func)
    results = comparator.run_full_comparison(k=args.k, n_clusters=args.n_clusters)
    comparator.print_results(results)
    
    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.save_results:
        results['timestamp'] = time.strftime('%Y-%m-%d %H:%M:%S')
        save_results(results, 'simple_comparison_results.json')
    
    print("\nğŸ‰ ç®—æ³•æ¯”è¾ƒå®Œæˆï¼")
