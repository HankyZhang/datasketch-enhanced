"""
æ–¹æ³•3å‚æ•°è°ƒä¼˜ï¼šK-Means HNSWç³»ç»Ÿ (Method 3 Parameter Tuning: K-Means HNSW System)

æœ¬æ¨¡å—æä¾›K-Means HNSWç³»ç»Ÿçš„å‚æ•°è°ƒä¼˜å’Œä¼˜åŒ–åŠŸèƒ½ã€‚
åŒ…å«å…¨é¢çš„è¯„ä¼°ã€å‚æ•°æ‰«æå’Œæ€§èƒ½åˆ†æã€‚

åŠŸèƒ½ç‰¹æ€§:
- å…¨é¢çš„å‚æ•°æ‰«æå’Œä¼˜åŒ–
- åŸºå‡†å¯¹æ¯”è¯„ä¼° (HNSWåŸºçº¿ã€çº¯K-Meansã€K-Means HNSW)
- å¬å›ç‡å’ŒæŸ¥è¯¢æ—¶é—´åˆ†æ
- è‡ªé€‚åº”å‚æ•°è°ƒæ•´
- ç»“æœä¿å­˜å’Œåˆ†æ

This module provides parameter tuning and optimization for the K-Means HNSW system.
It includes comprehensive evaluation, parameter sweeps, and performance analysis.
"""

import os
import sys
import time
import json
import argparse
import random
import traceback
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from itertools import product

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ (Add parent directory to path)
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from method3.kmeans_hnsw import KMeansHNSW
from hnsw.hnsw import HNSW
# å¼•å…¥ Hybrid HNSW ç»“æ„ç”¨äºå¯¹æ¯”è¯„ä¼° (Import Hybrid HNSW for comparative evaluation)
from hybrid_hnsw.hnsw_hybrid import HNSWHybrid
# ä½¿ç”¨sklearn MiniBatchKMeansä½œä¸ºçº¯k-meansåŸºçº¿ (Switch to sklearn MiniBatchKMeans for pure k-means baseline)
from sklearn.cluster import MiniBatchKMeans


class KMeansHNSWMultiPivot:
    """Multi-Pivot K-Means HNSWç³»ç»Ÿ - é›†æˆåˆ°v1.pyä¸­
    
    ä¸KMeansHNSWå…±äº«ç›¸åŒçš„HNSWç´¢å¼•å’ŒK-Meansèšç±»ï¼Œä»…åœ¨å­èŠ‚ç‚¹æŸ¥æ‰¾ç­–ç•¥ä¸Šä¸åŒã€‚
    ä½¿ç”¨å¤šä¸ªæ¢çº½ç‚¹æ¥ä¸°å¯Œæ¯ä¸ªè´¨å¿ƒçš„å­èŠ‚ç‚¹é€‰æ‹©ã€‚
    """
    
    def __init__(
        self,
        base_index: HNSW,
        n_clusters: int = 100,
        k_children: int = 800,
        child_search_ef: Optional[int] = None,
        # Multi-pivot specific parameters
        num_pivots: int = 3,
        pivot_selection_strategy: str = 'line_perp_third',
        pivot_overquery_factor: float = 1.2,
        # Adaptive/repair options (same as KMeansHNSW)
        adaptive_k_children: bool = False,
        k_children_scale: float = 1.5,
        k_children_min: int = 100,
        k_children_max: Optional[int] = None,
        diversify_max_assignments: Optional[int] = None,
        repair_min_assignments: Optional[int] = None,
        # Shared K-Means support (æ–°å¢å…±äº«æ”¯æŒ)
        shared_kmeans_model: Optional[MiniBatchKMeans] = None,
        shared_dataset_vectors: Optional[np.ndarray] = None
    ):
        self.base_index = base_index
        self.n_clusters = n_clusters
        self.k_children = k_children
        self.distance_func = base_index._distance_func
        
        # Multi-pivot parameters
        self.num_pivots = max(1, num_pivots)
        self.pivot_selection_strategy = pivot_selection_strategy
        self.pivot_overquery_factor = max(1.0, pivot_overquery_factor)
        
        # Adaptive/repair options
        self.adaptive_k_children = adaptive_k_children
        self.k_children_scale = k_children_scale
        self.k_children_min = k_children_min
        self.k_children_max = k_children_max
        self.diversify_max_assignments = diversify_max_assignments
        self.repair_min_assignments = repair_min_assignments
        
        # Shared K-Means support (æ–°å¢å…±äº«æ”¯æŒ)
        self.shared_kmeans_model = shared_kmeans_model
        self.shared_dataset_vectors = shared_dataset_vectors
        
        # Child search ef
        if child_search_ef is None:
            min_ef = max(k_children + 50, int(k_children * 1.2))
            self.child_search_ef = min_ef
        else:
            self.child_search_ef = child_search_ef
        
        # Core components (will be built)
        self.kmeans_model = None
        self.centroids = None
        self.centroid_ids = []
        self.parent_child_map = {}
        self.child_vectors = {}
        self._centroid_matrix = None
        
        # Stats tracking
        self.stats = {
            'method': 'multi_pivot_kmeans_hnsw',
            'n_clusters': n_clusters,
            'k_children': k_children,
            'child_search_ef': self.child_search_ef,
            'num_pivots': self.num_pivots,
            'pivot_strategy': self.pivot_selection_strategy,
            'pivot_overquery_factor': self.pivot_overquery_factor,
            'shared_kmeans_used': shared_kmeans_model is not None,
            'shared_data_used': shared_dataset_vectors is not None
        }
        self.search_times = []
        
        # Build the system
        self._build_system()
    
    def _build_system(self):
        """æ„å»ºå¤šæ¢çº½K-Means HNSWç³»ç»Ÿ"""
        shared_info = ""
        if self.shared_kmeans_model is not None:
            shared_info += " (å…±äº«K-Meansæ¨¡å‹)"
        if self.shared_dataset_vectors is not None:
            shared_info += " (å…±äº«æ•°æ®å‘é‡)"
            
        print(f"Building Multi-Pivot K-Means HNSW system with {self.n_clusters} clusters, {self.num_pivots} pivots{shared_info}...")
        
        # Step 1: Extract vectors from HNSW index
        self._extract_dataset_vectors()
        
        # Step 2: Perform K-Means clustering
        self._perform_kmeans_clustering()
        
        # Step 3: Assign children using multi-pivot strategy
        self._assign_children_via_multi_pivot()
        
        # Step 4: Build centroid index for fast search
        self._build_centroid_index()
        
        print(f"Multi-Pivot K-Means HNSW system built with {len(self.parent_child_map)} centroids")
    
    def _extract_dataset_vectors(self):
        """ä»HNSWç´¢å¼•æå–å‘é‡æ•°æ® (æ”¯æŒå…±äº«æ•°æ®å‘é‡)"""
        if self.shared_dataset_vectors is not None:
            print("  Using shared dataset vectors...")
            self.dataset_vectors = self.shared_dataset_vectors
            return
        
        dataset_vectors = []
        for node_id, node in self.base_index._nodes.items():
            vector = node.point
            if vector is not None:
                dataset_vectors.append(vector)
        self.dataset_vectors = np.array(dataset_vectors)
    
    def _perform_kmeans_clustering(self):
        """æ‰§è¡ŒK-Meansèšç±» (æ”¯æŒå…±äº«æ¨¡å‹)"""
        # Multi-Pivotå¿…é¡»ä½¿ç”¨å…±äº«çš„K-Meansæ¨¡å‹ (Multi-Pivot must use shared K-Means model)
        if self.shared_kmeans_model is None:
            raise ValueError("Multi-Pivot KMeans HNSW requires a shared_kmeans_model. "
                           "Please provide a pre-trained MiniBatchKMeans model.")
        
        print("  Using shared MiniBatchKMeans model...")
        self.kmeans_model = self.shared_kmeans_model
        self.centroids = self.kmeans_model.cluster_centers_
        self.n_clusters = self.centroids.shape[0]
        self.centroid_ids = [f"centroid_{i}" for i in range(self.n_clusters)]
        
        # è‡ªé€‚åº”è°ƒæ•´k_children (åŸºäºå…±äº«æ¨¡å‹çš„èšç±»æ•°é‡)
        if self.adaptive_k_children:
            avg_cluster_size = len(self.dataset_vectors) / self.n_clusters
            adaptive_k = int(avg_cluster_size * self.k_children_scale)
            adaptive_k = max(self.k_children_min, adaptive_k)
            if self.k_children_max:
                adaptive_k = min(self.k_children_max, adaptive_k)
            self.k_children = adaptive_k
            print(f"  è‡ªé€‚åº”è°ƒæ•´k_children: {self.k_children} (å¹³å‡èšç±»å¤§å°: {avg_cluster_size:.1f})")
        
        print(f"Shared K-Means clustering loaded with {self.n_clusters} clusters")
    
    def _assign_children_via_multi_pivot(self):
        """ä½¿ç”¨å¤šæ¢çº½ç­–ç•¥åˆ†é…å­èŠ‚ç‚¹"""
        print(f"Assigning children via multi-pivot HNSW (pivots={self.num_pivots}, ef={self.child_search_ef})...")
        
        if self.num_pivots == 1:
            # é€€åŒ–åˆ°å•æ¢çº½
            self._assign_children_single_pivot()
            return
        
        k_each_query = max(self.k_children, int(self.k_children * self.pivot_overquery_factor))
        assignment_counts = {} if self.repair_min_assignments else None
        
        for idx_c, centroid_id in enumerate(self.centroid_ids):
            centroid_vector = self.centroids[idx_c]
            
            # ç¬¬ä¸€ä¸ªæ¢çº½ï¼šè´¨å¿ƒæœ¬èº«
            pivots = [centroid_vector]
            
            # ç¬¬ä¸€æ¬¡æŸ¥è¯¢ï¼šä»è´¨å¿ƒå¼€å§‹
            neighbors_A = self.base_index.query(centroid_vector, k=k_each_query, ef=self.child_search_ef)
            S_A = [node_id for node_id, _ in neighbors_A]
            all_candidates = set(S_A)
            
            # å­˜å‚¨å‘é‡ç”¨äºåç»­è®¡ç®—
            for node_id in S_A:
                if node_id not in self.child_vectors:
                    if node_id in self.base_index._nodes:
                        self.child_vectors[node_id] = self.base_index._nodes[node_id].point
            
            if not S_A:
                self.parent_child_map[centroid_id] = []
                continue
            
            # æ·»åŠ æ›´å¤šæ¢çº½
            for pivot_idx in range(1, self.num_pivots):
                if pivot_idx == 1:
                    # ç¬¬äºŒä¸ªæ¢çº½ï¼šè·ç¦»è´¨å¿ƒæœ€è¿œçš„ç‚¹
                    farthest_node, farthest_vec = self._find_farthest_from_centroid(centroid_vector, S_A)
                    if farthest_vec is not None:
                        pivots.append(farthest_vec)
                elif pivot_idx == 2 and self.pivot_selection_strategy == 'line_perp_third':
                    # ç¬¬ä¸‰ä¸ªæ¢çº½ï¼šå‚ç›´è·ç¦»æœ€å¤§çš„ç‚¹
                    perp_vec = self._find_perpendicular_pivot(pivots[0], pivots[1], all_candidates)
                    if perp_vec is not None:
                        pivots.append(perp_vec)
                else:
                    # åç»­æ¢çº½ï¼šæœ€å¤§æœ€å°è·ç¦»ç­–ç•¥
                    max_min_vec = self._find_max_min_distance_pivot(pivots, all_candidates)
                    if max_min_vec is not None:
                        pivots.append(max_min_vec)
                    else:
                        break
                
                # ä»æ–°æ¢çº½æŸ¥è¯¢æ›´å¤šå€™é€‰
                if len(pivots) > pivot_idx:
                    new_neighbors = self.base_index.query(pivots[-1], k=k_each_query, ef=self.child_search_ef)
                    for node_id, _ in new_neighbors:
                        all_candidates.add(node_id)
                        if node_id not in self.child_vectors:
                            if node_id in self.base_index._nodes:
                                self.child_vectors[node_id] = self.base_index._nodes[node_id].point
            
            # ä»æ‰€æœ‰å€™é€‰ä¸­é€‰æ‹©æœ€ä½³çš„k_childrenä¸ª
            selected_children = self._select_best_children_from_candidates(
                list(all_candidates), pivots, self.k_children
            )
            
            # åº”ç”¨å¤šæ ·åŒ–è¿‡æ»¤
            if self.diversify_max_assignments and assignment_counts is not None:
                selected_children = self._apply_diversify_filter(
                    selected_children, assignment_counts, self.diversify_max_assignments
                )
            
            self.parent_child_map[centroid_id] = selected_children
            
            # æ›´æ–°åˆ†é…è®¡æ•°
            if assignment_counts is not None:
                for child_id in selected_children:
                    assignment_counts[child_id] = assignment_counts.get(child_id, 0) + 1
            
            if (idx_c + 1) % 10 == 0:
                print(f"  Processed {idx_c + 1}/{self.n_clusters} centroids")
        
        # ä¿®å¤åˆ†é…
        if self.repair_min_assignments and assignment_counts is not None:
            self._repair_child_assignments(assignment_counts)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        total_children = sum(len(children) for children in self.parent_child_map.values())
        self.stats['num_children'] = len(self.child_vectors)
        self.stats['avg_children_per_centroid'] = total_children / max(1, self.n_clusters)
        print(f"Multi-pivot assignment completed: {total_children} assignments, {len(self.child_vectors)} unique children")
    
    def _assign_children_single_pivot(self):
        """å•æ¢çº½åˆ†é… (é€€åŒ–æƒ…å†µ)"""
        print("Using single-pivot assignment (fallback)...")
        assignment_counts = {} if self.repair_min_assignments else None
        
        for idx_c, centroid_id in enumerate(self.centroid_ids):
            centroid_vector = self.centroids[idx_c]
            results = self.base_index.query(centroid_vector, k=self.k_children, ef=self.child_search_ef)
            children = [node_id for node_id, _ in results]
            
            # åº”ç”¨å¤šæ ·åŒ–è¿‡æ»¤
            if self.diversify_max_assignments and assignment_counts is not None:
                children = self._apply_diversify_filter(
                    children, assignment_counts, self.diversify_max_assignments
                )
            
            self.parent_child_map[centroid_id] = children
            
            # å­˜å‚¨å­èŠ‚ç‚¹å‘é‡
            for child_id in children:
                if child_id not in self.child_vectors:
                    self.child_vectors[child_id] = self.base_index[child_id]
            
            # æ›´æ–°åˆ†é…è®¡æ•°
            if assignment_counts is not None:
                for child_id in children:
                    assignment_counts[child_id] = assignment_counts.get(child_id, 0) + 1
        
        # ä¿®å¤åˆ†é…
        if self.repair_min_assignments and assignment_counts is not None:
            self._repair_child_assignments(assignment_counts)
    
    def _find_farthest_from_centroid(self, centroid_vector, candidates):
        """æ‰¾åˆ°è·ç¦»è´¨å¿ƒæœ€è¿œçš„å€™é€‰ç‚¹"""
        max_distance = -1
        farthest_node = None
        farthest_vec = None
        
        for node_id in candidates:
            if node_id in self.base_index._nodes:
                node_vector = self.base_index._nodes[node_id].point
            else:
                continue
            distance = self.distance_func(centroid_vector, node_vector)
            if distance > max_distance:
                max_distance = distance
                farthest_node = node_id
                farthest_vec = node_vector
        
        return farthest_node, farthest_vec
    
    def _find_perpendicular_pivot(self, pivot_a, pivot_b, candidates):
        """æ‰¾åˆ°å‚ç›´äºA-Bçº¿æ®µè·ç¦»æœ€å¤§çš„ç‚¹"""
        ab_vector = pivot_b - pivot_a
        ab_norm_sq = np.dot(ab_vector, ab_vector)
        
        if ab_norm_sq < 1e-12:
            return None
        
        max_perp_distance = -1
        best_vector = None
        
        for node_id in candidates:
            node_vector = self.child_vectors.get(node_id)
            if node_vector is None:
                node_vector = self.base_index._nodes[node_id].point
            
            # è®¡ç®—å‚ç›´è·ç¦»
            ac_vector = node_vector - pivot_a
            projection_coeff = np.dot(ac_vector, ab_vector) / ab_norm_sq
            projection = projection_coeff * ab_vector
            perpendicular = ac_vector - projection
            perp_distance = np.linalg.norm(perpendicular)
            
            if perp_distance > max_perp_distance:
                max_perp_distance = perp_distance
                best_vector = node_vector
        
        return best_vector
    
    def _find_max_min_distance_pivot(self, existing_pivots, candidates):
        """æ‰¾åˆ°ä¸ç°æœ‰æ¢çº½æœ€å°è·ç¦»æœ€å¤§çš„å€™é€‰ç‚¹"""
        best_score = -1
        best_vector = None
        
        for node_id in candidates:
            node_vector = self.child_vectors.get(node_id)
            if node_vector is None:
                node_vector = self.base_index._nodes[node_id].point
            
            # è®¡ç®—åˆ°æ‰€æœ‰ç°æœ‰æ¢çº½çš„æœ€å°è·ç¦»
            min_distance = min(
                self.distance_func(node_vector, pivot) for pivot in existing_pivots
            )
            
            if min_distance > best_score:
                best_score = min_distance
                best_vector = node_vector
        
        return best_vector
    
    def _select_best_children_from_candidates(self, candidate_ids, pivots, k_children):
        """ä»å€™é€‰èŠ‚ç‚¹ä¸­é€‰æ‹©æœ€ä¼˜çš„k_childrenä¸ªå­èŠ‚ç‚¹"""
        if len(candidate_ids) <= k_children:
            return candidate_ids
        
        # è®¡ç®—æ¯ä¸ªå€™é€‰ç‚¹åˆ°æœ€è¿‘æ¢çº½çš„è·ç¦»
        scores = []
        for node_id in candidate_ids:
            node_vector = self.child_vectors.get(node_id)
            if node_vector is None:
                node_vector = self.base_index._nodes[node_id].point
            
            min_distance = min(
                self.distance_func(node_vector, pivot) for pivot in pivots
            )
            scores.append((min_distance, node_id))
        
        # é€‰æ‹©è·ç¦»æœ€å°çš„k_childrenä¸ª
        scores.sort()
        return [node_id for _, node_id in scores[:k_children]]
    
    def _apply_diversify_filter(self, children, assignment_counts, max_assignments):
        """åº”ç”¨å¤šæ ·åŒ–è¿‡æ»¤å™¨"""
        filtered_children = []
        for child_id in children:
            current_count = assignment_counts.get(child_id, 0)
            if current_count < max_assignments:
                filtered_children.append(child_id)
        return filtered_children
    
    def _repair_child_assignments(self, assignment_counts):
        """ä¿®å¤å­èŠ‚ç‚¹åˆ†é…ä»¥ç¡®ä¿æœ€å°è¦†ç›–"""
        print(f"Multi-Pivot Repair phase: ensuring minimum {self.repair_min_assignments} assignments...")
        
        all_base_nodes = set(self.base_index.keys())
        assigned_nodes = set(assignment_counts.keys())
        unassigned_nodes = all_base_nodes - assigned_nodes
        
        under_assigned = {
            node_id for node_id, count in assignment_counts.items()
            if count < self.repair_min_assignments
        }
        under_assigned.update(unassigned_nodes)
        
        print(f"  Found {len(under_assigned)} under-assigned nodes ({len(unassigned_nodes)} completely unassigned)")
        
        for node_id in under_assigned:
            try:
                node_vector = self.base_index._nodes[node_id].point
                
                # æ‰¾åˆ°æœ€è¿‘çš„è´¨å¿ƒå¹¶åˆ†é…
                distances = []
                for i, centroid_vector in enumerate(self.centroids):
                    distance = self.distance_func(node_vector, centroid_vector)
                    distances.append((distance, self.centroid_ids[i]))
                
                distances.sort()
                current_assignments = assignment_counts.get(node_id, 0)
                needed_assignments = max(0, self.repair_min_assignments - current_assignments)
                
                for _, centroid_id in distances[:needed_assignments]:
                    if node_id not in self.parent_child_map[centroid_id]:
                        self.parent_child_map[centroid_id].append(node_id)
                        self.child_vectors[node_id] = node_vector
                        assignment_counts[node_id] = assignment_counts.get(node_id, 0) + 1
            except Exception as e:
                print(f"    âš ï¸ Failed to repair node {node_id}: {e}")
                continue
        
        final_assigned = set(assignment_counts.keys())
        coverage = len(final_assigned) / len(all_base_nodes) if all_base_nodes else 0.0
        self.stats['coverage_fraction'] = coverage
        print(f"  Multi-Pivot repair completed. Final coverage: {coverage:.3f} ({len(final_assigned)}/{len(all_base_nodes)} nodes)")
    
    def _build_centroid_index(self):
        """æ„å»ºè´¨å¿ƒç´¢å¼•ç”¨äºå¿«é€Ÿæœç´¢"""
        if self.centroids is None:
            raise ValueError("Centroids not computed yet")
        self._centroid_matrix = self.centroids.copy()
    
    def search(self, query_vector, k=10, n_probe=10):
        """ä¸¤é˜¶æ®µæœç´¢ï¼šè´¨å¿ƒæœç´¢ â†’ å­èŠ‚ç‚¹æœç´¢"""
        start_time = time.time()
        
        # Stage 1: æ‰¾åˆ°æœ€è¿‘çš„è´¨å¿ƒ
        closest_centroids = self._stage1_centroid_search(query_vector, n_probe)
        
        # Stage 2: åœ¨é€‰å®šè´¨å¿ƒçš„å­èŠ‚ç‚¹ä¸­æœç´¢
        results = self._stage2_child_search(query_vector, closest_centroids, k)
        
        # è®°å½•æœç´¢æ—¶é—´
        search_time = (time.time() - start_time) * 1000
        self.search_times.append(search_time)
        
        return results
    
    def _stage1_centroid_search(self, query_vector, n_probe):
        """Stage 1: æ‰¾åˆ°æœ€è¿‘çš„K-Meansè´¨å¿ƒ"""
        distances = np.linalg.norm(self._centroid_matrix - query_vector, axis=1)
        closest_indices = np.argsort(distances)[:n_probe]
        return [(self.centroid_ids[i], distances[i]) for i in closest_indices]
    
    def _stage2_child_search(self, query_vector, closest_centroids, k):
        """Stage 2: åœ¨å­èŠ‚ç‚¹ä¸­æœç´¢"""
        # æ”¶é›†å€™é€‰å­èŠ‚ç‚¹
        candidate_children = set()
        for centroid_id, _ in closest_centroids:
            children = self.parent_child_map.get(centroid_id, [])
            candidate_children.update(children)
        
        if not candidate_children:
            return []
        
        # è®¡ç®—è·ç¦»å¹¶æ’åº
        candidate_scores = []
        for child_id in candidate_children:
            child_vector = self.child_vectors.get(child_id)
            if child_vector is not None:
                distance = self.distance_func(query_vector, child_vector)
                candidate_scores.append((distance, child_id))
        
        # æ’åºå¹¶è¿”å›æ ¼å¼ä¸º (child_id, distance) çš„ç»“æœ
        candidate_scores.sort()
        return [(child_id, distance) for distance, child_id in candidate_scores[:k]]
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        if self.search_times:
            stats['avg_search_time_ms'] = float(np.mean(self.search_times))
            stats['std_search_time_ms'] = float(np.std(self.search_times))
        return stats


class KMeansHNSWEvaluator:
    """
    K-Means HNSWç³»ç»Ÿæ€§èƒ½å…¨é¢è¯„ä¼°å™¨ (Comprehensive evaluator for K-Means HNSW system performance)
    
    æ­¤ç±»æä¾›äº†å¯¹K-Means HNSWç³»ç»Ÿçš„å…¨é¢è¯„ä¼°åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - çœŸå®å€¼(Ground Truth)è®¡ç®—
    - å¬å›ç‡è¯„ä¼°
    - å‚æ•°æ‰«æå’Œä¼˜åŒ–
    - ä¸åŸºçº¿æ–¹æ³•çš„æ€§èƒ½å¯¹æ¯”
    """
    
    def __init__(
        self, 
        dataset: np.ndarray, 
        query_set: np.ndarray, 
        query_ids: List[int],
        distance_func: callable
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨ (Initialize the evaluator)
        
        Args:
            dataset: å®Œæ•´æ•°æ®é›†å‘é‡ (Full dataset vectors) - shape: [n_vectors, dim]
            query_set: æŸ¥è¯¢å‘é‡ (Query vectors) - shape: [n_queries, dim]  
            query_ids: æŸ¥è¯¢å‘é‡IDåˆ—è¡¨ (IDs for query vectors)
            distance_func: ç”¨äºçœŸå®å€¼è®¡ç®—çš„è·ç¦»å‡½æ•° (Distance function for ground truth computation)
        """
        self.dataset = dataset
        self.query_set = query_set
        self.query_ids = query_ids
        self.distance_func = distance_func
        
        # çœŸå®å€¼ç¼“å­˜ (Ground truth cache)
        self._ground_truth_cache = {}
    
    def compute_ground_truth(
        self, 
        k: int, 
        exclude_query_ids: bool = True
    ) -> Dict[int, List[Tuple[int, float]]]:
        """
        ä½¿ç”¨æš´åŠ›æœç´¢è®¡ç®—çœŸå®å€¼ (Compute ground truth using brute force search)
        
        é€šè¿‡å¯¹æ¯ä¸ªæŸ¥è¯¢å‘é‡è®¡ç®—ä¸æ‰€æœ‰æ•°æ®å‘é‡çš„è·ç¦»ï¼Œæ‰¾å‡ºçœŸæ­£çš„kä¸ªæœ€è¿‘é‚»ã€‚
        è¿™æ˜¯è¯„ä¼°å…¶ä»–ç®—æ³•å¬å›ç‡çš„æ ‡å‡†åŸºå‡†ã€‚
        
        æ³¨æ„ï¼šåœ¨å½“å‰å®ç°ä¸­ï¼ŒæŸ¥è¯¢å‘é‡å’Œæ•°æ®é›†å‘é‡æ˜¯ç‹¬ç«‹ç”Ÿæˆçš„ï¼Œå› æ­¤
        exclude_query_ids å‚æ•°é€šå¸¸åº”è®¾ä¸º Falseï¼Œé™¤éæŸ¥è¯¢å‘é‡æ˜¯ä»æ•°æ®é›†ä¸­é‡‡æ ·çš„ã€‚
        
        Args:
            k: æœ€è¿‘é‚»æ•°é‡ (Number of nearest neighbors)
            exclude_query_ids: æ˜¯å¦ä»ç»“æœä¸­æ’é™¤æŸ¥è¯¢ID (Whether to exclude query IDs from results)
                              ä»…å½“æŸ¥è¯¢å‘é‡æ˜¯æ•°æ®é›†å­é›†æ—¶æ‰æœ‰æ„ä¹‰ (Only meaningful when queries are subset of dataset)
            
        Returns:
            å­—å…¸ï¼šæŸ¥è¯¢ID -> (è·ç¦», æ•°æ®ç´¢å¼•)å…ƒç»„åˆ—è¡¨ (Dictionary mapping query_id to list of (distance, data_index) tuples)
        """
        cache_key = (k, exclude_query_ids)
        if cache_key in self._ground_truth_cache:
            return self._ground_truth_cache[cache_key]
        
        print(f"æ­£åœ¨è®¡ç®— {len(self.query_set)} ä¸ªæŸ¥è¯¢çš„çœŸå®å€¼ (k={k}, exclude_query_ids={exclude_query_ids})...")
        print(f"Computing ground truth for {len(self.query_set)} queries against {len(self.dataset)} data points")
        start_time = time.time()
        
        ground_truth = {}
        excluded_count = 0
        
        for i, (query_vector, query_id) in enumerate(zip(self.query_set, self.query_ids)):
            distances = []
            
            for j, data_vector in enumerate(self.dataset):
                if exclude_query_ids and j == query_id:
                    excluded_count += 1
                    continue  # è·³è¿‡æŸ¥è¯¢æœ¬èº« (Skip the query itself)
                
                distance = self.distance_func(query_vector, data_vector)
                distances.append((distance, j))
            
            # æŒ‰è·ç¦»æ’åºå¹¶å–å‰kä¸ª (Sort by distance and take top-k)
            distances.sort()
            ground_truth[query_id] = distances[:k]
            
            if (i + 1) % 10 == 0:
                print(f"  å·²å¤„ç† {i + 1}/{len(self.query_set)} ä¸ªæŸ¥è¯¢ (Processed {i + 1}/{len(self.query_set)} queries)")
        
        elapsed = time.time() - start_time
        if exclude_query_ids and excluded_count == 0:
            print(f"âš ï¸  è­¦å‘Šï¼šexclude_query_ids=Trueä½†æ²¡æœ‰æ’é™¤ä»»ä½•æ•°æ®ç‚¹ã€‚æŸ¥è¯¢å‘é‡å¯èƒ½ä¸åœ¨æ•°æ®é›†ä¸­ã€‚")
            print(f"   Warning: exclude_query_ids=True but no data points were excluded. Query vectors may not be in dataset.")
        
        print(f"çœŸå®å€¼è®¡ç®—å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}ç§’ï¼Œæ’é™¤äº† {excluded_count} ä¸ªæ•°æ®ç‚¹")
        print(f"Ground truth computed in {elapsed:.2f}s, excluded {excluded_count} data points")
        
        self._ground_truth_cache[cache_key] = ground_truth
        return ground_truth
    
    def evaluate_recall(
        self,
        kmeans_hnsw: KMeansHNSW,
        k: int,
        n_probe: int,
        ground_truth: Optional[Dict] = None,
        exclude_query_ids: bool = True
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°K-Means HNSWç³»ç»Ÿçš„å¬å›ç‡æ€§èƒ½ (Evaluate recall performance of the K-Means HNSW system)
        
        è®¡ç®—ç³»ç»Ÿåœ¨ç»™å®šå‚æ•°ä¸‹çš„å¬å›ç‡ã€æŸ¥è¯¢æ—¶é—´ç­‰æ€§èƒ½æŒ‡æ ‡ã€‚
        å¬å›ç‡ = æ‰¾åˆ°çš„çœŸå®é‚»å±…æ•° / åº”è¯¥æ‰¾åˆ°çš„é‚»å±…æ•°
        
        Args:
            kmeans_hnsw: è¦è¯„ä¼°çš„K-Means HNSWç³»ç»Ÿ (The K-Means HNSW system to evaluate)
            k: è¿”å›ç»“æœæ•°é‡ (Number of results to return)
            n_probe: æ¢æµ‹çš„èšç±»ä¸­å¿ƒæ•°é‡ (Number of centroids to probe)
            ground_truth: é¢„è®¡ç®—çš„çœŸå®å€¼(å¯é€‰) (Precomputed ground truth, optional)
            exclude_query_ids: æ˜¯å¦ä»è¯„ä¼°ä¸­æ’é™¤æŸ¥è¯¢ID (Whether to exclude query IDs from evaluation)
            
        Returns:
            åŒ…å«å¬å›ç‡æŒ‡æ ‡å’Œæ€§èƒ½æ•°æ®çš„å­—å…¸ (Dictionary containing recall metrics and performance data)
        """
        if ground_truth is None:
            ground_truth = self.compute_ground_truth(k, exclude_query_ids)
        
        print(f"æ­£åœ¨è¯„ä¼° {len(self.query_set)} ä¸ªæŸ¥è¯¢çš„å¬å›ç‡ (k={k}, n_probe={n_probe})... (Evaluating recall)")
        start_time = time.time()
        
        total_correct = 0
        total_expected = len(self.query_set) * k
        query_times = []
        individual_recalls = []
        
        for i, (query_vector, query_id) in enumerate(zip(self.query_set, self.query_ids)):
            # Get ground truth for this query
            true_neighbors = {neighbor_id for _, neighbor_id in ground_truth[query_id]}
            
            # Search using K-Means HNSW
            search_start = time.time()
            results = kmeans_hnsw.search(query_vector, k=k, n_probe=n_probe)
            search_time = time.time() - search_start
            query_times.append(search_time)
            
            # Count correct results
            found_neighbors = {node_id for node_id, _ in results}
            correct = len(true_neighbors & found_neighbors)
            total_correct += correct
            
            # Individual recall for this query
            individual_recall = correct / k if k > 0 else 0.0
            individual_recalls.append(individual_recall)
            
            if (i + 1) % 20 == 0:
                current_recall = total_correct / ((i + 1) * k)
                print(f"  Processed {i + 1}/{len(self.query_set)} queries, "
                      f"current recall: {current_recall:.4f}")
        
        # Calculate final metrics
        overall_recall = total_correct / total_expected
        avg_query_time = np.mean(query_times)
        std_query_time = np.std(query_times)
        total_evaluation_time = time.time() - start_time
        
        return {
            'recall_at_k': overall_recall,
            'total_correct': total_correct,
            'total_expected': total_expected,
            'individual_recalls': individual_recalls,
            'avg_individual_recall': np.mean(individual_recalls),
            'std_individual_recall': np.std(individual_recalls),
            'avg_query_time_ms': avg_query_time * 1000,
            'std_query_time_ms': std_query_time * 1000,
            'total_evaluation_time': total_evaluation_time,
            'k': k,
            'n_probe': n_probe,
            'system_stats': kmeans_hnsw.get_stats()
        }
    
    def evaluate_multi_pivot_recall(
        self,
        multi_pivot_hnsw: KMeansHNSWMultiPivot,
        k: int,
        n_probe: int,
        ground_truth: Optional[Dict] = None,
        exclude_query_ids: bool = True
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°Multi-Pivot K-Means HNSWç³»ç»Ÿçš„å¬å›ç‡æ€§èƒ½
        (Evaluate recall performance of the Multi-Pivot K-Means HNSW system)
        """
        if ground_truth is None:
            ground_truth = self.compute_ground_truth(k, exclude_query_ids)
        
        print(f"æ­£åœ¨è¯„ä¼°Multi-Pivot {len(self.query_set)} ä¸ªæŸ¥è¯¢çš„å¬å›ç‡ (k={k}, n_probe={n_probe})...")
        start_time = time.time()
        
        total_correct = 0
        total_expected = len(self.query_set) * k
        query_times = []
        individual_recalls = []
        
        for i, (query_vector, query_id) in enumerate(zip(self.query_set, self.query_ids)):
            # Get ground truth for this query
            true_neighbors = {neighbor_id for _, neighbor_id in ground_truth[query_id]}
            
            # Search using Multi-Pivot K-Means HNSW
            search_start = time.time()
            results = multi_pivot_hnsw.search(query_vector, k=k, n_probe=n_probe)
            search_time = time.time() - search_start
            query_times.append(search_time)
            
            # Count correct results
            found_neighbors = {node_id for node_id, _ in results}
            correct = len(true_neighbors & found_neighbors)
            total_correct += correct
            
            # Individual recall for this query
            individual_recall = correct / k if k > 0 else 0.0
            individual_recalls.append(individual_recall)
            
            if (i + 1) % 20 == 0:
                current_recall = total_correct / ((i + 1) * k)
                print(f"  Processed {i + 1}/{len(self.query_set)} queries, "
                      f"current recall: {current_recall:.4f}")
        
        # Calculate final metrics
        overall_recall = total_correct / total_expected
        avg_query_time = np.mean(query_times)
        std_query_time = np.std(query_times)
        total_evaluation_time = time.time() - start_time
        
        return {
            'recall_at_k': overall_recall,
            'total_correct': total_correct,
            'total_expected': total_expected,
            'individual_recalls': individual_recalls,
            'avg_individual_recall': np.mean(individual_recalls),
            'std_individual_recall': np.std(individual_recalls),
            'avg_query_time_ms': avg_query_time * 1000,
            'std_query_time_ms': std_query_time * 1000,
            'total_evaluation_time': total_evaluation_time,
            'k': k,
            'n_probe': n_probe,
            'system_stats': multi_pivot_hnsw.get_stats()
        }

    # -------------------- Phase-Specific Evaluations --------------------
    def evaluate_hnsw_baseline(
        self,
        base_index: HNSW,
        k: int,
        ef: int,
        ground_truth: Dict
    ) -> Dict[str, Any]:
        """Evaluate recall using ONLY the base HNSW index (Phase 1)."""
        query_times = []
        total_correct = 0
        total_expected = len(self.query_set) * k
        individual_recalls = []
        
        print(f"ğŸ” è¯„ä¼°HNSWåŸºçº¿æ€§èƒ½ (k={k}, ef={ef})...")
        
        for query_vector, query_id in zip(self.query_set, self.query_ids):
            # Ground truth format: {query_id: [(distance, node_id), ...]}
            # Extract the node_ids (which are data indices) from ground truth
            true_neighbors = {node_id for _, node_id in ground_truth[query_id]}
            
            t0 = time.time()
            results = base_index.query(query_vector, k=k, ef=ef)
            dt = time.time() - t0
            query_times.append(dt)
            
            # HNSW query returns [(node_id, distance), ...]
            found = {nid for nid, _ in results}
            correct = len(true_neighbors & found)
            total_correct += correct
            
            # Individual recall for this query
            individual_recall = correct / k if k > 0 else 0.0
            individual_recalls.append(individual_recall)
            
            # Debug info for first few queries
            if query_id < 3:
                print(f"  Query {query_id}: found {len(found)} results, {correct}/{k} correct, recall={individual_recall:.4f}")
                print(f"    True neighbors (first 5): {list(true_neighbors)[:5]}")
                print(f"    Found neighbors (first 5): {list(found)[:5]}")
        
        avg_recall = np.mean(individual_recalls)
        print(f"  HNSWåŸºçº¿å¬å›ç‡: {avg_recall:.4f} (æ€»è®¡ {total_correct}/{total_expected})")
        
        return {
            'phase': 'baseline_hnsw',
            'ef': ef,
            'recall_at_k': total_correct / total_expected if total_expected else 0.0,
            'avg_query_time_ms': float(np.mean(query_times) * 1000),
            'std_query_time_ms': float(np.std(query_times) * 1000),
            'total_correct': total_correct,
            'total_expected': total_expected,
            'individual_recalls': individual_recalls,
            'avg_individual_recall': float(np.mean(individual_recalls)),
            'std_individual_recall': float(np.std(individual_recalls))
        }

    def evaluate_hybrid_hnsw(
        self,
        hybrid_index: 'HNSWHybrid',
        k: int,
        n_probe: int,
        ground_truth: Dict
    ) -> Dict[str, Any]:
        """Evaluate recall for level-based Hybrid HNSW (parents from HNSW levels)."""
        query_times = []
        total_correct = 0
        total_expected = len(self.query_set) * k
        individual_recalls = []

        for query_vector, query_id in zip(self.query_set, self.query_ids):
            true_neighbors = {node_id for _, node_id in ground_truth[query_id]}
            t0 = time.time()
            results = hybrid_index.search(query_vector, k=k, n_probe=n_probe)
            dt = time.time() - t0
            query_times.append(dt)
            found = {nid for nid, _ in results}
            correct = len(true_neighbors & found)
            total_correct += correct
            individual_recalls.append(correct / k if k > 0 else 0.0)

        return {
            'phase': 'hybrid_hnsw_level',
            'n_probe': n_probe,
            'recall_at_k': total_correct / total_expected if total_expected else 0.0,
            'avg_query_time_ms': float(np.mean(query_times) * 1000),
            'std_query_time_ms': float(np.std(query_times) * 1000),
            'total_correct': total_correct,
            'total_expected': total_expected,
            'individual_recalls': individual_recalls,
            'avg_individual_recall': float(np.mean(individual_recalls)),
            'std_individual_recall': float(np.std(individual_recalls)),
            'hybrid_stats': hybrid_index.get_stats()
        }


    def parameter_sweep(
        self,
        base_index: HNSW,
        param_grid: Dict[str, List[Any]],
        evaluation_params: Dict[str, Any],
        max_combinations: Optional[int] = None,
        adaptive_config: Optional[Dict[str, Any]] = None,
        multi_pivot_config: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œå…¨é¢çš„å‚æ•°æ‰«æä¼˜åŒ– (å«Multi-Pivotæ‰©å±•)
        (Perform comprehensive parameter sweep for optimization with Multi-Pivot extension)

        é€šè¿‡ç³»ç»Ÿæ€§åœ°æµ‹è¯•ä¸åŒå‚æ•°ç»„åˆï¼Œæ‰¾åˆ°æœ€ä¼˜çš„K-Means HNSWé…ç½®ã€‚
        åŒ…æ‹¬åŸºçº¿HNSWã€çº¯K-Meansã€Hybrid HNSWã€å•æ¢çº½KMeans-HNSWä»¥åŠMulti-Pivot KMeans-HNSWçš„å¯¹æ¯”è¯„ä¼°ã€‚
        """
        if adaptive_config is None:
            adaptive_config = {
                'adaptive_k_children': False,
                'k_children_scale': 1.5,
                'k_children_min': 100,
                'k_children_max': None,
                'diversify_max_assignments': None,
                'repair_min_assignments': 1
            }

        if multi_pivot_config is None:
            multi_pivot_config = {
                'enabled': False,
                'num_pivots': 3,
                'pivot_selection_strategy': 'line_perp_third',
                'pivot_overquery_factor': 1.2
            }

        print("ğŸ”¬================== äº”æ–¹æ³•å¯¹æ¯”è¯„ä¼°ç³»ç»Ÿ ==================")
        print("ğŸ“Š è¯„ä¼°æµç¨‹: HNSW â†’ K-Means â†’ Hybrid HNSW â†’ KMeans HNSW â†’ Multi-Pivot KMeans HNSW")
        print(f"ğŸ¯ Multi-Pivotå¯ç”¨çŠ¶æ€: {multi_pivot_config.get('enabled', False)}")
        print("================================================================")

        # ========== æ­¥éª¤1: å‡†å¤‡å‚æ•°ç»„åˆ ==========
        param_names = list(param_grid.keys())
        param_values = list(param_grid.values())
        combinations = list(product(*param_values))
        if max_combinations and len(combinations) > max_combinations:
            print(f"é™åˆ¶æµ‹è¯• {max_combinations} ä¸ªç»„åˆï¼Œæ€»å…± {len(combinations)} ä¸ª")
            combinations = random.sample(combinations, max_combinations)
        print(f"ğŸ“‹ å°†æµ‹è¯• {len(combinations)} ä¸ªå‚æ•°ç»„åˆ")

        results: List[Dict[str, Any]] = []
        k_values = evaluation_params.get('k_values', [10])
        n_probe_values = evaluation_params.get('n_probe_values', [5, 10, 20])
        hybrid_parent_level = evaluation_params.get('hybrid_parent_level', 2)
        enable_hybrid = evaluation_params.get('enable_hybrid', True)

        # ========== æ­¥éª¤2: é¢„è®¡ç®—çœŸå®å€¼ (Ground Truth) ==========
        print(f"\nğŸ¯ æ­¥éª¤2: é¢„è®¡ç®—çœŸå®å€¼ (k_values: {k_values})")
        ground_truths: Dict[int, Dict] = {}
        for k in k_values:
            ground_truths[k] = self.compute_ground_truth(k, exclude_query_ids=False)
        
        # ========== æ­¥éª¤3: é¢„è®­ç»ƒå…±äº«K-Meansæ¨¡å‹ ==========
        print(f"\nğŸ¤– æ­¥éª¤3: é¢„è®­ç»ƒå…±äº«K-Meansæ¨¡å‹ä»¥é¿å…é‡å¤è®¡ç®—")
        shared_dataset_vectors = []
        for node_id, node in base_index._nodes.items():
            if node.point is not None:
                shared_dataset_vectors.append(node.point)
        shared_dataset_vectors = np.array(shared_dataset_vectors)
        print(f"   æå–äº† {len(shared_dataset_vectors)} ä¸ªæ•°æ®å‘é‡")
        
        # ä¸ºæ¯ä¸ªn_clusterså€¼é¢„è®­ç»ƒK-Meansæ¨¡å‹
        shared_kmeans_models: Dict[int, MiniBatchKMeans] = {}
        unique_n_clusters = set(params[param_names.index('n_clusters')] for params in combinations)
        
        for n_clusters in unique_n_clusters:
            print(f"   é¢„è®­ç»ƒK-Meansæ¨¡å‹ (n_clusters={n_clusters})...")
            actual_clusters = min(n_clusters, len(shared_dataset_vectors))
            kmeans_model = MiniBatchKMeans(
                n_clusters=actual_clusters,
                random_state=42,
                max_iter=100,
                batch_size=min(100, len(shared_dataset_vectors))
            )
            kmeans_model.fit(shared_dataset_vectors)
            shared_kmeans_models[n_clusters] = kmeans_model
            print(f"     âœ… å®Œæˆ: {actual_clusters} clusters, inertia={kmeans_model.inertia_:.2f}")
        
        print(f"âœ… å…±äº«K-Meansæ¨¡å‹é¢„è®­ç»ƒå®Œæˆ ({len(shared_kmeans_models)} ä¸ªæ¨¡å‹)")
        print(f"ğŸ’¡ K-Meansæ¨¡å‹å°†è¢«æ‰€æœ‰æ–¹æ³•é‡ç”¨ï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”")

        # ========== æ­¥éª¤4: å¼€å§‹å‚æ•°ç»„åˆè¯„ä¼° ==========
        for i, combination in enumerate(combinations):
            print(f"\nğŸ”¬ =========== å‚æ•°ç»„åˆ {i + 1}/{len(combinations)} ===========")
            params = dict(zip(param_names, combination))
            print(f"ğŸ“ å½“å‰å‚æ•°: {params}")

            try:
                phase_records: List[Dict[str, Any]] = []
                
                # è·å–å½“å‰ç»„åˆçš„å…±äº«K-Meansæ¨¡å‹
                current_n_clusters = params['n_clusters']
                shared_model = shared_kmeans_models[current_n_clusters]
                print(f"ğŸ¤– ä½¿ç”¨é¢„è®­ç»ƒçš„K-Meansæ¨¡å‹ (n_clusters={current_n_clusters})")
                
                # ========== æ–¹æ³•1: HNSWåŸºçº¿ ==========
                print(f"\nğŸ“Š æ–¹æ³•1: HNSWåŸºçº¿è¯„ä¼°")
                base_ef = base_index._ef_construction
                print(f"   å‚æ•°: ef={base_ef}")
                for k in k_values:
                    b_eval = self.evaluate_hnsw_baseline(base_index, k, base_ef, ground_truths[k])
                    phase_records.append({**b_eval, 'k': k})
                    print(f"   âœ… k={k}: recall={b_eval['recall_at_k']:.4f}, æ—¶é—´={b_eval['avg_query_time_ms']:.2f}ms")

                # ========== æ–¹æ³•2: çº¯K-Meansèšç±» ==========
                print(f"\nğŸ“Š æ–¹æ³•2: çº¯K-Meansèšç±»è¯„ä¼°")
                print(f"   å‚æ•°: n_clusters={current_n_clusters}, n_probe={n_probe_values}")
                for k in k_values:
                    for n_probe in n_probe_values:
                        c_eval = self._evaluate_pure_kmeans_from_existing_shared(
                            shared_model, shared_dataset_vectors, base_index,
                            k, ground_truths[k], n_probe=n_probe
                        )
                        c_eval['phase'] = 'clusters_only'
                        phase_records.append({**c_eval, 'k': k})
                        print(f"   âœ… k={k} n_probe={n_probe}: recall={c_eval['recall_at_k']:.4f}, æ—¶é—´={c_eval['avg_query_time_ms']:.2f}ms")

                # ========== æ–¹æ³•3: Hybrid HNSW ==========
                if enable_hybrid:
                    print(f"\nğŸ“Š æ–¹æ³•3: Hybrid HNSWè¯„ä¼°")
                    print(f"   å‚æ•°: parent_level={hybrid_parent_level}, k_children={params['k_children']}")
                    try:
                        hybrid_build_start = time.time()
                        hybrid_index = HNSWHybrid(
                            base_index=base_index,
                            parent_level=hybrid_parent_level,
                            k_children=params['k_children'],
                            approx_ef=params.get('child_search_ef'),
                            parent_child_method='approx',
                            diversify_max_assignments=adaptive_config.get('diversify_max_assignments'),
                            repair_min_assignments=adaptive_config.get('repair_min_assignments'),
                            adaptive_k_children=adaptive_config.get('adaptive_k_children', False),
                            k_children_scale=adaptive_config.get('k_children_scale', 1.5),
                            k_children_min=adaptive_config.get('k_children_min', 100),
                            k_children_max=adaptive_config.get('k_children_max')
                        )
                        hybrid_build_time = time.time() - hybrid_build_start
                        hybrid_stats = hybrid_index.get_stats()
                        
                        print(f"   æ„å»ºå®Œæˆ: {hybrid_stats.get('num_parents', 0)} parents, "
                              f"{hybrid_stats.get('num_children', 0)} children, "
                              f"coverage: {hybrid_stats.get('coverage_fraction', 0):.4f}")
                        
                        for k in k_values:
                            for n_probe in n_probe_values:
                                h_eval = self.evaluate_hybrid_hnsw(hybrid_index, k, n_probe, ground_truths[k])
                                h_eval['hybrid_build_time'] = hybrid_build_time
                                h_eval['hybrid_k_children'] = hybrid_stats.get('k_children', params['k_children'])
                                phase_records.append({**h_eval, 'k': k})
                                print(f"   âœ… k={k} n_probe={n_probe}: recall={h_eval['recall_at_k']:.4f}, æ—¶é—´={h_eval['avg_query_time_ms']:.2f}ms")
                    except Exception as he:
                        print(f"   âŒ Hybrid HNSW è¯„ä¼°å¤±è´¥: {he}")

                # ========== æ–¹æ³•4: KMeans HNSW (å•æ¢çº½) ==========
                print(f"\nğŸ“Š æ–¹æ³•4: KMeans HNSW (å•æ¢çº½)è¯„ä¼°")
                print(f"   å‚æ•°: n_clusters={current_n_clusters}, k_children={params['k_children']}")
                
                construction_start = time.time()
                kmeans_hnsw = KMeansHNSW(
                    base_index=base_index,
                    **params,
                    adaptive_k_children=adaptive_config['adaptive_k_children'],
                    k_children_scale=adaptive_config['k_children_scale'],
                    k_children_min=adaptive_config['k_children_min'],
                    k_children_max=adaptive_config['k_children_max'],
                    diversify_max_assignments=adaptive_config['diversify_max_assignments'],
                    repair_min_assignments=adaptive_config['repair_min_assignments'],
                    shared_kmeans_model=shared_model,
                    shared_dataset_vectors=shared_dataset_vectors
                )
                construction_time = time.time() - construction_start
                print(f"   æ„å»ºå®Œæˆ (è€—æ—¶: {construction_time:.2f}ç§’)")
                actual_n_clusters = kmeans_hnsw.n_clusters

                for k in k_values:
                    for n_probe in n_probe_values:
                        eval_result = self.evaluate_recall(kmeans_hnsw, k, n_probe, ground_truths[k])
                        phase_records.append({**eval_result, 'phase': 'kmeans_hnsw_single_pivot'})
                        print(f"   âœ… k={k} n_probe={n_probe}: recall={eval_result['recall_at_k']:.4f}, æ—¶é—´={eval_result['avg_query_time_ms']:.2f}ms")




                # ========== æ–¹æ³•5: Multi-Pivot KMeans HNSW ==========
                if multi_pivot_config.get('enabled', False):
                    print(f"\nğŸ“Š æ–¹æ³•5: Multi-Pivot KMeans HNSWè¯„ä¼°")
                    print(f"   å‚æ•°: pivots={multi_pivot_config.get('num_pivots', 3)}, "
                          f"strategy={multi_pivot_config.get('pivot_selection_strategy', 'line_perp_third')}")
                    try:
                        multi_pivot_start = time.time()
                        multi_pivot_hnsw = KMeansHNSWMultiPivot(
                            base_index=base_index,
                            **params,
                            num_pivots=multi_pivot_config.get('num_pivots', 3),
                            pivot_selection_strategy=multi_pivot_config.get('pivot_selection_strategy', 'line_perp_third'),
                            pivot_overquery_factor=multi_pivot_config.get('pivot_overquery_factor', 1.2),
                            adaptive_k_children=adaptive_config['adaptive_k_children'],
                            k_children_scale=adaptive_config['k_children_scale'],
                            k_children_min=adaptive_config['k_children_min'],
                            k_children_max=adaptive_config['k_children_max'],
                            diversify_max_assignments=adaptive_config['diversify_max_assignments'],
                            repair_min_assignments=adaptive_config['repair_min_assignments'],
                            shared_kmeans_model=shared_model,
                            shared_dataset_vectors=shared_dataset_vectors
                        )
                        multi_pivot_build_time = time.time() - multi_pivot_start
                        print(f"   æ„å»ºå®Œæˆ (è€—æ—¶: {multi_pivot_build_time:.2f}ç§’)")
                        
                        for k in k_values:
                            for n_probe in n_probe_values:
                                mp_eval_result = self.evaluate_multi_pivot_recall(multi_pivot_hnsw, k, n_probe, ground_truths[k])
                                phase_records.append({**mp_eval_result, 'phase': 'kmeans_hnsw_multi_pivot', 'multi_pivot_build_time': multi_pivot_build_time})
                                print(f"   âœ… k={k} n_probe={n_probe}: recall={mp_eval_result['recall_at_k']:.4f}, æ—¶é—´={mp_eval_result['avg_query_time_ms']:.2f}ms")
                    
                    except Exception as mp_e:
                        print(f"   âŒ Multi-Pivot KMeans HNSW è¯„ä¼°å¤±è´¥: {mp_e}")
                        traceback.print_exc()

                # ========== ç»„åˆæ€»ç»“ ==========
                print(f"\nğŸ“ˆ å‚æ•°ç»„åˆ {i + 1} è¯„ä¼°å®Œæˆ!")
                best_recall = max(r['recall_at_k'] for r in phase_records if 'recall_at_k' in r)
                methods_tested = len(set(r.get('phase', r.get('method', 'unknown')) for r in phase_records))
                print(f"   æµ‹è¯•äº† {methods_tested} ç§æ–¹æ³•ï¼Œæœ€ä½³å¬å›ç‡: {best_recall:.4f}")
                
                combination_results = {
                    'parameters': params,
                    'construction_time': construction_time if 'construction_time' in locals() else 0.0,
                    'phase_evaluations': phase_records,
                    'multi_pivot_enabled': multi_pivot_config.get('enabled', False),
                    'best_recall': best_recall,
                    'methods_count': methods_tested
                }
                results.append(combination_results)
                
            except Exception as e:
                print(f"âŒ å‚æ•°ç»„åˆ {params} è¯„ä¼°å‡ºé”™: {e}")
                traceback.print_exc()
                continue

        # ========== æœ€ç»ˆæ€»ç»“ ==========
        print(f"\nï¿½ ================== äº”æ–¹æ³•å¯¹æ¯”è¯„ä¼°å®Œæˆ ==================")
        print(f"ğŸ“Š æ€»è®¡æµ‹è¯•: {len(results)} ä¸ªå‚æ•°ç»„åˆ")
        print(f"ğŸ¯ Multi-Pivotå¯ç”¨: {multi_pivot_config.get('enabled', False)}")
        
        if results:
            overall_best = max(results, key=lambda x: x.get('best_recall', 0))
            print(f"ğŸ¥‡ å…¨å±€æœ€ä½³å¬å›ç‡: {overall_best.get('best_recall', 0):.4f}")
            print(f"ğŸ”§ æœ€ä½³å‚æ•°ç»„åˆ: {overall_best.get('parameters', {})}")
        
        print(f"================================================================")
        return results
    

    
    def _evaluate_pure_kmeans_from_existing_shared(
        self, 
        kmeans_model: MiniBatchKMeans, 
        dataset_vectors: np.ndarray,
        base_index: HNSW,
        k: int, 
        ground_truth: Dict, 
        n_probe: int = 1
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨å…±äº«K-Meansæ¨¡å‹ç›´æ¥è¯„ä¼°çº¯K-Meansæ€§èƒ½
        (Evaluate pure K-Means using shared model directly)
        """
        print(f"    ä½¿ç”¨å…±äº«K-Meansæ¨¡å‹è¿›è¡Œè¯„ä¼° (n_clusters={kmeans_model.n_clusters}, n_probe={n_probe})")
        
        # è·å–èšç±»ä¸­å¿ƒå’Œæ•°æ®æ ‡ç­¾
        centers = kmeans_model.cluster_centers_
        n_clusters = centers.shape[0]
        labels = kmeans_model.predict(dataset_vectors)
        
        # æ„å»ºèšç±»åˆ°æˆå‘˜çš„æ˜ å°„
        clusters = [[] for _ in range(n_clusters)]
        dataset_idx_to_original_id = list(base_index.keys())
        
        for dataset_idx, cluster_id in enumerate(labels):
            original_id = dataset_idx_to_original_id[dataset_idx]
            clusters[cluster_id].append((dataset_idx, original_id))
        
        query_times = []
        total_correct = 0
        total_expected = len(self.query_set) * k
        individual_recalls = []
        
        # Cap n_probe to number of clusters
        n_probe_eff = min(n_probe, n_clusters)
        
        for query_vector, query_id in zip(self.query_set, self.query_ids):
            search_start = time.time()
            
            # è®¡ç®—åˆ°èšç±»ä¸­å¿ƒçš„è·ç¦»
            diffs = centers - query_vector
            distances_to_centroids = np.linalg.norm(diffs, axis=1)
            
            # è·å–æœ€è¿‘çš„n_probeä¸ªèšç±»ä¸­å¿ƒ
            probe_centroids = np.argpartition(distances_to_centroids, n_probe_eff - 1)[:n_probe_eff]
            probe_centroids = probe_centroids[np.argsort(distances_to_centroids[probe_centroids])]
            
            # æ”¶é›†æ‰€æœ‰è¢«æ¢æµ‹èšç±»çš„æˆå‘˜
            all_candidates = []
            for cluster_idx in probe_centroids:
                cluster_members = clusters[cluster_idx]
                for dataset_idx, original_id in cluster_members:
                    if original_id != query_id:  # æ’é™¤æŸ¥è¯¢æœ¬èº«
                        member_vec = dataset_vectors[dataset_idx]
                        dist = np.linalg.norm(member_vec - query_vector)
                        all_candidates.append((dist, original_id))
            
            # æŒ‰è·ç¦»æ’åºå¹¶å–top-k
            all_candidates.sort(key=lambda x: x[0])
            results = all_candidates[:k]
            found_neighbors = {original_id for _, original_id in results}
            
            search_time = time.time() - search_start
            query_times.append(search_time)
            
            # è®¡ç®—å¬å›ç‡
            true_neighbors = {neighbor_id for _, neighbor_id in ground_truth[query_id]}
            correct = len(true_neighbors & found_neighbors)
            total_correct += correct
            individual_recalls.append(correct / k if k > 0 else 0.0)
        
        overall_recall = total_correct / total_expected if total_expected > 0 else 0.0
        
        return {
            'method': 'pure_kmeans_shared_model',
            'recall_at_k': overall_recall,
            'total_correct': total_correct,
            'total_expected': total_expected,
            'individual_recalls': individual_recalls,
            'avg_individual_recall': float(np.mean(individual_recalls)),
            'std_individual_recall': float(np.std(individual_recalls)),
            'avg_query_time_ms': float(np.mean(query_times) * 1000),
            'std_query_time_ms': float(np.std(query_times) * 1000),
            'clustering_time': 0.0,  # ä½¿ç”¨å…±äº«æ¨¡å‹ï¼Œæ—¶é—´ä¸º0
            'n_clusters': n_clusters,
            'n_probe': n_probe_eff,
            'k': k,
            'used_shared_model': True
        }


def save_results(results: Dict[str, Any], filename: str):
    """Save evaluation results to JSON file."""
    # Convert numpy arrays to lists for JSON serialization
    def convert_numpy(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, dict):
            return {k: convert_numpy(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy(item) for item in obj]
        return obj
    
    results_serializable = convert_numpy(results)
    
    with open(filename, 'w') as f:
        json.dump(results_serializable, f, indent=2)
    
    print(f"Results saved to {filename}")


def load_sift_data():
    """
    åŠ è½½SIFTæ•°æ®é›†ç”¨äºè¯„ä¼° (Load SIFT dataset for evaluation)
    
    SIFT (Scale-Invariant Feature Transform) æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ç»å…¸ç‰¹å¾æè¿°ç¬¦ã€‚
    è¯¥æ•°æ®é›†åŒ…å«100ä¸‡ä¸ª128ç»´çš„ç‰¹å¾å‘é‡ï¼Œå¸¸ç”¨äºç›¸ä¼¼æ€§æœç´¢ç®—æ³•çš„åŸºå‡†æµ‹è¯•ã€‚
    
    Returns:
        tuple: (base_vectors, query_vectors) æˆ– (None, None) å¦‚æœåŠ è½½å¤±è´¥
    """
    sift_dir = os.path.join(os.path.dirname(__file__), '..', 'sift')
    
    try:
        def read_fvecs(path: str, max_vectors: Optional[int] = None) -> np.ndarray:
            """
            è¯»å–.fvecsæ–‡ä»¶ (FAISSæ ¼å¼)ã€‚æ¯ä¸ªå‘é‡å­˜å‚¨ä¸ºï¼šint32ç»´åº¦ + ç»´åº¦ä¸ªfloat32å€¼ã€‚
            æ­¤å®ç°é€šè¿‡å…ˆè¯»å–int32å¤´éƒ¨æ¥é¿å…è§£æé”™è¯¯ã€‚
            
            Read .fvecs file (FAISS format). Each vector stored as: int32 dim + dim float32.
            This implementation avoids mis-parsing by reading int32 header first.
            """
            if not os.path.exists(path):
                raise FileNotFoundError(path)
            raw = np.fromfile(path, dtype=np.int32)
            if raw.size == 0:
                raise ValueError(f"ç©ºçš„fvecsæ–‡ä»¶: {path} (Empty fvecs file)")
            dim = raw[0]
            if dim <= 0 or dim > 4096:
                raise ValueError(f"ä¸åˆç†çš„å‘é‡ç»´åº¦ {dim}ï¼Œè§£æè‡ª {path} (Unreasonable vector dimension)")
            record_size = dim + 1
            count = raw.size // record_size
            raw = raw.reshape(count, record_size)
            vecs = raw[:, 1:].astype(np.float32)
            if max_vectors is not None and count > max_vectors:
                vecs = vecs[:max_vectors]
            return vecs

        base_path = os.path.join(sift_dir, 'sift_base.fvecs')
        query_path = os.path.join(sift_dir, 'sift_query.fvecs')

        # ä¸ºè°ƒä¼˜æ¼”ç¤ºé™åˆ¶æ•°é‡ä»¥ä¿æŒåˆç†çš„è¿è¡Œæ—¶é—´ (Limit for tuning demo to keep runtime reasonable)
        base_vectors = read_fvecs(base_path, max_vectors=50000)
        query_vectors = read_fvecs(query_path, max_vectors=1000)

        print(f"å·²åŠ è½½SIFTæ•°æ®: {base_vectors.shape[0]} ä¸ªåŸºç¡€å‘é‡, "
              f"{query_vectors.shape[0]} ä¸ªæŸ¥è¯¢å‘é‡, ç»´åº¦ {base_vectors.shape[1]}")
        print(f"Loaded SIFT data: {base_vectors.shape[0]} base vectors, "
              f"{query_vectors.shape[0]} query vectors, dimension {base_vectors.shape[1]}")

        return base_vectors, query_vectors
    
    except Exception as e:
        print(f"åŠ è½½SIFTæ•°æ®æ—¶å‡ºé”™: {e} (Error loading SIFT data)")
        print("æ”¹ç”¨åˆæˆæ•°æ®... (Using synthetic data instead)")
        return None, None


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="K-Means HNSWå‚æ•°è°ƒä¼˜å’Œè¯„ä¼° (K-Means HNSW Parameter Tuning and Evaluation)")
    
    # æ•°æ®é›†é€‰é¡¹ (Dataset options)
    parser.add_argument('--dataset-size', type=int, default=10000, 
                        help='ä½¿ç”¨çš„åŸºç¡€å‘é‡æ•°é‡ (é»˜è®¤: 10000) (Number of base vectors to use)')
    parser.add_argument('--query-size', type=int, default=50, 
                        help='ä½¿ç”¨çš„æŸ¥è¯¢å‘é‡æ•°é‡ (é»˜è®¤: 50) (Number of query vectors to use)')
    parser.add_argument('--dimension', type=int, default=128, 
                        help='åˆæˆæ•°æ®çš„å‘é‡ç»´åº¦ (å¦‚æœæœªåŠ è½½SIFT) (Vector dimensionality for synthetic data)')
    parser.add_argument('--no-sift', action='store_true', 
                        help='å¼ºåˆ¶ä½¿ç”¨åˆæˆæ•°æ®ï¼Œå³ä½¿SIFTæ–‡ä»¶å­˜åœ¨ (Force synthetic data even if SIFT files exist)')
    
    # è‡ªé€‚åº”/å¤šæ ·åŒ–/ä¿®å¤é€‰é¡¹ (Adaptive/diversification/repair options)
    parser.add_argument('--adaptive-k-children', action='store_true', 
                        help='å¯ç”¨åŸºäºå¹³å‡èšç±»å¤§å°çš„è‡ªé€‚åº”k_children (Enable adaptive k_children based on avg cluster size)')
    parser.add_argument('--k-children-scale', type=float, default=1.5, 
                        help='è‡ªé€‚åº”k_childrençš„ç¼©æ”¾å› å­ (é»˜è®¤1.5) (Scale factor for adaptive k_children)')
    parser.add_argument('--k-children-min', type=int, default=100, 
                        help='è‡ªé€‚åº”æ—¶çš„æœ€å°k_children (Minimum k_children when adaptive)')
    parser.add_argument('--k-children-max', type=int, default=None, 
                        help='è‡ªé€‚åº”æ—¶çš„æœ€å¤§k_children (å¯é€‰) (Maximum k_children when adaptive)')
    parser.add_argument('--diversify-max-assignments', type=int, default=None, 
                        help='æ¯ä¸ªå­èŠ‚ç‚¹çš„æœ€å¤§åˆ†é…æ•° (å¯ç”¨å¤šæ ·åŒ–) (Max assignments per child - enable diversification)')
    parser.add_argument('--repair-min-assignments', type=int, default=None, 
                        help='æ„å»ºä¿®å¤æœŸé—´æ¯ä¸ªå­èŠ‚ç‚¹çš„æœ€å°åˆ†é…æ•° (éœ€è¦å¤šæ ·åŒ–) (Min assignments per child during build repair)')
    parser.add_argument('--hybrid-parent-level', type=int, default=2,
                        help='Hybrid HNSW çˆ¶èŠ‚ç‚¹å±‚çº§ (é»˜è®¤:2) (Parent level for level-based Hybrid HNSW)')
    parser.add_argument('--no-hybrid', action='store_true',
                        help='ç¦ç”¨Hybrid HNSWè¯„ä¼° (Disable Hybrid HNSW evaluation)')
    
    # Multi-pivot ç‰¹å®šé€‰é¡¹
    parser.add_argument('--enable-multi-pivot', action='store_true',
                        help='å¯ç”¨Multi-Pivot KMeans HNSWè¯„ä¼°')
    parser.add_argument('--num-pivots', type=int, default=3,
                        help='æ¯ä¸ªèšç±»çš„æ¢çº½ç‚¹æ•°é‡ (é»˜è®¤: 3)')
    parser.add_argument('--pivot-selection-strategy', type=str, default='line_perp_third',
                        choices=['line_perp_third', 'max_min_distance'],
                        help='æ¢çº½ç‚¹é€‰æ‹©ç­–ç•¥')
    parser.add_argument('--pivot-overquery-factor', type=float, default=1.2,
                        help='æ¢çº½æŸ¥è¯¢çš„è¿‡åº¦æŸ¥è¯¢å› å­ (é»˜è®¤: 1.2)')
    
    args = parser.parse_args()

    print("ğŸ”¬ K-Means HNSW + Multi-Pivotå‚æ•°è°ƒä¼˜å’Œè¯„ä¼°ç³»ç»Ÿ")
    print(f"ğŸ“Š è¯·æ±‚çš„æ•°æ®é›†å¤§å°: {args.dataset_size}, æŸ¥è¯¢å¤§å°: {args.query_size}")
    print(f"ğŸ¯ Multi-Pivotå¯ç”¨çŠ¶æ€: {args.enable_multi_pivot}")
    print(f"   Requested dataset size: {args.dataset_size}, query size: {args.query_size}")
    
    # å°è¯•åŠ è½½SIFTæ•°æ®ï¼Œå¤±è´¥åˆ™ä½¿ç”¨åˆæˆæ•°æ® (Try to load SIFT data, fall back to synthetic unless disabled)
    base_vectors, query_vectors = (None, None)
    if not args.no_sift:
        base_vectors, query_vectors = load_sift_data()
    
    if base_vectors is None:
        # åˆ›å»ºåˆæˆæ•°æ® (Create synthetic data)
        print("ğŸ² åˆ›å»ºåˆæˆæ•°æ®é›†... (Creating synthetic dataset)")
        base_vectors = np.random.randn(max(args.dataset_size, 10000), args.dimension).astype(np.float32)
        query_vectors = np.random.randn(max(args.query_size, 100), args.dimension).astype(np.float32)
    
    # åˆ‡ç‰‡åˆ°è¯·æ±‚çš„å¤§å° (æŒ‰å¯ç”¨é‡é™åˆ¶) (Slice to requested sizes)
    if len(base_vectors) > args.dataset_size:
        base_vectors = base_vectors[:args.dataset_size]
    if len(query_vectors) > args.query_size:
        query_vectors = query_vectors[:args.query_size]
    print(f"ğŸ“ˆ ä½¿ç”¨åŸºç¡€å‘é‡: {len(base_vectors)} | æŸ¥è¯¢: {len(query_vectors)} | ç»´åº¦: {base_vectors.shape[1]}")
    print(f"   Using base vectors: {len(base_vectors)} | queries: {len(query_vectors)} | dim: {base_vectors.shape[1]}")
    query_ids = list(range(len(query_vectors)))
    
    # è·ç¦»å‡½æ•° (Distance function)
    distance_func = lambda x, y: np.linalg.norm(x - y)
    
    # æ„å»ºåŸºç¡€HNSWç´¢å¼• (Build base HNSW index)
    print("ğŸ—ï¸  æ„å»ºåŸºç¡€HNSWç´¢å¼•... (Building base HNSW index)")
    # åŸºçº¿ HNSW ef å›ºå®šä¸º 200ï¼ˆç”¨æˆ·æŒ‡å®šé€»è¾‘ï¼‰
    base_index = HNSW(distance_func=distance_func, m=16, ef_construction=200)
    
    for i, vector in enumerate(base_vectors):
        base_index.insert(i, vector)
        if (i + 1) % 1000 == 0:
            print(f"  Inserted {i + 1}/{len(base_vectors)} vectors")
    
    print(f"Base HNSW index built with {len(base_index)} vectors")
    
    # Initialize evaluator
    evaluator = KMeansHNSWEvaluator(base_vectors, query_vectors, query_ids, distance_func)
    
    # Define parameter grid for sweep
    # Adjust default cluster count heuristics for larger datasets: scale choices
    if args.dataset_size <= 2000:
        cluster_options = [10]
    elif args.dataset_size <= 5000:
        cluster_options = [16, 32]
    else:
        cluster_options = [32, 64, 128]

    param_grid = {
        'n_clusters': cluster_options,
        'k_children': [200],
        'child_search_ef': [300]
    }
    
    evaluation_params = {
        'k_values': [10],
    'n_probe_values': [5, 10, 20],
    'hybrid_parent_level': args.hybrid_parent_level,
    'enable_hybrid': (not args.no_hybrid)
    }
    
    # Perform parameter sweep
    print("\nStarting parameter sweep...")
    # Limit combinations to keep runtime sane on large sets
    max_combos = 9 if len(cluster_options) > 1 else None
    
    # å‡†å¤‡è‡ªé€‚åº”é…ç½® (Prepare adaptive configuration)
    adaptive_config = {
        'adaptive_k_children': args.adaptive_k_children,
        'k_children_scale': args.k_children_scale,
        'k_children_min': args.k_children_min,
        'k_children_max': args.k_children_max,
        'diversify_max_assignments': args.diversify_max_assignments,
        'repair_min_assignments': args.repair_min_assignments if args.repair_min_assignments is not None else 1
    }
    
    # å‡†å¤‡Multi-Pivoté…ç½®
    multi_pivot_config = {
        'enabled': args.enable_multi_pivot,
        'num_pivots': args.num_pivots,
        'pivot_selection_strategy': args.pivot_selection_strategy,
        'pivot_overquery_factor': args.pivot_overquery_factor
    }
    
    sweep_results = evaluator.parameter_sweep(
        base_index,
        param_grid,
        evaluation_params,
        max_combinations=max_combos,
        adaptive_config=adaptive_config,
        multi_pivot_config=multi_pivot_config
    )
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªå‚æ•°ç»„åˆè¿›è¡Œæ¼”ç¤º (Use first parameter combination for demonstration)
    if sweep_results:
        # å–ç¬¬ä¸€ä¸ªæ‰«æç»“æœä½œä¸ºæ¼”ç¤ºå‚æ•°
        demo_result = sweep_results[0]
        demo_params = demo_result['parameters']
        print(f"\nUsing first parameter combination for demonstration: {demo_params}")
        print("\nğŸ¯ Parameter sweep completed! All comparisons are available in sweep_results.")

        # Save results
        results = {
            'sweep_results': sweep_results,
            'demo_parameters': demo_params,
            'multi_pivot_config': multi_pivot_config,
            'adaptive_config': {
                'adaptive_k_children': args.adaptive_k_children,
                'k_children_scale': args.k_children_scale,
                'k_children_min': args.k_children_min,
                'k_children_max': args.k_children_max,
                'diversify_max_assignments': args.diversify_max_assignments,
                'repair_min_assignments': args.repair_min_assignments,
                # manual repair parameters removed
            },
            'evaluation_info': {
                'dataset_size': len(base_vectors),
                'query_size': len(query_vectors),
                'dimension': base_vectors.shape[1],
                'multi_pivot_enabled': args.enable_multi_pivot,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
        }
        save_results(results, 'method3_tuning_results.json')
        
    print(f"\nâœ… Multi-Pivot parameter tuning completed!")
    if args.enable_multi_pivot:
        print("ğŸ¯ Five-method comparison results saved:")
        print("   1. HNSWåŸºçº¿ (HNSW Baseline)")
        print("   2. çº¯K-Means (Pure K-Means)")
        print("   3. Hybrid HNSW")  
        print("   4. KMeans HNSW (å•æ¢çº½)")
        print("   5. Multi-Pivot KMeans HNSW (å¤šæ¢çº½)")
    else:
        print("ğŸ’¡ æç¤º: ä½¿ç”¨ --enable-multi-pivot å¯ç”¨Multi-Pivotæ–¹æ¡ˆçš„å¯¹æ¯”è¯„ä¼°")
    
    print("Results saved to method3_tuning_results.json")