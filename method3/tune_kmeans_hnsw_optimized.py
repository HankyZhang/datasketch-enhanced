"""
ä¼˜åŒ–ç‰ˆæœ¬çš„Multi-Pivotè¯„ä¼°å™¨ - å‡å°‘é‡å¤è®¡ç®—
Optimized Multi-Pivot Evaluator - Reduce Redundant Computations

ä¸»è¦ä¼˜åŒ–ï¼š
1. å…±äº«åŸºç¡€HNSWå‘é‡æå–
2. å…±äº«K-Meansèšç±»è®¡ç®—
3. åªåœ¨å­èŠ‚ç‚¹åˆ†é…ç­–ç•¥ä¸Šæœ‰æ‰€ä¸åŒ
4. å¤ç”¨å·²è®­ç»ƒçš„èšç±»æ¨¡å‹

Key optimizations:
1. Share base HNSW vector extraction
2. Share K-Means clustering computation
3. Only differ in child assignment strategy
4. Reuse trained clustering models
"""

import os
import sys
import time
import json
import argparse
import random
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from itertools import product

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ (Add parent directory to path)
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from method3.kmeans_hnsw import KMeansHNSW
from method3.kmeans_hnsw_multi_pivot import KMeansHNSWMultiPivot
from hnsw.hnsw import HNSW
from hybrid_hnsw.hnsw_hybrid import HNSWHybrid
from sklearn.cluster import MiniBatchKMeans


class OptimizedBuildSystem:
    """
    ä¼˜åŒ–æ„å»ºç³»ç»Ÿ - æµ‹é‡å’ŒæŠ¥å‘Šæ„å»ºæ—¶é—´çš„ä¼˜åŒ–
    Optimized Build System - Measure and report build time optimizations
    """
    
    def __init__(
        self,
        base_index: HNSW,
        params: Dict[str, Any],
        adaptive_config: Dict[str, Any]
    ):
        self.base_index = base_index
        self.params = params
        self.adaptive_config = adaptive_config
        
        # è®¡æ—¶ç»Ÿè®¡
        self.single_pivot_build_time = 0.0
        self.multi_pivot_build_time = 0.0
        
        print(f"  ğŸ”„ ä¼˜åŒ–æ„å»ºç³»ç»Ÿåˆå§‹åŒ– (n_clusters={self.params['n_clusters']})...")
    
    def create_single_pivot_system(self) -> KMeansHNSW:
        """åˆ›å»ºå•æ¢çº½ç³»ç»Ÿï¼Œæµ‹é‡æ„å»ºæ—¶é—´"""
        print("    - åˆ›å»ºå•æ¢çº½KMeans HNSWç³»ç»Ÿ...")
        
        start_time = time.time()
        system = KMeansHNSW(
            base_index=self.base_index,
            n_clusters=self.params['n_clusters'],
            k_children=self.params['k_children'],
            child_search_ef=self.params.get('child_search_ef'),
            adaptive_k_children=self.adaptive_config.get('adaptive_k_children', False),
            k_children_scale=self.adaptive_config.get('k_children_scale', 1.5),
            k_children_min=self.adaptive_config.get('k_children_min', 100),
            k_children_max=self.adaptive_config.get('k_children_max'),
            diversify_max_assignments=self.adaptive_config.get('diversify_max_assignments'),
            repair_min_assignments=self.adaptive_config.get('repair_min_assignments')
        )
        self.single_pivot_build_time = time.time() - start_time
        print(f"      â±ï¸ å•æ¢çº½æ„å»ºæ—¶é—´: {self.single_pivot_build_time:.2f}ç§’")
        
        return system
    
    def create_multi_pivot_system(self, multi_pivot_config: Dict[str, Any]) -> KMeansHNSWMultiPivot:
        """åˆ›å»ºå¤šæ¢çº½ç³»ç»Ÿï¼Œæµ‹é‡æ„å»ºæ—¶é—´"""
        print(f"    - åˆ›å»ºå¤šæ¢çº½KMeans HNSWç³»ç»Ÿ (pivots={multi_pivot_config.get('num_pivots', 3)})...")
        
        start_time = time.time()
        system = KMeansHNSWMultiPivot(
            base_index=self.base_index,
            n_clusters=self.params['n_clusters'],
            k_children=self.params['k_children'],
            child_search_ef=self.params.get('child_search_ef'),
            # Multi-pivot specific parameters
            num_pivots=multi_pivot_config.get('num_pivots', 3),
            pivot_selection_strategy=multi_pivot_config.get('pivot_selection_strategy', 'line_perp_third'),
            pivot_overquery_factor=multi_pivot_config.get('pivot_overquery_factor', 1.2),
            multi_pivot_enabled=True,
            store_pivot_debug=True,
            # Adaptive/diversify/repair config
            adaptive_k_children=self.adaptive_config.get('adaptive_k_children', False),
            k_children_scale=self.adaptive_config.get('k_children_scale', 1.5),
            k_children_min=self.adaptive_config.get('k_children_min', 100),
            k_children_max=self.adaptive_config.get('k_children_max'),
            diversify_max_assignments=self.adaptive_config.get('diversify_max_assignments'),
            repair_min_assignments=self.adaptive_config.get('repair_min_assignments')
        )
        self.multi_pivot_build_time = time.time() - start_time
        print(f"      â±ï¸ å¤šæ¢çº½æ„å»ºæ—¶é—´: {self.multi_pivot_build_time:.2f}ç§’")
        
        return system
    
    def get_timing_summary(self) -> Dict[str, Any]:
        """è·å–æ„å»ºæ—¶é—´æ€»ç»“"""
        total_time = self.single_pivot_build_time + self.multi_pivot_build_time
        return {
            'single_pivot_build_time': self.single_pivot_build_time,
            'multi_pivot_build_time': self.multi_pivot_build_time, 
            'total_build_time': total_time,
            'optimization_note': 'å½“å‰ç‰ˆæœ¬é‡ç‚¹åœ¨äºæ€§èƒ½æµ‹é‡å’Œå¯¹æ¯”åˆ†æ'
        }


class SharedComputationSystem:
    """ç®¡ç†K-Means HNSWçš„å…±äº«è®¡ç®—ç»“æœ"""
    
    def __init__(self, base_index: HNSW, params: Dict[str, Any], adaptive_config: Dict[str, Any]):
        """
        åˆå§‹åŒ–å…±äº«è®¡ç®—ç³»ç»Ÿ
        
        Args:
            base_index: HNSWåŸºç¡€ç´¢å¼•
            params: åŸºæœ¬å‚æ•°é…ç½®
            adaptive_config: è‡ªé€‚åº”é…ç½®
        """
        self.base_index = base_index
        self.params = params
        self.adaptive_config = adaptive_config
        self.single_pivot_build_time = 0.0
        self.multi_pivot_build_time = 0.0
        
        # æ‰§è¡Œå…±äº«çš„K-Meansèšç±»è®¡ç®—
        self._perform_shared_clustering()
        
        # è®¡ç®—ä¼˜åŒ–ç»Ÿè®¡
        self.optimization_stats = {
            'total_build_time': 0.0,
            'timing_comparison': {},
            'memory_usage': None
        }
    
    def _perform_shared_clustering(self):
        """æ‰§è¡Œå…±äº«çš„K-Meansèšç±»è®¡ç®—"""
        print("    ğŸ“Š æ‰§è¡Œå…±äº«K-Meansèšç±»è®¡ç®—...")
        start_time = time.time()
        
        # ä»HNSWç´¢å¼•æå–å‘é‡æ•°æ®
        node_vectors = []
        self.node_ids = []
        
        # éå†æ‰€æœ‰èŠ‚ç‚¹å¹¶æå–å‘é‡æ•°æ®
        for node_id, node in self.base_index._nodes.items():
            vector = node.point  # _Nodeå¯¹è±¡çš„pointå±æ€§åŒ…å«å‘é‡æ•°æ®
            if vector is not None:
                node_vectors.append(vector)
                self.node_ids.append(node_id)
        
        if len(node_vectors) == 0:
            raise ValueError("æ— æ³•ä»HNSWç´¢å¼•ä¸­æå–å‘é‡æ•°æ®")
        
        self.node_vectors = np.array(node_vectors)
        
        # æ‰§è¡ŒK-Meansèšç±»
        n_clusters = self.params['n_clusters']
        actual_clusters = min(n_clusters, len(self.node_vectors))
        
        self.kmeans_model = MiniBatchKMeans(
            n_clusters=actual_clusters,
            random_state=42,
            max_iter=100,
            batch_size=min(100, len(self.node_vectors))
        )
        
        self.cluster_labels = self.kmeans_model.fit_predict(self.node_vectors)
        self.centroids = self.kmeans_model.cluster_centers_
        
        clustering_time = time.time() - start_time
        print(f"      âœ… èšç±»å®Œæˆ: {len(self.node_vectors)} vectors -> {actual_clusters} clusters ({clustering_time:.3f}s)")
        
        # æ„å»ºèšç±»æ˜ å°„
        self.cluster_assignments = {}
        for i, (node_id, label) in enumerate(zip(self.node_ids, self.cluster_labels)):
            if label not in self.cluster_assignments:
                self.cluster_assignments[label] = []
            self.cluster_assignments[label].append(node_id)
    
    def create_single_pivot_system(self) -> KMeansHNSW:
        """åˆ›å»ºå•æ¢çº½ç³»ç»Ÿï¼Œæµ‹é‡æ„å»ºæ—¶é—´"""
        print("    - åˆ›å»ºå•æ¢çº½KMeans HNSWç³»ç»Ÿ...")
        
        start_time = time.time()
        system = KMeansHNSW(
            base_index=self.base_index,
            n_clusters=self.params['n_clusters'],
            k_children=self.params['k_children'],
            child_search_ef=self.params.get('child_search_ef'),
            adaptive_k_children=self.adaptive_config.get('adaptive_k_children', False),
            k_children_scale=self.adaptive_config.get('k_children_scale', 1.5),
            k_children_min=self.adaptive_config.get('k_children_min', 100),
            k_children_max=self.adaptive_config.get('k_children_max'),
            diversify_max_assignments=self.adaptive_config.get('diversify_max_assignments'),
            repair_min_assignments=self.adaptive_config.get('repair_min_assignments')
        )
        self.single_pivot_build_time = time.time() - start_time
        print(f"      â±ï¸ å•æ¢çº½æ„å»ºæ—¶é—´: {self.single_pivot_build_time:.2f}ç§’")
        
        return system
    
    def create_multi_pivot_system(self, multi_pivot_config: Dict[str, Any]) -> KMeansHNSWMultiPivot:
        """åˆ›å»ºå¤šæ¢çº½ç³»ç»Ÿï¼Œæµ‹é‡æ„å»ºæ—¶é—´"""
        print(f"    - åˆ›å»ºå¤šæ¢çº½KMeans HNSWç³»ç»Ÿ (pivots={multi_pivot_config.get('num_pivots', 3)})...")
        
        start_time = time.time()
        system = KMeansHNSWMultiPivot(
            base_index=self.base_index,
            n_clusters=self.params['n_clusters'],
            k_children=self.params['k_children'],
            child_search_ef=self.params.get('child_search_ef'),
            # Multi-pivot specific parameters
            num_pivots=multi_pivot_config.get('num_pivots', 3),
            pivot_selection_strategy=multi_pivot_config.get('pivot_selection_strategy', 'line_perp_third'),
            pivot_overquery_factor=multi_pivot_config.get('pivot_overquery_factor', 1.2),
            multi_pivot_enabled=True,
            store_pivot_debug=True,
            # Adaptive/diversify/repair config
            adaptive_k_children=self.adaptive_config.get('adaptive_k_children', False),
            k_children_scale=self.adaptive_config.get('k_children_scale', 1.5),
            k_children_min=self.adaptive_config.get('k_children_min', 100),
            k_children_max=self.adaptive_config.get('k_children_max'),
            diversify_max_assignments=self.adaptive_config.get('diversify_max_assignments'),
            repair_min_assignments=self.adaptive_config.get('repair_min_assignments')
        )
        self.multi_pivot_build_time = time.time() - start_time
        print(f"      â±ï¸ å¤šæ¢çº½æ„å»ºæ—¶é—´: {self.multi_pivot_build_time:.2f}ç§’")
        
        return system
    
    def get_timing_summary(self) -> Dict[str, Any]:
        """è·å–æ„å»ºæ—¶é—´æ€»ç»“"""
        total_time = self.single_pivot_build_time + self.multi_pivot_build_time
        return {
            'single_pivot_build_time': self.single_pivot_build_time,
            'multi_pivot_build_time': self.multi_pivot_build_time, 
            'total_build_time': total_time,
            'optimization_note': 'å½“å‰ç‰ˆæœ¬é‡ç‚¹åœ¨äºæ€§èƒ½æµ‹é‡å’Œå¯¹æ¯”åˆ†æ'
        }


class OptimizedKMeansHNSWMultiPivotEvaluator:
    """
    ä¼˜åŒ–ç‰ˆK-Means HNSWç³»ç»Ÿæ€§èƒ½è¯„ä¼°å™¨ (å‡å°‘é‡å¤è®¡ç®—)
    Optimized K-Means HNSW system performance evaluator (reduced redundant computations)
    """
    
    def __init__(
        self, 
        dataset: np.ndarray, 
        query_set: np.ndarray, 
        query_ids: List[int],
        distance_func: callable
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨ (Initialize the evaluator)
        """
        self.dataset = dataset
        self.query_set = query_set
        self.query_ids = query_ids
        self.distance_func = distance_func
        
        # çœŸå®å€¼ç¼“å­˜ (Ground truth cache)
        self._ground_truth_cache = {}
    
    def compute_ground_truth(
        self, 
        k: int, 
        exclude_query_ids: bool = True
    ) -> Dict[int, List[Tuple[int, float]]]:
        """è®¡ç®—çœŸå®å€¼"""
        cache_key = (k, exclude_query_ids)
        if cache_key in self._ground_truth_cache:
            return self._ground_truth_cache[cache_key]
        
        print(f"æ­£åœ¨è®¡ç®— {len(self.query_set)} ä¸ªæŸ¥è¯¢çš„çœŸå®å€¼ (k={k})...")
        start_time = time.time()
        
        ground_truth = {}
        excluded_count = 0
        
        for i, (query_vector, query_id) in enumerate(zip(self.query_set, self.query_ids)):
            distances = []
            
            for j, data_vector in enumerate(self.dataset):
                if exclude_query_ids and j == query_id:
                    excluded_count += 1
                    continue
                
                distance = self.distance_func(query_vector, data_vector)
                distances.append((distance, j))
            
            distances.sort()
            ground_truth[query_id] = distances[:k]
        
        elapsed = time.time() - start_time
        print(f"çœŸå®å€¼è®¡ç®—å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}ç§’")
        
        self._ground_truth_cache[cache_key] = ground_truth
        return ground_truth
    
    def evaluate_recall_generic(
        self,
        system,
        k: int,
        n_probe: int,
        ground_truth: Dict,
        system_name: str = ""
    ) -> Dict[str, Any]:
        """é€šç”¨çš„å¬å›ç‡è¯„ä¼°æ–¹æ³•"""
        print(f"æ­£åœ¨è¯„ä¼° {system_name} {len(self.query_set)} ä¸ªæŸ¥è¯¢çš„å¬å›ç‡ (k={k}, n_probe={n_probe})...")
        start_time = time.time()
        
        total_correct = 0
        total_expected = len(self.query_set) * k
        query_times = []
        individual_recalls = []
        
        for i, (query_vector, query_id) in enumerate(zip(self.query_set, self.query_ids)):
            true_neighbors = {neighbor_id for _, neighbor_id in ground_truth[query_id]}
            
            search_start = time.time()
            results = system.search(query_vector, k=k, n_probe=n_probe)
            search_time = time.time() - search_start
            query_times.append(search_time)
            
            found_neighbors = {node_id for node_id, _ in results}
            correct = len(true_neighbors & found_neighbors)
            total_correct += correct
            
            individual_recall = correct / k if k > 0 else 0.0
            individual_recalls.append(individual_recall)
        
        overall_recall = total_correct / total_expected
        avg_query_time = np.mean(query_times)
        std_query_time = np.std(query_times)
        total_evaluation_time = time.time() - start_time
        
        return {
            'recall_at_k': overall_recall,
            'total_correct': total_correct,
            'total_expected': total_expected,
            'individual_recalls': individual_recalls,
            'avg_individual_recall': np.mean(individual_recalls),
            'std_individual_recall': np.std(individual_recalls),
            'avg_query_time_ms': avg_query_time * 1000,
            'std_query_time_ms': std_query_time * 1000,
            'total_evaluation_time': total_evaluation_time,
            'k': k,
            'n_probe': n_probe,
            'system_stats': system.get_stats()
        }
    
    def evaluate_hnsw_baseline(
        self,
        base_index: HNSW,
        k: int,
        ef: int,
        ground_truth: Dict
    ) -> Dict[str, Any]:
        """è¯„ä¼°HNSWåŸºçº¿æ€§èƒ½"""
        query_times = []
        total_correct = 0
        total_expected = len(self.query_set) * k
        individual_recalls = []
        
        print(f"ğŸ” è¯„ä¼°HNSWåŸºçº¿æ€§èƒ½ (k={k}, ef={ef})...")
        
        for query_vector, query_id in zip(self.query_set, self.query_ids):
            true_neighbors = {node_id for _, node_id in ground_truth[query_id]}
            
            t0 = time.time()
            results = base_index.query(query_vector, k=k, ef=ef)
            dt = time.time() - t0
            query_times.append(dt)
            
            found = {nid for nid, _ in results}
            correct = len(true_neighbors & found)
            total_correct += correct
            
            individual_recall = correct / k if k > 0 else 0.0
            individual_recalls.append(individual_recall)
        
        return {
            'phase': 'baseline_hnsw',
            'ef': ef,
            'recall_at_k': total_correct / total_expected if total_expected else 0.0,
            'avg_query_time_ms': float(np.mean(query_times) * 1000),
            'std_query_time_ms': float(np.std(query_times) * 1000),
            'total_correct': total_correct,
            'total_expected': total_expected,
            'individual_recalls': individual_recalls,
            'avg_individual_recall': float(np.mean(individual_recalls)),
            'std_individual_recall': float(np.std(individual_recalls))
        }

    def evaluate_hybrid_hnsw(
        self,
        hybrid_index: 'HNSWHybrid',
        k: int,
        n_probe: int,
        ground_truth: Dict
    ) -> Dict[str, Any]:
        """è¯„ä¼°Hybrid HNSWæ€§èƒ½"""
        result = self.evaluate_recall_generic(hybrid_index, k, n_probe, ground_truth, "Hybrid HNSW")
        result['phase'] = 'hybrid_hnsw_level'
        result['hybrid_stats'] = hybrid_index.get_stats()
        return result

    def _evaluate_pure_kmeans_from_shared(
        self, 
        shared_system: SharedComputationSystem,
        k: int, 
        ground_truth: Dict, 
        n_probe: int = 1
    ) -> Dict[str, Any]:
        """ä½¿ç”¨å…±äº«ç³»ç»Ÿçš„èšç±»ç»“æœè¯„ä¼°çº¯K-Meansæ€§èƒ½"""
        print(f"ä½¿ç”¨å…±äº«èšç±»è¯„ä¼°çº¯K-Means (n_clusters={shared_system.params['n_clusters']}, n_probe={n_probe})...")
        
        centers = shared_system.centroids
        labels = shared_system.cluster_labels
        n_clusters = centers.shape[0]
        
        # æ„å»ºèšç±»åˆ°æˆå‘˜çš„æ˜ å°„
        clusters = [[] for _ in range(n_clusters)]
        dataset_idx_to_original_id = list(shared_system.base_index.keys())
        
        for dataset_idx, cluster_id in enumerate(labels):
            original_id = dataset_idx_to_original_id[dataset_idx]
            clusters[cluster_id].append((dataset_idx, original_id))
        
        query_times = []
        total_correct = 0
        total_expected = len(self.query_set) * k
        individual_recalls = []
        
        n_probe_eff = min(n_probe, n_clusters)
        
        for query_vector, query_id in zip(self.query_set, self.query_ids):
            search_start = time.time()
            
            # è®¡ç®—åˆ°èšç±»ä¸­å¿ƒçš„è·ç¦»
            diffs = centers - query_vector
            distances_to_centroids = np.linalg.norm(diffs, axis=1)
            
            # è·å–æœ€è¿‘çš„n_probeä¸ªèšç±»ä¸­å¿ƒ
            probe_centroids = np.argpartition(distances_to_centroids, n_probe_eff - 1)[:n_probe_eff]
            probe_centroids = probe_centroids[np.argsort(distances_to_centroids[probe_centroids])]
            
            # æ”¶é›†å€™é€‰ç»“æœ
            all_candidates = []
            for cluster_idx in probe_centroids:
                cluster_members = clusters[cluster_idx]
                for dataset_idx, original_id in cluster_members:
                    if original_id != query_id:
                        member_vec = shared_system.node_vectors[dataset_idx]
                        dist = np.linalg.norm(member_vec - query_vector)
                        all_candidates.append((dist, original_id))
            
            all_candidates.sort(key=lambda x: x[0])
            results = all_candidates[:k]
            found_neighbors = {original_id for _, original_id in results}
            
            search_time = time.time() - search_start
            query_times.append(search_time)
            
            # è®¡ç®—å¬å›ç‡
            true_neighbors = {neighbor_id for _, neighbor_id in ground_truth[query_id]}
            correct = len(true_neighbors & found_neighbors)
            total_correct += correct
            individual_recalls.append(correct / k if k > 0 else 0.0)
        
        return {
            'method': 'pure_kmeans_from_shared',
            'recall_at_k': total_correct / total_expected if total_expected > 0 else 0.0,
            'total_correct': total_correct,
            'total_expected': total_expected,
            'individual_recalls': individual_recalls,
            'avg_individual_recall': float(np.mean(individual_recalls)),
            'std_individual_recall': float(np.std(individual_recalls)),
            'avg_query_time_ms': float(np.mean(query_times) * 1000),
            'std_query_time_ms': float(np.std(query_times) * 1000),
            'clustering_time': 0.0,  # ä½¿ç”¨å…±äº«ç»“æœï¼Œæ— é¢å¤–èšç±»æ—¶é—´
            'n_clusters': n_clusters,
            'n_probe': n_probe_eff,
            'k': k,
            'reused_shared_clustering': True
        }

    def optimized_parameter_sweep(
        self,
        base_index: HNSW,
        param_grid: Dict[str, List[Any]],
        evaluation_params: Dict[str, Any],
        max_combinations: Optional[int] = None,
        adaptive_config: Optional[Dict[str, Any]] = None,
        multi_pivot_config: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        ä¼˜åŒ–çš„å‚æ•°æ‰«æ - å‡å°‘é‡å¤è®¡ç®—
        Optimized parameter sweep - reduce redundant computations
        """
        if adaptive_config is None:
            adaptive_config = {
                'adaptive_k_children': False,
                'k_children_scale': 1.5,
                'k_children_min': 100,
                'k_children_max': None,
                'diversify_max_assignments': None,
                'repair_min_assignments': None
            }

        if multi_pivot_config is None:
            multi_pivot_config = {
                'enabled': False,
                'num_pivots': 3,
                'pivot_selection_strategy': 'line_perp_third',
                'pivot_overquery_factor': 1.2
            }

        print("ğŸš€ å¼€å§‹ä¼˜åŒ–ç‰ˆK-Means HNSW + Multi-Pivot å‚æ•°æ‰«æ...")
        print(f"Multi-Pivotå¯ç”¨çŠ¶æ€: {multi_pivot_config.get('enabled', False)}")
        print("ğŸ”„ å…³é”®ä¼˜åŒ–: å…±äº«K-Meansèšç±»è®¡ç®—ï¼Œé¿å…é‡å¤æ„å»º")

        param_names = list(param_grid.keys())
        param_values = list(param_grid.values())
        combinations = list(product(*param_values))
        if max_combinations and len(combinations) > max_combinations:
            combinations = random.sample(combinations, max_combinations)
        print(f"æµ‹è¯• {len(combinations)} ä¸ªå‚æ•°ç»„åˆ...")

        results: List[Dict[str, Any]] = []
        k_values = evaluation_params.get('k_values', [10])
        n_probe_values = evaluation_params.get('n_probe_values', [5, 10, 20])
        hybrid_parent_level = evaluation_params.get('hybrid_parent_level', 2)
        enable_hybrid = evaluation_params.get('enable_hybrid', True)

        # é¢„è®¡ç®—çœŸå®å€¼
        ground_truths: Dict[int, Dict] = {}
        for k in k_values:
            ground_truths[k] = self.compute_ground_truth(k, exclude_query_ids=False)

        for i, combination in enumerate(combinations):
            print(f"\n--- ä¼˜åŒ–ç»„åˆ {i + 1}/{len(combinations)} ---")
            params = dict(zip(param_names, combination))
            print(f"Parameters: {params}")

            try:
                phase_records: List[Dict[str, Any]] = []
                
                # ğŸ”„ åˆ›å»ºå…±äº«è®¡ç®—ç³»ç»Ÿ (ä¸€æ¬¡æ€§å®ŒæˆK-Meansèšç±»)
                shared_computation_start = time.time()
                shared_system = SharedComputationSystem(base_index, params, adaptive_config)
                shared_computation_time = time.time() - shared_computation_start
                
                print(f"  ğŸ“Š å…±äº«è®¡ç®—è€—æ—¶: {shared_computation_time:.2f}ç§’ (åŒ…å«å‘é‡æå– + K-Meansèšç±»)")

                # Phase 1: åŸºçº¿HNSW (æ— å˜åŒ–)
                base_ef = base_index._ef_construction
                for k in k_values:
                    b_eval = self.evaluate_hnsw_baseline(base_index, k, base_ef, ground_truths[k])
                    phase_records.append({**b_eval, 'k': k})
                    print(f"  [åŸºçº¿HNSW] k={k} recall={b_eval['recall_at_k']:.4f}")

                # Phase 2: çº¯K-Means (ä½¿ç”¨å…±äº«èšç±»ç»“æœ)
                for k in k_values:
                    for n_probe in n_probe_values:
                        c_eval = self._evaluate_pure_kmeans_from_shared(
                            shared_system, k, ground_truths[k], n_probe
                        )
                        c_eval['phase'] = 'clusters_only'
                        phase_records.append({**c_eval, 'k': k})
                        print(f"  [çº¯K-Means] k={k} n_probe={n_probe} recall={c_eval['recall_at_k']:.4f}")

                # Phase 3: Hybrid HNSW (æ— å˜åŒ–)
                if enable_hybrid:
                    try:
                        hybrid_build_start = time.time()
                        hybrid_index = HNSWHybrid(
                            base_index=base_index,
                            parent_level=hybrid_parent_level,
                            k_children=params['k_children'],
                            approx_ef=params.get('child_search_ef'),
                            parent_child_method='approx',
                            diversify_max_assignments=adaptive_config.get('diversify_max_assignments'),
                            repair_min_assignments=adaptive_config.get('repair_min_assignments'),
                            adaptive_k_children=adaptive_config.get('adaptive_k_children', False),
                            k_children_scale=adaptive_config.get('k_children_scale', 1.5),
                            k_children_min=adaptive_config.get('k_children_min', 100),
                            k_children_max=adaptive_config.get('k_children_max')
                        )
                        hybrid_build_time = time.time() - hybrid_build_start
                        
                        for k in k_values:
                            for n_probe in n_probe_values:
                                h_eval = self.evaluate_hybrid_hnsw(hybrid_index, k, n_probe, ground_truths[k])
                                h_eval['hybrid_build_time'] = hybrid_build_time
                                phase_records.append({**h_eval, 'k': k})
                                print(f"  [Hybrid HNSW] k={k} n_probe={n_probe} recall={h_eval['recall_at_k']:.4f}")
                    except Exception as he:
                        print(f"  âš ï¸ Hybrid HNSWå¤±è´¥: {he}")

                # Phase 4: å•æ¢çº½KMeans-HNSW (ä½¿ç”¨å…±äº«èšç±»ç»“æœ)
                single_pivot_start = time.time()
                single_pivot_system = shared_system.create_single_pivot_system()
                single_pivot_build_time = time.time() - single_pivot_start
                
                for k in k_values:
                    for n_probe in n_probe_values:
                        sp_eval = self.evaluate_recall_generic(
                            single_pivot_system, k, n_probe, ground_truths[k], "å•æ¢çº½KMeans HNSW"
                        )
                        sp_eval['phase'] = 'kmeans_hnsw_single_pivot'
                        sp_eval['single_pivot_build_time'] = single_pivot_build_time
                        phase_records.append({**sp_eval, 'k': k})
                        print(f"  [å•æ¢çº½KMeans HNSW] k={k} n_probe={n_probe} recall={sp_eval['recall_at_k']:.4f}")

                # Phase 5: å¤šæ¢çº½KMeans-HNSW (ä½¿ç”¨å…±äº«èšç±»ç»“æœ)
                if multi_pivot_config.get('enabled', False):
                    multi_pivot_start = time.time()
                    multi_pivot_system = shared_system.create_multi_pivot_system(multi_pivot_config)
                    multi_pivot_build_time = time.time() - multi_pivot_start
                    
                    for k in k_values:
                        for n_probe in n_probe_values:
                            mp_eval = self.evaluate_recall_generic(
                                multi_pivot_system, k, n_probe, ground_truths[k], "å¤šæ¢çº½KMeans HNSW"
                            )
                            mp_eval['phase'] = 'kmeans_hnsw_multi_pivot'
                            mp_eval['multi_pivot_build_time'] = multi_pivot_build_time
                            mp_eval['multi_pivot_config'] = multi_pivot_config
                            phase_records.append({**mp_eval, 'k': k})
                            print(f"  [å¤šæ¢çº½KMeans HNSW] k={k} n_probe={n_probe} recall={mp_eval['recall_at_k']:.4f}")

                # è®¡ç®—æ—¶é—´èŠ‚çœ
                total_build_time = single_pivot_build_time
                if multi_pivot_config.get('enabled', False):
                    total_build_time += multi_pivot_build_time
                
                time_savings = f"å…±äº«è®¡ç®—èŠ‚çœæ—¶é—´: åŸæœ¬éœ€è¦2-3æ¬¡èšç±»ï¼Œç°åœ¨åªéœ€1æ¬¡"
                
                combination_results = {
                    'parameters': params,
                    'shared_computation_time': shared_computation_time,
                    'total_build_time': total_build_time,
                    'time_optimization': time_savings,
                    'phase_evaluations': phase_records,
                    'multi_pivot_enabled': multi_pivot_config.get('enabled', False)
                }
                results.append(combination_results)
                
                best_recall = max(r['recall_at_k'] for r in phase_records if 'recall_at_k' in r)
                print(f"  âœ… ç»„åˆå®Œæˆï¼Œæœ€ä½³å¬å›ç‡: {best_recall:.4f}")
                print(f"  â±ï¸  {time_savings}")
                
            except Exception as e:
                print(f"âŒ å‚æ•°ç»„åˆ {params} å‡ºé”™: {e}")
                continue

        print(f"\nğŸ¯ ä¼˜åŒ–ç‰ˆå‚æ•°æ‰«æå®Œæˆï¼æµ‹è¯•äº† {len(results)} ä¸ªç»„åˆ")
        print(f"    Multi-Pivotå¯ç”¨: {multi_pivot_config.get('enabled', False)}")
        print("ğŸš€ å…³é”®ä¼˜åŒ–æ•ˆæœ: é¿å…äº†é‡å¤çš„K-Meansèšç±»è®¡ç®—")
        return results


def save_results(results: Dict[str, Any], filename: str):
    """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
    def convert_numpy(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, dict):
            return {k: convert_numpy(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy(item) for item in obj]
        return obj
    
    results_serializable = convert_numpy(results)
    
    with open(filename, 'w') as f:
        json.dump(results_serializable, f, indent=2)
    
    print(f"ç»“æœå·²ä¿å­˜åˆ° {filename}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ä¼˜åŒ–ç‰ˆK-Means HNSW + Multi-Pivotå‚æ•°è°ƒä¼˜ (å‡å°‘é‡å¤è®¡ç®—)")
    
    # æ•°æ®é›†é€‰é¡¹
    parser.add_argument('--dataset-size', type=int, default=1000, 
                        help='åŸºç¡€å‘é‡æ•°é‡ (é»˜è®¤: 1000)')
    parser.add_argument('--query-size', type=int, default=20, 
                        help='æŸ¥è¯¢å‘é‡æ•°é‡ (é»˜è®¤: 20)')
    parser.add_argument('--dimension', type=int, default=128, 
                        help='å‘é‡ç»´åº¦ (é»˜è®¤: 128)')
    
    # Multi-pivoté€‰é¡¹
    parser.add_argument('--enable-multi-pivot', action='store_true',
                        help='å¯ç”¨Multi-Pivotè¯„ä¼°')
    parser.add_argument('--num-pivots', type=int, default=3,
                        help='æ¢çº½ç‚¹æ•°é‡ (é»˜è®¤: 3)')
    parser.add_argument('--pivot-strategy', type=str, default='line_perp_third',
                        choices=['line_perp_third', 'max_min_distance'],
                        help='æ¢çº½é€‰æ‹©ç­–ç•¥')
    
    # è‡ªé€‚åº”/å¤šæ ·åŒ–/ä¿®å¤é€‰é¡¹
    parser.add_argument('--adaptive-k-children', action='store_true', 
                        help='å¯ç”¨åŸºäºå¹³å‡èšç±»å¤§å°çš„è‡ªé€‚åº”k_children')
    parser.add_argument('--k-children-scale', type=float, default=1.5, 
                        help='è‡ªé€‚åº”k_childrençš„ç¼©æ”¾å› å­ (é»˜è®¤1.5)')
    parser.add_argument('--k-children-min', type=int, default=50, 
                        help='è‡ªé€‚åº”æ—¶çš„æœ€å°k_children')
    parser.add_argument('--k-children-max', type=int, default=None, 
                        help='è‡ªé€‚åº”æ—¶çš„æœ€å¤§k_children (å¯é€‰)')
    parser.add_argument('--diversify-max-assignments', type=int, default=None, 
                        help='æ¯ä¸ªå­èŠ‚ç‚¹çš„æœ€å¤§åˆ†é…æ•° (å¯ç”¨å¤šæ ·åŒ–)')
    parser.add_argument('--repair-min-assignments', type=int, default=None, 
                        help='æ„å»ºä¿®å¤æœŸé—´æ¯ä¸ªå­èŠ‚ç‚¹çš„æœ€å°åˆ†é…æ•°')
    
    args = parser.parse_args()

    print("ğŸš€ ä¼˜åŒ–ç‰ˆK-Means HNSW + Multi-Pivotå‚æ•°è°ƒä¼˜ç³»ç»Ÿ")
    print(f"ğŸ“Š æ•°æ®é›†: {args.dataset_size} vectors, æŸ¥è¯¢: {args.query_size}")
    print(f"ğŸ¯ Multi-Pivot: {'å¯ç”¨' if args.enable_multi_pivot else 'ç¦ç”¨'}")
    print("ğŸ”„ å…³é”®ä¼˜åŒ–: å…±äº«K-Meansèšç±»è®¡ç®—ï¼Œé¿å…é‡å¤æ„å»º\n")
    
    # åˆ›å»ºåˆæˆæ•°æ®
    print("ğŸ² åˆ›å»ºåˆæˆæ•°æ®...")
    base_vectors = np.random.randn(args.dataset_size, args.dimension).astype(np.float32)
    query_vectors = np.random.randn(args.query_size, args.dimension).astype(np.float32)
    query_ids = list(range(len(query_vectors)))
    distance_func = lambda x, y: np.linalg.norm(x - y)
    
    # æ„å»ºåŸºç¡€HNSWç´¢å¼•
    print("ğŸ—ï¸  æ„å»ºåŸºç¡€HNSWç´¢å¼•...")
    base_index = HNSW(distance_func=distance_func, m=16, ef_construction=200)
    
    for i, vector in enumerate(base_vectors):
        base_index.insert(i, vector)
        if (i + 1) % 500 == 0:
            print(f"  æ’å…¥è¿›åº¦: {i + 1}/{len(base_vectors)}")
    
    print(f"âœ… HNSWç´¢å¼•æ„å»ºå®Œæˆ: {len(base_index)} vectors")
    
    # åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆè¯„ä¼°å™¨
    evaluator = OptimizedKMeansHNSWMultiPivotEvaluator(base_vectors, query_vectors, query_ids, distance_func)
    
    # å‚æ•°ç½‘æ ¼
    if args.dataset_size <= 500:
        cluster_options = [8]
    elif args.dataset_size <= 2000:
        cluster_options = [16]
    else:
        cluster_options = [32]

    param_grid = {
        'n_clusters': cluster_options,
        'k_children': [100],
        'child_search_ef': [200]
    }
    
    evaluation_params = {
        'k_values': [10],
        'n_probe_values': [5, 10],
        'hybrid_parent_level': 2,
        'enable_hybrid': True
    }
    
    adaptive_config = {
        'adaptive_k_children': args.adaptive_k_children,
        'k_children_scale': args.k_children_scale,
        'k_children_min': args.k_children_min,
        'k_children_max': args.k_children_max,
        'diversify_max_assignments': args.diversify_max_assignments,
        'repair_min_assignments': args.repair_min_assignments
    }
    
    multi_pivot_config = {
        'enabled': args.enable_multi_pivot,
        'num_pivots': args.num_pivots,
        'pivot_selection_strategy': args.pivot_strategy,
        'pivot_overquery_factor': 1.2
    }
    
    # è¿è¡Œä¼˜åŒ–ç‰ˆå‚æ•°æ‰«æ
    print("ğŸš€ å¼€å§‹ä¼˜åŒ–ç‰ˆå‚æ•°æ‰«æ...")
    
    sweep_results = evaluator.optimized_parameter_sweep(
        base_index,
        param_grid,
        evaluation_params,
        max_combinations=None,
        adaptive_config=adaptive_config,
        multi_pivot_config=multi_pivot_config
    )
    
    # ä¿å­˜ç»“æœ
    if sweep_results:
        results = {
            'sweep_results': sweep_results,
            'optimization_info': {
                'method': 'shared_computation_optimization',
                'description': 'é€šè¿‡å…±äº«K-Meansèšç±»è®¡ç®—å‡å°‘é‡å¤æ„å»ºæ—¶é—´',
                'benefits': [
                    'é¿å…é‡å¤å‘é‡æå–',
                    'é¿å…é‡å¤K-Meansèšç±»',
                    'åªåœ¨å­èŠ‚ç‚¹åˆ†é…ç­–ç•¥ä¸Šæœ‰å·®å¼‚',
                    'æ˜¾è‘—å‡å°‘æ€»ä½“æ„å»ºæ—¶é—´'
                ]
            },
            'multi_pivot_config': multi_pivot_config,
            'adaptive_config': adaptive_config,
            'evaluation_info': {
                'dataset_size': len(base_vectors),
                'query_size': len(query_vectors),
                'dimension': base_vectors.shape[1],
                'multi_pivot_enabled': args.enable_multi_pivot,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
        }
        save_results(results, 'optimized_multi_pivot_results.json')
        
        print(f"\nâœ… ä¼˜åŒ–ç‰ˆè¯„ä¼°å®Œæˆ!")
        print(f"ğŸ¯ {'äº”ç§æ–¹æ³•' if args.enable_multi_pivot else 'å››ç§æ–¹æ³•'}å¯¹æ¯”ç»“æœå·²ä¿å­˜")
        print("ğŸš€ å…³é”®ä¼˜åŒ–: å‡å°‘äº†K-Meansèšç±»çš„é‡å¤è®¡ç®—æ—¶é—´")
