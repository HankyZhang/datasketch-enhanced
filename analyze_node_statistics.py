"""
èŠ‚ç‚¹ç»Ÿè®¡åˆ†æ - æ¯”è¾ƒä¸åŒæ–¹æ³•çš„èŠ‚ç‚¹åˆ†é…æƒ…å†µ
åˆ†æKMeansHNSWã€KMeansHNSW Multi-Pivotã€HybridHNSWåœ¨repairå‰åçš„èŠ‚ç‚¹ç»Ÿè®¡
"""
import numpy as np
import sys
import os
import time
from collections import defaultdict
from typing import Dict, List, Tuple, Set, Any

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from method3.kmeans_hnsw import KMeansHNSW
from method3.kmeans_hnsw_multi_pivot import KMeansHNSWMultiPivot
from method3.tune_kmeans_hnsw import KMeansHNSWEvaluator
from hnsw.hnsw import HNSW
from hybrid_hnsw.hnsw_hybrid import HNSWHybrid

def analyze_node_statistics():
    """è¯¦ç»†åˆ†æä¸åŒç³»ç»Ÿçš„èŠ‚ç‚¹åˆ†é…ç»Ÿè®¡"""
    print("ğŸ“Š èŠ‚ç‚¹ç»Ÿè®¡åˆ†æ - æ¯”è¾ƒä¸åŒæ–¹æ³•çš„èŠ‚ç‚¹åˆ†é…æƒ…å†µ")
    print("="*70)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    n_data = 5000
    n_queries = 50
    dim = 128
    
    dataset = np.random.randn(n_data, dim).astype(np.float32)
    query_vectors = np.random.randn(n_queries, dim).astype(np.float32)
    query_ids = list(range(n_queries))
    
    distance_func = lambda x, y: np.linalg.norm(x - y)
    
    # æ„å»ºåŸºç¡€HNSWç´¢å¼•
    print("ğŸ—ï¸ æ„å»ºåŸºç¡€HNSWç´¢å¼•...")
    base_index = HNSW(distance_func=distance_func, m=16, ef_construction=200)
    for i, vector in enumerate(dataset):
        base_index.insert(i, vector)
    
    print(f"âœ… åŸºç¡€HNSWç´¢å¼•: {len(base_index)} ä¸ªèŠ‚ç‚¹")
    
    # æµ‹è¯•å‚æ•°
    test_params = {
        'n_clusters': 32,
        'k_children': 200,
        'child_search_ef': 300
    }
    
    print(f"\nğŸ”§ æµ‹è¯•å‚æ•°: {test_params}")
    print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {n_data}, æŸ¥è¯¢æ•°é‡: {n_queries}")
    
    # === 1. KMeansHNSW (åŸå§‹å•æ¢çº½) ===
    print(f"\n{'='*70}")
    print("1ï¸âƒ£ KMeansHNSW (åŸå§‹å•æ¢çº½ç³»ç»Ÿ)")
    print("="*70)
    
    # æ— repairç‰ˆæœ¬
    print("\nğŸ“‹ æµ‹è¯•: æ— repair/diversify")
    kmeans_hnsw_clean = KMeansHNSW(
        base_index=base_index,
        n_clusters=test_params['n_clusters'],
        k_children=test_params['k_children'],
        child_search_ef=test_params['child_search_ef'],
        diversify_max_assignments=None,
        repair_min_assignments=None
    )
    
    stats_clean = analyze_system_nodes(kmeans_hnsw_clean, "KMeansHNSW (æ— repair)")
    
    # å¸¦repairç‰ˆæœ¬
    print("\nğŸ“‹ æµ‹è¯•: å¸¦repair=1")
    kmeans_hnsw_repair = KMeansHNSW(
        base_index=base_index,
        n_clusters=test_params['n_clusters'],
        k_children=test_params['k_children'],
        child_search_ef=test_params['child_search_ef'],
        diversify_max_assignments=None,
        repair_min_assignments=1
    )
    
    stats_repair = analyze_system_nodes(kmeans_hnsw_repair, "KMeansHNSW (repair=1)")
    
    # === 2. KMeansHNSW Multi-Pivot ===
    print(f"\n{'='*70}")
    print("2ï¸âƒ£ KMeansHNSW Multi-Pivot (å¤šæ¢çº½ç³»ç»Ÿ)")
    print("="*70)
    
    # æ— repairç‰ˆæœ¬
    print("\nğŸ“‹ æµ‹è¯•: æ— repair/diversify")
    try:
        multi_pivot_clean = KMeansHNSWMultiPivot(
            base_index=base_index,
            n_clusters=test_params['n_clusters'],
            k_children=test_params['k_children'],
            child_search_ef=test_params['child_search_ef'],
            num_pivots=3,
            pivot_selection_strategy='line_perp_third',
            diversify_max_assignments=None,
            repair_min_assignments=None
        )
        
        multi_stats_clean = analyze_system_nodes(multi_pivot_clean, "Multi-Pivot (æ— repair)")
    except Exception as e:
        print(f"âŒ Multi-Pivot (æ— repair) æ„å»ºå¤±è´¥: {e}")
        multi_stats_clean = None
    
    # å¸¦repairç‰ˆæœ¬
    print("\nğŸ“‹ æµ‹è¯•: å¸¦repair=1")
    try:
        multi_pivot_repair = KMeansHNSWMultiPivot(
            base_index=base_index,
            n_clusters=test_params['n_clusters'],
            k_children=test_params['k_children'],
            child_search_ef=test_params['child_search_ef'],
            num_pivots=3,
            pivot_selection_strategy='line_perp_third',
            diversify_max_assignments=None,
            repair_min_assignments=1
        )
        
        multi_stats_repair = analyze_system_nodes(multi_pivot_repair, "Multi-Pivot (repair=1)")
    except Exception as e:
        print(f"âŒ Multi-Pivot (repair=1) æ„å»ºå¤±è´¥: {e}")
        multi_stats_repair = None
    
    # === 3. Hybrid HNSW ===
    print(f"\n{'='*70}")
    print("3ï¸âƒ£ Hybrid HNSW (å±‚çº§ç³»ç»Ÿ)")
    print("="*70)
    
    try:
        hybrid_index = HNSWHybrid(
            base_index=base_index,
            parent_level=2,
            k_children=test_params['k_children'],
            child_search_ef=test_params['child_search_ef']
        )
        
        hybrid_stats = analyze_hybrid_nodes(hybrid_index, "Hybrid HNSW")
    except Exception as e:
        print(f"âŒ Hybrid HNSW æ„å»ºå¤±è´¥: {e}")
        hybrid_stats = None
    
    # === 4. å¬å›ç‡æµ‹è¯• ===
    print(f"\n{'='*70}")
    print("4ï¸âƒ£ å¬å›ç‡å¯¹æ¯”æµ‹è¯•")
    print("="*70)
    
    evaluator = KMeansHNSWEvaluator(dataset, query_vectors, query_ids, distance_func)
    ground_truth = evaluator.compute_ground_truth(k=10, exclude_query_ids=False)
    
    recall_results = {}
    
    # æµ‹è¯•KMeansHNSW
    if kmeans_hnsw_clean:
        recall_clean = evaluator.evaluate_recall(kmeans_hnsw_clean, k=10, n_probe=5, ground_truth=ground_truth)
        recall_results['KMeansHNSW (æ— repair)'] = recall_clean['recall_at_k']
        print(f"ğŸ“Š KMeansHNSW (æ— repair) å¬å›ç‡: {recall_clean['recall_at_k']:.4f}")
    
    if kmeans_hnsw_repair:
        recall_repair = evaluator.evaluate_recall(kmeans_hnsw_repair, k=10, n_probe=5, ground_truth=ground_truth)
        recall_results['KMeansHNSW (repair=1)'] = recall_repair['recall_at_k']
        print(f"ğŸ“Š KMeansHNSW (repair=1) å¬å›ç‡: {recall_repair['recall_at_k']:.4f}")
    
    # æµ‹è¯•Multi-Pivot
    if multi_pivot_clean:
        recall_multi_clean = evaluator.evaluate_recall(multi_pivot_clean, k=10, n_probe=5, ground_truth=ground_truth)
        recall_results['Multi-Pivot (æ— repair)'] = recall_multi_clean['recall_at_k']
        print(f"ğŸ“Š Multi-Pivot (æ— repair) å¬å›ç‡: {recall_multi_clean['recall_at_k']:.4f}")
    
    if multi_pivot_repair:
        recall_multi_repair = evaluator.evaluate_recall(multi_pivot_repair, k=10, n_probe=5, ground_truth=ground_truth)
        recall_results['Multi-Pivot (repair=1)'] = recall_multi_repair['recall_at_k']
        print(f"ğŸ“Š Multi-Pivot (repair=1) å¬å›ç‡: {recall_multi_repair['recall_at_k']:.4f}")
    
    # æµ‹è¯•Hybrid HNSW
    if hybrid_index:
        recall_hybrid = evaluator.evaluate_hybrid_hnsw(hybrid_index, k=10, n_probe=5, ground_truth=ground_truth)
        recall_results['Hybrid HNSW'] = recall_hybrid['recall_at_k']
        print(f"ğŸ“Š Hybrid HNSW å¬å›ç‡: {recall_hybrid['recall_at_k']:.4f}")
    
    # === 5. ç»¼åˆå¯¹æ¯”è¡¨æ ¼ ===
    print(f"\n{'='*70}")
    print("5ï¸âƒ£ ç»¼åˆç»Ÿè®¡å¯¹æ¯”è¡¨")
    print("="*70)
    
    create_comparison_table([
        (stats_clean, "KMeansHNSW (æ— repair)"),
        (stats_repair, "KMeansHNSW (repair=1)"),
        (multi_stats_clean, "Multi-Pivot (æ— repair)"),
        (multi_stats_repair, "Multi-Pivot (repair=1)"),
        (hybrid_stats, "Hybrid HNSW")
    ], recall_results, n_data)

def analyze_system_nodes(system, name: str) -> Dict[str, Any]:
    """åˆ†æç³»ç»Ÿçš„èŠ‚ç‚¹åˆ†é…ç»Ÿè®¡"""
    print(f"\nğŸ” åˆ†æ {name}...")
    
    # è·å–åŸºæœ¬ç»Ÿè®¡
    stats = system.get_stats()
    total_assigned_nodes = stats.get('num_children', 0)
    coverage_fraction = stats.get('coverage_fraction', 0.0)
    avg_children_per_centroid = stats.get('avg_children_per_centroid', 0.0)
    
    # åˆ†æå»é‡æƒ…å†µ
    all_assigned_nodes = set()
    duplicate_count = 0
    total_assignments = 0
    
    if hasattr(system, 'parent_child_map'):
        for centroid_id, children in system.parent_child_map.items():
            for child_id in children:
                total_assignments += 1
                if child_id in all_assigned_nodes:
                    duplicate_count += 1
                else:
                    all_assigned_nodes.add(child_id)
    
    unique_assigned_nodes = len(all_assigned_nodes)
    duplication_rate = duplicate_count / total_assignments if total_assignments > 0 else 0.0
    
    result = {
        'name': name,
        'total_assignments': total_assignments,
        'unique_nodes': unique_assigned_nodes,
        'duplicate_assignments': duplicate_count,
        'duplication_rate': duplication_rate,
        'coverage_fraction': coverage_fraction,
        'avg_children_per_centroid': avg_children_per_centroid,
        'reported_num_children': total_assigned_nodes
    }
    
    print(f"  ğŸ“Š æ€»åˆ†é…æ•°: {total_assignments}")
    print(f"  ğŸ¯ å»é‡åèŠ‚ç‚¹æ•°: {unique_assigned_nodes}")
    print(f"  ğŸ”„ é‡å¤åˆ†é…æ•°: {duplicate_count}")
    print(f"  ğŸ“ˆ é‡å¤ç‡: {duplication_rate:.3f}")
    print(f"  ğŸ“ è¦†ç›–ç‡: {coverage_fraction:.3f}")
    print(f"  âš–ï¸ å¹³å‡å­èŠ‚ç‚¹/è´¨å¿ƒ: {avg_children_per_centroid:.1f}")
    
    return result

def analyze_hybrid_nodes(system, name: str) -> Dict[str, Any]:
    """åˆ†æHybrid HNSWçš„èŠ‚ç‚¹ç»Ÿè®¡"""
    print(f"\nğŸ” åˆ†æ {name}...")
    
    try:
        stats = system.get_stats()
        total_assigned_nodes = stats.get('num_children', 0)
        coverage_fraction = stats.get('coverage_fraction', 0.0)
        
        # Hybrid HNSWçš„ç‰¹æ®Šç»Ÿè®¡
        num_parents = stats.get('num_parents', 0)
        
        # åˆ†æå»é‡æƒ…å†µ
        all_assigned_nodes = set()
        duplicate_count = 0
        total_assignments = 0
        
        if hasattr(system, 'parent_child_map'):
            for parent_id, children in system.parent_child_map.items():
                for child_id in children:
                    total_assignments += 1
                    if child_id in all_assigned_nodes:
                        duplicate_count += 1
                    else:
                        all_assigned_nodes.add(child_id)
        
        unique_assigned_nodes = len(all_assigned_nodes)
        duplication_rate = duplicate_count / total_assignments if total_assignments > 0 else 0.0
        
        result = {
            'name': name,
            'total_assignments': total_assignments,
            'unique_nodes': unique_assigned_nodes,
            'duplicate_assignments': duplicate_count,
            'duplication_rate': duplication_rate,
            'coverage_fraction': coverage_fraction,
            'num_parents': num_parents,
            'avg_children_per_parent': total_assignments / num_parents if num_parents > 0 else 0.0,
            'reported_num_children': total_assigned_nodes
        }
        
        print(f"  ğŸ“Š çˆ¶èŠ‚ç‚¹æ•°: {num_parents}")
        print(f"  ğŸ“Š æ€»åˆ†é…æ•°: {total_assignments}")
        print(f"  ğŸ¯ å»é‡åèŠ‚ç‚¹æ•°: {unique_assigned_nodes}")
        print(f"  ğŸ”„ é‡å¤åˆ†é…æ•°: {duplicate_count}")
        print(f"  ğŸ“ˆ é‡å¤ç‡: {duplication_rate:.3f}")
        print(f"  ğŸ“ è¦†ç›–ç‡: {coverage_fraction:.3f}")
        print(f"  âš–ï¸ å¹³å‡å­èŠ‚ç‚¹/çˆ¶èŠ‚ç‚¹: {result['avg_children_per_parent']:.1f}")
        
        return result
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        return {
            'name': name,
            'error': str(e)
        }

def create_comparison_table(stats_list, recall_results, total_nodes):
    """åˆ›å»ºå¯¹æ¯”è¡¨æ ¼"""
    print("\nğŸ“‹ è¯¦ç»†å¯¹æ¯”è¡¨æ ¼:")
    print("-" * 120)
    print(f"{'æ–¹æ³•':<25} {'æ€»åˆ†é…':<8} {'å»é‡å':<8} {'é‡å¤æ•°':<8} {'é‡å¤ç‡':<8} {'è¦†ç›–ç‡':<8} {'å¬å›ç‡':<8}")
    print("-" * 120)
    
    for stats, name in stats_list:
        if stats and 'error' not in stats:
            recall = recall_results.get(name, 0.0)
            print(f"{name:<25} "
                  f"{stats['total_assignments']:<8} "
                  f"{stats['unique_nodes']:<8} "
                  f"{stats['duplicate_assignments']:<8} "
                  f"{stats['duplication_rate']:<8.3f} "
                  f"{stats['coverage_fraction']:<8.3f} "
                  f"{recall:<8.4f}")
    
    print("-" * 120)
    print(f"æ€»æ•°æ®é›†èŠ‚ç‚¹æ•°: {total_nodes}")
    
    # åˆ†æé—®é¢˜
    print(f"\nğŸ” é—®é¢˜åˆ†æ:")
    print(f"1. è¦†ç›–ç‡ä½å¯èƒ½å¯¼è‡´å¬å›ç‡ä¸‹é™")
    print(f"2. é‡å¤åˆ†é…é«˜è¡¨ç¤ºæ•ˆç‡æµªè´¹")
    print(f"3. repairæœºåˆ¶åº”è¯¥æé«˜è¦†ç›–ç‡")
    print(f"4. Multi-Pivotç†è®ºä¸Šåº”è¯¥æ¯”å•æ¢çº½æ›´å¥½")

if __name__ == "__main__":
    analyze_node_statistics()
